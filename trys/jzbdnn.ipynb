{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import InputLayer, Conv1D, BatchNormalization, ReLU, Dense, Flatten, Softmax, LSTM\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Reshape\n",
    "from tensorflow.keras.layers import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import mode\n",
    "import os\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "data_train = pd.read_csv(data_path+'sensor_train.csv')\n",
    "data_test = pd.read_csv(data_path+'sensor_test.csv')\n",
    "data_test['fragment_id'] += 10000\n",
    "label = 'behavior_id'\n",
    "y_train = data_train.groupby('fragment_id')['behavior_id'].min().values\n",
    "\n",
    "data = pd.concat([data_train, data_test], sort=False)\n",
    "df = data.drop_duplicates(subset=['fragment_id']).reset_index(drop=True)[['fragment_id', 'behavior_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_data(data, df):\n",
    "    # 加速度绝对值\n",
    "    data['acc'] = (data['acc_x'] ** 2 + data['acc_y'] ** 2 + data['acc_z'] ** 2) ** 0.5\n",
    "    data['accg'] = (data['acc_xg'] ** 2 + data['acc_yg'] ** 2 + data['acc_zg'] ** 2) ** 0.5\n",
    "    \n",
    "    data['accxy'] = (data['acc_x'] ** 2 + data['acc_y'] ** 2) ** 0.5\n",
    "    data['accyz'] = (data['acc_y'] ** 2 + data['acc_z'] ** 2) ** 0.5\n",
    "    data['accxz'] = (data['acc_x'] ** 2 + data['acc_z'] ** 2) ** 0.5\n",
    "    data['accxyg'] = (data['acc_xg'] ** 2 + data['acc_yg'] ** 2) ** 0.5\n",
    "    data['accyzg'] = (data['acc_yg'] ** 2 + data['acc_zg'] ** 2) ** 0.5\n",
    "    data['accxzg'] = (data['acc_xg'] ** 2 + data['acc_zg'] ** 2) ** 0.5\n",
    "    \n",
    "#     data['acc_sub'] = ((data['acc_xg'] - data['acc_x']) ** 2 + (data['acc_yg'] - data['acc_y']) ** 2 + (data['acc_zg'] - data['acc_z']) ** 2) ** 0.5\n",
    "#     data['acc_subxy'] = ((data['acc_xg'] - data['acc_x']) ** 2 + (data['acc_yg'] - data['acc_y']) ** 2) ** 0.5\n",
    "#     data['acc_subxz'] = ((data['acc_xg'] - data['acc_x']) ** 2 + (data['acc_zg'] - data['acc_z']) ** 2) ** 0.5\n",
    "#     data['acc_subyz'] = ((data['acc_yg'] - data['acc_y']) ** 2 + (data['acc_zg'] - data['acc_z']) ** 2) ** 0.5\n",
    "    \n",
    "    # 统计特征\n",
    "    for f in tqdm([f for f in data.columns if 'acc' in f]):\n",
    "        for stat in ['min', 'mean', 'median', 'std']:   # skew\n",
    "            df['{}_{}'.format(f, stat)] = data.groupby('fragment_id')[f].agg(stat).values\n",
    "    \n",
    "    train_df = df[df[label].isna()==False].reset_index(drop=True)\n",
    "    test_df = df[df[label].isna()==True].reset_index(drop=True)\n",
    "    \n",
    "    drop_feat = [] \n",
    "    used_feat = [f for f in train_df.columns if f not in (['fragment_id', label] + drop_feat)] \n",
    "    print('特征个数 ', len(used_feat))\n",
    "    print(used_feat)\n",
    "    \n",
    "    X_train = train_df[used_feat].values\n",
    "    y_train = train_df[label].values\n",
    "    X_test = test_df[used_feat].values\n",
    "\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "\n",
    "def load_y(df):\n",
    "    return y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:02<00:00,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征个数  68\n",
      "['acc_x_min', 'acc_x_mean', 'acc_x_median', 'acc_x_std', 'acc_y_min', 'acc_y_mean', 'acc_y_median', 'acc_y_std', 'acc_z_min', 'acc_z_mean', 'acc_z_median', 'acc_z_std', 'acc_xg_min', 'acc_xg_mean', 'acc_xg_median', 'acc_xg_std', 'acc_yg_min', 'acc_yg_mean', 'acc_yg_median', 'acc_yg_std', 'acc_zg_min', 'acc_zg_mean', 'acc_zg_median', 'acc_zg_std', 'acc_min', 'acc_mean', 'acc_median', 'acc_std', 'accg_min', 'accg_mean', 'accg_median', 'accg_std', 'accxy_min', 'accxy_mean', 'accxy_median', 'accxy_std', 'accyz_min', 'accyz_mean', 'accyz_median', 'accyz_std', 'accxz_min', 'accxz_mean', 'accxz_median', 'accxz_std', 'accxyg_min', 'accxyg_mean', 'accxyg_median', 'accxyg_std', 'accyzg_min', 'accyzg_mean', 'accyzg_median', 'accyzg_std', 'accxzg_min', 'accxzg_mean', 'accxzg_median', 'accxzg_std', 'acc_subxy_min', 'acc_subxy_mean', 'acc_subxy_median', 'acc_subxy_std', 'acc_subxz_min', 'acc_subxz_mean', 'acc_subxz_median', 'acc_subxz_std', 'acc_subyz_min', 'acc_subyz_mean', 'acc_subyz_median', 'acc_subyz_std']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_features, _, test_features = load_features_data(data, df)\n",
    "y = load_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# train_features = scaler.fit_transform(train_features)\n",
    "# test_features  = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Net(type='dense'):\n",
    "#     fea_input = Input(shape=(train_features.shape[1]))\n",
    "\n",
    "#     dense = Dense(32, activation='relu')(fea_input)\n",
    "#     dense = BatchNormalization()(dense)\n",
    "#     dense = Dense(64, activation='relu')(dense)\n",
    "#     dense = Dropout(0.2)(dense)\n",
    "#     dense = Dense(128, activation='relu')(dense)\n",
    "#     dense = Dropout(0.2)(dense)\n",
    "#     dense = Dense(256, activation='relu')(dense)\n",
    "#     dense = Dropout(0.3)(dense)\n",
    "\n",
    "#     dense = BatchNormalization()(Dropout(0.2)(Dense(64, activation='relu')(Flatten()(dense))))\n",
    "#     X = Dense(19, activation='softmax')(dense)\n",
    "#     return Model([fea_input], X)\n",
    "\n",
    "\n",
    "# def Net(type='dense'):\n",
    "#     fea_input = Input(shape=(train_features.shape[1]))\n",
    "\n",
    "#     dense = Dense(32, activation='relu')(fea_input)\n",
    "#     dense = BatchNormalization()(dense)\n",
    "#     dense = Dense(64, activation='relu')(dense)\n",
    "#     dense = Dropout(0.1)(dense)\n",
    "#     dense = Dense(128, activation='relu')(dense)\n",
    "#     dense = Dropout(0.1)(dense)\n",
    "#     dense = Dense(256, activation='relu')(dense)\n",
    "#     dense = Dropout(0.2)(dense)\n",
    "#     dense = BatchNormalization()(Dropout(0.2)(Dense(64, activation='relu')(Flatten()(dense))))\n",
    "#     X = Dense(19, activation='softmax')(dense)\n",
    "#     return Model([fea_input], X)\n",
    "\n",
    "# def Net(type='dense'):\n",
    "#     fea_input = Input(shape=(train_features.shape[1]))\n",
    "\n",
    "#     dense = Dense(32, activation='relu')(fea_input)\n",
    "#     dense = BatchNormalization()(dense)\n",
    "#     dense = Dense(64, activation='relu')(dense)\n",
    "# #     dense = Dropout(0.1)(dense)\n",
    "#     dense = Dense(128, activation='relu')(dense)\n",
    "# #     dense = Dropout(0.1)(dense)\n",
    "#     dense = Dense(256, activation='relu')(dense)\n",
    "# #     dense = Dropout(0.1)(dense)\n",
    "#     dense = BatchNormalization()(Dropout(0.2)(Dense(64, activation='relu')(Flatten()(dense))))\n",
    "#     X = Dense(19, activation='softmax')(dense)\n",
    "#     return Model([fea_input], X)\n",
    "\n",
    "# def Net(type='dense'):\n",
    "#     fea_input = Input(shape=(train_features.shape[1]))\n",
    "\n",
    "#     dense = Dense(32, activation='relu')(fea_input)\n",
    "#     dense = BatchNormalization()(dense)\n",
    "#     dense = Dense(64, activation='relu')(dense)\n",
    "# #     dense = Dropout(0.1)(dense)\n",
    "#     dense = Dense(128, activation='relu')(dense)\n",
    "# #     dense = Dropout(0.1)(dense)\n",
    "#     dense = Dense(256, activation='relu')(dense)\n",
    "# #     dense = Dropout(0.1)(dense)\n",
    "#     dense = Dense(512, activation='relu')(dense)\n",
    "#     dense = BatchNormalization()(Dropout(0.2)(Dense(64, activation='relu')(Flatten()(dense))))\n",
    "#     X = Dense(19, activation='softmax')(dense)\n",
    "#     return Model([fea_input], X)\n",
    "\n",
    "# def Net(type='dense'):\n",
    "#     fea_input = Input(shape=(train_features.shape[1]))\n",
    "    \n",
    "#     dense = Dense(64, activation='relu')(fea_input)\n",
    "#     dense = BatchNormalization()(dense)\n",
    "#     dense = Dense(128, activation='relu')(dense)\n",
    "#     dense = Dense(256, activation='relu')(dense)\n",
    "#     dense = Dense(512, activation='relu')(dense)\n",
    "#     dense = BatchNormalization()(Dropout(0.2)(Dense(64, activation='relu')(Flatten()(dense))))\n",
    "#     X = Dense(19, activation='softmax')(dense)\n",
    "#     return Model([fea_input], X)\n",
    "\n",
    "# def Net(type='dense'):\n",
    "#     fea_input = Input(shape=(train_features.shape[1]))\n",
    "    \n",
    "#     dense = Dense(128, activation='relu')(fea_input)\n",
    "#     dense = BatchNormalization()(dense)\n",
    "# #     dense = Dense(128, activation='relu')(dense)\n",
    "#     dense = Dense(256, activation='relu')(dense)\n",
    "#     dense = Dense(512, activation='relu')(dense)\n",
    "#     dense = Dense(1024, activation='relu')(dense)\n",
    "#     dense = BatchNormalization()(Dropout(0.2)(Dense(64, activation='relu')(Flatten()(dense))))\n",
    "#     X = Dense(19, activation='softmax')(dense)\n",
    "#     return Model([fea_input], X)\n",
    "\n",
    "\n",
    "# def Net(type='dense'):\n",
    "#     fea_input = Input(shape=(train_features.shape[1]))\n",
    "    \n",
    "#     dense = Dense(128, activation='relu')(fea_input)\n",
    "#     dense = BatchNormalization()(dense)\n",
    "# #     dense = Dense(128, activation='relu')(dense)\n",
    "#     dense = Dense(256, activation='relu')(dense)\n",
    "#     dense = Dense(512, activation='relu')(dense)\n",
    "# #     dense = Dense(1024, activation='relu')(dense)\n",
    "#     dense = BatchNormalization()(Dropout(0.2)(Dense(64, activation='relu')(Flatten()(dense))))\n",
    "#     X = Dense(19, activation='softmax')(dense)\n",
    "#     return Model([fea_input], X)\n",
    "\n",
    "# def Net(type='dense'):\n",
    "#     fea_input = Input(shape=(train_features.shape[1]))\n",
    "    \n",
    "#     dense = Dense(256, activation='relu')(fea_input)\n",
    "#     dense = BatchNormalization()(dense)\n",
    "#     dense = Dense(512, activation='relu')(dense)\n",
    "#     dense = Dense(1024, activation='relu')(dense)\n",
    "#     dense = BatchNormalization()(Dropout(0.2)(Dense(64, activation='relu')(Flatten()(dense))))\n",
    "#     X = Dense(19, activation='softmax')(dense)\n",
    "#     return Model([fea_input], X)\n",
    "\n",
    "def Net(type='dense'):\n",
    "    fea_input = Input(shape=(train_features.shape[1]))\n",
    "    \n",
    "    dense = Dense(256, activation='relu')(fea_input)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dense(512, activation='relu')(dense)\n",
    "    dense = Dense(1024, activation='relu')(dense)\n",
    "    dense = BatchNormalization()(Dropout(0.2)(Dense(64, activation='relu')(Flatten()(dense))))\n",
    "    X = Dense(19, activation='softmax')(dense)\n",
    "    return Model([fea_input], X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================train 1th fold========================================\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 68)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 19)                1235      \n",
      "=================================================================\n",
      "Total params: 742,675\n",
      "Trainable params: 742,035\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "Train on 5833 samples, validate on 1459 samples\n",
      "Epoch 1/500\n",
      "5833/5833 - 3s - loss: 2.1085 - acc: 0.3259 - val_loss: 2.2869 - val_acc: 0.3454\n",
      "Epoch 2/500\n",
      "5833/5833 - 1s - loss: 1.7721 - acc: 0.4140 - val_loss: 1.8153 - val_acc: 0.3777\n",
      "Epoch 3/500\n",
      "5833/5833 - 1s - loss: 1.6567 - acc: 0.4327 - val_loss: 2.3842 - val_acc: 0.2529\n",
      "Epoch 4/500\n",
      "5833/5833 - 1s - loss: 1.5631 - acc: 0.4526 - val_loss: 2.5302 - val_acc: 0.2968\n",
      "Epoch 5/500\n",
      "5833/5833 - 1s - loss: 1.5037 - acc: 0.4862 - val_loss: 1.9068 - val_acc: 0.3962\n",
      "Epoch 6/500\n",
      "5833/5833 - 1s - loss: 1.4542 - acc: 0.5009 - val_loss: 2.6410 - val_acc: 0.2673\n",
      "Epoch 7/500\n",
      "5833/5833 - 1s - loss: 1.3965 - acc: 0.5136 - val_loss: 2.1798 - val_acc: 0.3461\n",
      "Epoch 8/500\n",
      "5833/5833 - 1s - loss: 1.3383 - acc: 0.5345 - val_loss: 1.7247 - val_acc: 0.4599\n",
      "Epoch 9/500\n",
      "5833/5833 - 1s - loss: 1.2999 - acc: 0.5417 - val_loss: 2.0048 - val_acc: 0.3934\n",
      "Epoch 10/500\n",
      "5833/5833 - 1s - loss: 1.2703 - acc: 0.5558 - val_loss: 1.6903 - val_acc: 0.4572\n",
      "Epoch 11/500\n",
      "5833/5833 - 1s - loss: 1.2249 - acc: 0.5693 - val_loss: 3.0426 - val_acc: 0.3187\n",
      "Epoch 12/500\n",
      "5833/5833 - 1s - loss: 1.1996 - acc: 0.5831 - val_loss: 1.6577 - val_acc: 0.4531\n",
      "Epoch 13/500\n",
      "5833/5833 - 1s - loss: 1.1868 - acc: 0.5795 - val_loss: 1.8586 - val_acc: 0.4297\n",
      "Epoch 14/500\n",
      "5833/5833 - 1s - loss: 1.1421 - acc: 0.5935 - val_loss: 1.4965 - val_acc: 0.5161\n",
      "Epoch 15/500\n",
      "5833/5833 - 1s - loss: 1.1401 - acc: 0.5985 - val_loss: 1.7324 - val_acc: 0.4791\n",
      "Epoch 16/500\n",
      "5833/5833 - 1s - loss: 1.0896 - acc: 0.6177 - val_loss: 1.7340 - val_acc: 0.4229\n",
      "Epoch 17/500\n",
      "5833/5833 - 1s - loss: 1.0548 - acc: 0.6225 - val_loss: 2.1039 - val_acc: 0.3831\n",
      "Epoch 18/500\n",
      "5833/5833 - 1s - loss: 1.0637 - acc: 0.6239 - val_loss: 1.3838 - val_acc: 0.5531\n",
      "Epoch 19/500\n",
      "5833/5833 - 1s - loss: 1.0503 - acc: 0.6192 - val_loss: 1.3752 - val_acc: 0.5367\n",
      "Epoch 20/500\n",
      "5833/5833 - 1s - loss: 1.0271 - acc: 0.6376 - val_loss: 1.5340 - val_acc: 0.5065\n",
      "Epoch 21/500\n",
      "5833/5833 - 1s - loss: 1.0053 - acc: 0.6456 - val_loss: 2.0573 - val_acc: 0.4167\n",
      "Epoch 22/500\n",
      "5833/5833 - 1s - loss: 0.9928 - acc: 0.6568 - val_loss: 1.3829 - val_acc: 0.5497\n",
      "Epoch 23/500\n",
      "5833/5833 - 1s - loss: 0.9587 - acc: 0.6650 - val_loss: 1.6762 - val_acc: 0.4757\n",
      "Epoch 24/500\n",
      "5833/5833 - 1s - loss: 0.9381 - acc: 0.6664 - val_loss: 1.3177 - val_acc: 0.5689\n",
      "Epoch 25/500\n",
      "5833/5833 - 1s - loss: 0.9493 - acc: 0.6676 - val_loss: 1.9757 - val_acc: 0.4565\n",
      "Epoch 26/500\n",
      "5833/5833 - 1s - loss: 0.9080 - acc: 0.6798 - val_loss: 1.3707 - val_acc: 0.5469\n",
      "Epoch 27/500\n",
      "5833/5833 - 1s - loss: 0.9060 - acc: 0.6825 - val_loss: 1.5229 - val_acc: 0.5284\n",
      "Epoch 28/500\n",
      "5833/5833 - 1s - loss: 0.8890 - acc: 0.6864 - val_loss: 1.3980 - val_acc: 0.5668\n",
      "Epoch 29/500\n",
      "5833/5833 - 1s - loss: 0.8669 - acc: 0.6883 - val_loss: 1.1845 - val_acc: 0.6175\n",
      "Epoch 30/500\n",
      "5833/5833 - 1s - loss: 0.8584 - acc: 0.6936 - val_loss: 1.2118 - val_acc: 0.6059\n",
      "Epoch 31/500\n",
      "5833/5833 - 1s - loss: 0.8362 - acc: 0.7005 - val_loss: 1.5434 - val_acc: 0.5278\n",
      "Epoch 32/500\n",
      "5833/5833 - 1s - loss: 0.8269 - acc: 0.6993 - val_loss: 1.4112 - val_acc: 0.5559\n",
      "Epoch 33/500\n",
      "5833/5833 - 1s - loss: 0.8271 - acc: 0.7098 - val_loss: 1.7166 - val_acc: 0.4832\n",
      "Epoch 34/500\n",
      "5833/5833 - 1s - loss: 0.7982 - acc: 0.7134 - val_loss: 1.7003 - val_acc: 0.4983\n",
      "Epoch 35/500\n",
      "5833/5833 - 1s - loss: 0.7901 - acc: 0.7176 - val_loss: 2.0849 - val_acc: 0.4990\n",
      "Epoch 36/500\n",
      "5833/5833 - 1s - loss: 0.7761 - acc: 0.7233 - val_loss: 1.2898 - val_acc: 0.6032\n",
      "Epoch 37/500\n",
      "5833/5833 - 1s - loss: 0.7971 - acc: 0.7183 - val_loss: 1.5119 - val_acc: 0.5538\n",
      "Epoch 38/500\n",
      "5833/5833 - 1s - loss: 0.7704 - acc: 0.7231 - val_loss: 2.5655 - val_acc: 0.4304\n",
      "Epoch 39/500\n",
      "5833/5833 - 1s - loss: 0.7484 - acc: 0.7348 - val_loss: 1.2636 - val_acc: 0.6114\n",
      "Epoch 40/500\n",
      "5833/5833 - 1s - loss: 0.7436 - acc: 0.7343 - val_loss: 1.3588 - val_acc: 0.6038\n",
      "Epoch 41/500\n",
      "5833/5833 - 1s - loss: 0.7181 - acc: 0.7444 - val_loss: 1.2263 - val_acc: 0.6196\n",
      "Epoch 42/500\n",
      "5833/5833 - 1s - loss: 0.7400 - acc: 0.7401 - val_loss: 1.3084 - val_acc: 0.6045\n",
      "Epoch 43/500\n",
      "5833/5833 - 1s - loss: 0.7264 - acc: 0.7353 - val_loss: 1.0639 - val_acc: 0.6607\n",
      "Epoch 44/500\n",
      "5833/5833 - 1s - loss: 0.6870 - acc: 0.7545 - val_loss: 1.0975 - val_acc: 0.6498\n",
      "Epoch 45/500\n",
      "5833/5833 - 1s - loss: 0.7101 - acc: 0.7483 - val_loss: 1.9852 - val_acc: 0.5003\n",
      "Epoch 46/500\n",
      "5833/5833 - 1s - loss: 0.7087 - acc: 0.7516 - val_loss: 1.5552 - val_acc: 0.5517\n",
      "Epoch 47/500\n",
      "5833/5833 - 1s - loss: 0.6856 - acc: 0.7590 - val_loss: 1.4105 - val_acc: 0.5819\n",
      "Epoch 48/500\n",
      "5833/5833 - 1s - loss: 0.6696 - acc: 0.7631 - val_loss: 1.5329 - val_acc: 0.5737\n",
      "Epoch 49/500\n",
      "5833/5833 - 1s - loss: 0.6638 - acc: 0.7646 - val_loss: 1.2948 - val_acc: 0.6107\n",
      "Epoch 50/500\n",
      "5833/5833 - 1s - loss: 0.6697 - acc: 0.7548 - val_loss: 1.2575 - val_acc: 0.6011\n",
      "Epoch 51/500\n",
      "5833/5833 - 1s - loss: 0.6498 - acc: 0.7687 - val_loss: 1.2693 - val_acc: 0.6354\n",
      "Epoch 52/500\n",
      "5833/5833 - 1s - loss: 0.6502 - acc: 0.7672 - val_loss: 1.7179 - val_acc: 0.5716\n",
      "Epoch 53/500\n",
      "5833/5833 - 1s - loss: 0.6450 - acc: 0.7737 - val_loss: 1.2479 - val_acc: 0.6415\n",
      "Epoch 54/500\n",
      "5833/5833 - 1s - loss: 0.6500 - acc: 0.7632 - val_loss: 1.3652 - val_acc: 0.5853\n",
      "Epoch 55/500\n",
      "5833/5833 - 1s - loss: 0.6295 - acc: 0.7763 - val_loss: 1.1087 - val_acc: 0.6299\n",
      "Epoch 56/500\n",
      "5833/5833 - 1s - loss: 0.6309 - acc: 0.7739 - val_loss: 2.1282 - val_acc: 0.4880\n",
      "Epoch 57/500\n",
      "5833/5833 - 1s - loss: 0.6130 - acc: 0.7833 - val_loss: 1.2605 - val_acc: 0.6251\n",
      "Epoch 58/500\n",
      "5833/5833 - 1s - loss: 0.6128 - acc: 0.7831 - val_loss: 1.4065 - val_acc: 0.5977\n",
      "Epoch 59/500\n",
      "5833/5833 - 1s - loss: 0.5949 - acc: 0.7833 - val_loss: 1.3409 - val_acc: 0.6169\n",
      "Epoch 60/500\n",
      "5833/5833 - 1s - loss: 0.5967 - acc: 0.7876 - val_loss: 1.7537 - val_acc: 0.5394\n",
      "Epoch 61/500\n",
      "5833/5833 - 1s - loss: 0.5680 - acc: 0.7970 - val_loss: 1.0626 - val_acc: 0.6943\n",
      "Epoch 62/500\n",
      "5833/5833 - 1s - loss: 0.5847 - acc: 0.7914 - val_loss: 1.3091 - val_acc: 0.6429\n",
      "Epoch 63/500\n",
      "5833/5833 - 1s - loss: 0.5667 - acc: 0.8004 - val_loss: 1.6662 - val_acc: 0.5661\n",
      "Epoch 64/500\n",
      "5833/5833 - 1s - loss: 0.5759 - acc: 0.7955 - val_loss: 1.5512 - val_acc: 0.5840\n",
      "Epoch 65/500\n",
      "5833/5833 - 1s - loss: 0.5495 - acc: 0.8028 - val_loss: 1.2781 - val_acc: 0.6511\n",
      "Epoch 66/500\n",
      "5833/5833 - 1s - loss: 0.5635 - acc: 0.7986 - val_loss: 1.6446 - val_acc: 0.5771\n",
      "Epoch 67/500\n",
      "5833/5833 - 1s - loss: 0.5731 - acc: 0.7991 - val_loss: 1.4130 - val_acc: 0.5942\n",
      "Epoch 68/500\n",
      "5833/5833 - 1s - loss: 0.5392 - acc: 0.8056 - val_loss: 1.7845 - val_acc: 0.5675\n",
      "Epoch 69/500\n",
      "5833/5833 - 1s - loss: 0.5499 - acc: 0.8049 - val_loss: 1.2221 - val_acc: 0.6717\n",
      "Epoch 70/500\n",
      "5833/5833 - 1s - loss: 0.5453 - acc: 0.8054 - val_loss: 1.1612 - val_acc: 0.6648\n",
      "Epoch 71/500\n",
      "5833/5833 - 1s - loss: 0.5672 - acc: 0.7977 - val_loss: 1.5044 - val_acc: 0.5757\n",
      "Epoch 72/500\n",
      "5833/5833 - 1s - loss: 0.5290 - acc: 0.8111 - val_loss: 1.1521 - val_acc: 0.6799\n",
      "Epoch 73/500\n",
      "5833/5833 - 1s - loss: 0.5123 - acc: 0.8203 - val_loss: 1.5843 - val_acc: 0.6361\n",
      "Epoch 74/500\n",
      "5833/5833 - 1s - loss: 0.5494 - acc: 0.8044 - val_loss: 2.1106 - val_acc: 0.5195\n",
      "Epoch 75/500\n",
      "5833/5833 - 1s - loss: 0.5241 - acc: 0.8094 - val_loss: 1.5157 - val_acc: 0.6354\n",
      "Epoch 76/500\n",
      "5833/5833 - 1s - loss: 0.5290 - acc: 0.8171 - val_loss: 1.6546 - val_acc: 0.5696\n",
      "Epoch 77/500\n",
      "5833/5833 - 1s - loss: 0.5156 - acc: 0.8183 - val_loss: 2.0575 - val_acc: 0.5175\n",
      "Epoch 78/500\n",
      "5833/5833 - 1s - loss: 0.5145 - acc: 0.8227 - val_loss: 1.4408 - val_acc: 0.5929\n",
      "Epoch 79/500\n",
      "5833/5833 - 1s - loss: 0.4943 - acc: 0.8248 - val_loss: 1.2784 - val_acc: 0.6511\n",
      "Epoch 80/500\n",
      "5833/5833 - 1s - loss: 0.5317 - acc: 0.8152 - val_loss: 1.3592 - val_acc: 0.6271\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5833/5833 - 1s - loss: 0.4935 - acc: 0.8263 - val_loss: 1.6477 - val_acc: 0.6080\n",
      "Epoch 82/500\n",
      "5833/5833 - 1s - loss: 0.4318 - acc: 0.8517 - val_loss: 1.0677 - val_acc: 0.6923\n",
      "Epoch 83/500\n",
      "5833/5833 - 1s - loss: 0.4165 - acc: 0.8508 - val_loss: 1.0597 - val_acc: 0.6991\n",
      "Epoch 84/500\n",
      "5833/5833 - 1s - loss: 0.4052 - acc: 0.8568 - val_loss: 1.0208 - val_acc: 0.7060\n",
      "Epoch 85/500\n",
      "5833/5833 - 1s - loss: 0.3934 - acc: 0.8592 - val_loss: 1.0888 - val_acc: 0.7204\n",
      "Epoch 86/500\n",
      "5833/5833 - 1s - loss: 0.3868 - acc: 0.8613 - val_loss: 1.1016 - val_acc: 0.7142\n",
      "Epoch 87/500\n",
      "5833/5833 - 1s - loss: 0.4075 - acc: 0.8579 - val_loss: 0.9880 - val_acc: 0.7608\n",
      "Epoch 88/500\n",
      "5833/5833 - 1s - loss: 0.3671 - acc: 0.8733 - val_loss: 1.1486 - val_acc: 0.7162\n",
      "Epoch 89/500\n",
      "5833/5833 - 1s - loss: 0.3767 - acc: 0.8714 - val_loss: 1.0150 - val_acc: 0.7293\n",
      "Epoch 90/500\n",
      "5833/5833 - 1s - loss: 0.3821 - acc: 0.8642 - val_loss: 0.9869 - val_acc: 0.7368\n",
      "Epoch 91/500\n",
      "5833/5833 - 1s - loss: 0.3645 - acc: 0.8712 - val_loss: 1.2786 - val_acc: 0.6977\n",
      "Epoch 92/500\n",
      "5833/5833 - 1s - loss: 0.3736 - acc: 0.8656 - val_loss: 1.1873 - val_acc: 0.7060\n",
      "Epoch 93/500\n",
      "5833/5833 - 1s - loss: 0.3615 - acc: 0.8699 - val_loss: 1.1239 - val_acc: 0.7005\n",
      "Epoch 94/500\n",
      "5833/5833 - 1s - loss: 0.3604 - acc: 0.8683 - val_loss: 1.0625 - val_acc: 0.7286\n",
      "Epoch 95/500\n",
      "5833/5833 - 1s - loss: 0.3577 - acc: 0.8694 - val_loss: 1.0686 - val_acc: 0.7190\n",
      "Epoch 96/500\n",
      "5833/5833 - 1s - loss: 0.3736 - acc: 0.8700 - val_loss: 1.0797 - val_acc: 0.7354\n",
      "Epoch 97/500\n",
      "5833/5833 - 1s - loss: 0.3660 - acc: 0.8738 - val_loss: 1.0780 - val_acc: 0.7252\n",
      "Epoch 98/500\n",
      "5833/5833 - 1s - loss: 0.3631 - acc: 0.8695 - val_loss: 1.1420 - val_acc: 0.6977\n",
      "Epoch 99/500\n",
      "5833/5833 - 1s - loss: 0.3506 - acc: 0.8743 - val_loss: 1.1796 - val_acc: 0.7156\n",
      "Epoch 100/500\n",
      "5833/5833 - 1s - loss: 0.3628 - acc: 0.8706 - val_loss: 1.1014 - val_acc: 0.7334\n",
      "Epoch 101/500\n",
      "5833/5833 - 1s - loss: 0.3542 - acc: 0.8776 - val_loss: 1.0784 - val_acc: 0.7361\n",
      "Epoch 102/500\n",
      "5833/5833 - 1s - loss: 0.3506 - acc: 0.8759 - val_loss: 1.0672 - val_acc: 0.7306\n",
      "Epoch 103/500\n",
      "5833/5833 - 1s - loss: 0.3344 - acc: 0.8814 - val_loss: 1.4236 - val_acc: 0.6813\n",
      "Epoch 104/500\n",
      "5833/5833 - 1s - loss: 0.3357 - acc: 0.8827 - val_loss: 1.1282 - val_acc: 0.7238\n",
      "Epoch 105/500\n",
      "5833/5833 - 1s - loss: 0.3465 - acc: 0.8821 - val_loss: 1.1132 - val_acc: 0.7245\n",
      "Epoch 106/500\n",
      "5833/5833 - 1s - loss: 0.3591 - acc: 0.8779 - val_loss: 1.1505 - val_acc: 0.7149\n",
      "Epoch 107/500\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "5833/5833 - 1s - loss: 0.3501 - acc: 0.8829 - val_loss: 1.1544 - val_acc: 0.7210\n",
      "Epoch 108/500\n",
      "5833/5833 - 1s - loss: 0.3073 - acc: 0.8944 - val_loss: 1.0278 - val_acc: 0.7389\n",
      "Epoch 109/500\n",
      "5833/5833 - 1s - loss: 0.2858 - acc: 0.8982 - val_loss: 1.0154 - val_acc: 0.7526\n",
      "Epoch 110/500\n",
      "5833/5833 - 1s - loss: 0.2984 - acc: 0.8982 - val_loss: 1.0267 - val_acc: 0.7512\n",
      "Epoch 111/500\n",
      "5833/5833 - 1s - loss: 0.2845 - acc: 0.8997 - val_loss: 1.0243 - val_acc: 0.7382\n",
      "Epoch 112/500\n",
      "5833/5833 - 1s - loss: 0.2941 - acc: 0.8994 - val_loss: 1.0094 - val_acc: 0.7539\n",
      "Epoch 113/500\n",
      "5833/5833 - 1s - loss: 0.3020 - acc: 0.8953 - val_loss: 1.0011 - val_acc: 0.7690\n",
      "Epoch 114/500\n",
      "5833/5833 - 1s - loss: 0.2825 - acc: 0.9021 - val_loss: 0.9763 - val_acc: 0.7711\n",
      "Epoch 115/500\n",
      "5833/5833 - 1s - loss: 0.2932 - acc: 0.8937 - val_loss: 1.0245 - val_acc: 0.7745\n",
      "Epoch 116/500\n",
      "5833/5833 - 1s - loss: 0.2891 - acc: 0.9001 - val_loss: 1.0083 - val_acc: 0.7491\n",
      "Epoch 117/500\n",
      "5833/5833 - 1s - loss: 0.2700 - acc: 0.9028 - val_loss: 1.0077 - val_acc: 0.7539\n",
      "Epoch 118/500\n",
      "5833/5833 - 1s - loss: 0.2689 - acc: 0.9040 - val_loss: 0.9950 - val_acc: 0.7437\n",
      "Epoch 119/500\n",
      "5833/5833 - 1s - loss: 0.2737 - acc: 0.9023 - val_loss: 1.0088 - val_acc: 0.7615\n",
      "Epoch 120/500\n",
      "5833/5833 - 1s - loss: 0.2764 - acc: 0.9047 - val_loss: 1.0174 - val_acc: 0.7574\n",
      "Epoch 121/500\n",
      "5833/5833 - 1s - loss: 0.2695 - acc: 0.9074 - val_loss: 1.0137 - val_acc: 0.7690\n",
      "Epoch 122/500\n",
      "5833/5833 - 1s - loss: 0.2782 - acc: 0.9021 - val_loss: 1.0832 - val_acc: 0.7676\n",
      "Epoch 123/500\n",
      "5833/5833 - 1s - loss: 0.2750 - acc: 0.9047 - val_loss: 0.9995 - val_acc: 0.7814\n",
      "Epoch 124/500\n",
      "5833/5833 - 1s - loss: 0.2702 - acc: 0.9054 - val_loss: 1.1381 - val_acc: 0.7327\n",
      "Epoch 125/500\n",
      "5833/5833 - 1s - loss: 0.2732 - acc: 0.9067 - val_loss: 1.1916 - val_acc: 0.7498\n",
      "Epoch 126/500\n",
      "5833/5833 - 1s - loss: 0.2864 - acc: 0.9004 - val_loss: 1.0512 - val_acc: 0.7382\n",
      "Epoch 127/500\n",
      "5833/5833 - 1s - loss: 0.2708 - acc: 0.9025 - val_loss: 1.0393 - val_acc: 0.7642\n",
      "Epoch 128/500\n",
      "5833/5833 - 1s - loss: 0.2703 - acc: 0.9069 - val_loss: 1.0478 - val_acc: 0.7615\n",
      "Epoch 129/500\n"
     ]
    }
   ],
   "source": [
    "# acc_scores = []\n",
    "# combo_scores = []\n",
    "# proba_xx = np.zeros((7292, 19))\n",
    "# proba_t = np.zeros((7500, 19))\n",
    "# kfold = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "# for fold, (train_index, valid_index) in enumerate(kfold.split(train_features, y)):\n",
    "#     print(\"{}train {}th fold{}\".format('==' * 20, fold + 1, '==' * 20))\n",
    "#     y_ = to_categorical(y, num_classes=19)\n",
    "#     model = Net(type='conv1')\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='rmsprop',\n",
    "#                   metrics=['acc'])\n",
    "#     model.summary()\n",
    "#     plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "#                                 verbose=1,\n",
    "#                                 mode='max',\n",
    "#                                 factor=0.5,\n",
    "#                                 patience=20)\n",
    "#     early_stopping = EarlyStopping(monitor='val_acc',\n",
    "#                                    verbose=1,\n",
    "#                                    mode='max',\n",
    "#                                    patience=40)\n",
    "#     checkpoint = ModelCheckpoint(f'models/fold{fold}.h5',\n",
    "#                                  monitor='val_acc',\n",
    "#                                  verbose=0,\n",
    "#                                  mode='max',\n",
    "#                                  save_best_only=True)\n",
    "\n",
    "#     csv_logger = CSVLogger('logs/log.csv', separator=',', append=True)\n",
    "#     history = model.fit(train_features[train_index], y_[train_index],\n",
    "#                         epochs=500,\n",
    "#                         batch_size=32,\n",
    "#                         verbose=2,\n",
    "#                         shuffle=True,\n",
    "#                         validation_data=(train_features[valid_index],\n",
    "#                                          y_[valid_index]),\n",
    "#                         callbacks=[plateau, early_stopping, checkpoint, csv_logger])\n",
    "#     model.load_weights(f'models/fold{fold}.h5')\n",
    "#     proba_x = model.predict(train_features[valid_index], verbose=0, batch_size=1024)\n",
    "#     proba_xx[valid_index] = proba_x\n",
    "#     proba_t += model.predict(test_features, verbose=0, batch_size=1024) / 5.\n",
    "\n",
    "#     oof_y = np.argmax(proba_x, axis=1)\n",
    "#     score1 = accuracy_score(y[valid_index], oof_y)\n",
    "#     # print('accuracy_score',score1)\n",
    "#     score = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[valid_index], oof_y)) / oof_y.shape[0]\n",
    "#     print('accuracy_score', score1, 'acc_combo', score)\n",
    "#     acc_scores.append(score1)\n",
    "#     combo_scores.append(score)\n",
    "\n",
    "# #     print(history.history.keys())\n",
    "# #     # summarize history for accuracy\n",
    "# #     plt.plot(history.history['acc'])\n",
    "# #     plt.plot(history.history['val_acc'])\n",
    "# #     plt.title('model accuracy')\n",
    "# #     plt.ylabel('accuracy')\n",
    "# #     plt.xlabel('epoch')\n",
    "# #     plt.legend(['train', 'test'], loc='upper left')\n",
    "# #     plt.show()\n",
    "# #     # summarize history for loss\n",
    "# #     plt.plot(history.history['loss'])\n",
    "# #     plt.plot(history.history['val_loss'])\n",
    "# #     plt.title('model loss')\n",
    "# #     plt.ylabel('loss')\n",
    "# #     plt.xlabel('epoch')\n",
    "# #     plt.legend(['train', 'test'], loc='upper left')\n",
    "# #     plt.show()\n",
    "\n",
    "# print(\"5kflod mean acc score:{}\".format(np.mean(acc_scores)))\n",
    "# print(\"5kflod mean combo score:{}\".format(np.mean(combo_scores)))\n",
    "\n",
    "\n",
    "acc_scores = []\n",
    "combo_scores = []\n",
    "proba_xx = np.zeros((7292, 19))\n",
    "proba_t = np.zeros((7500, 19))\n",
    "kfold = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(train_features, y)):\n",
    "    print(\"{}train {}th fold{}\".format('==' * 20, fold + 1, '==' * 20))\n",
    "    y_ = to_categorical(y, num_classes=19)\n",
    "    model = Net(type='conv1')\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                verbose=1,\n",
    "                                mode='max',\n",
    "                                factor=0.5,\n",
    "                                patience=20)\n",
    "    early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                                   verbose=1,\n",
    "                                   mode='max',\n",
    "                                   patience=40)\n",
    "    checkpoint = ModelCheckpoint(f'models/fold{fold}.h5',\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=0,\n",
    "                                 mode='max',\n",
    "                                 save_best_only=True)\n",
    "\n",
    "    csv_logger = CSVLogger('logs/log.csv', separator=',', append=True)\n",
    "    history = model.fit(train_features[train_index], y_[train_index],\n",
    "                        epochs=500,\n",
    "                        batch_size=32,\n",
    "                        verbose=2,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(train_features[valid_index],\n",
    "                                         y_[valid_index]),\n",
    "                        callbacks=[plateau, early_stopping, checkpoint, csv_logger])\n",
    "    model.load_weights(f'models/fold{fold}.h5')\n",
    "    proba_x = model.predict(train_features[valid_index], verbose=0, batch_size=64)\n",
    "    proba_xx[valid_index] = proba_x\n",
    "    proba_t += model.predict(test_features, verbose=0, batch_size=64) / 5.\n",
    "\n",
    "    oof_y = np.argmax(proba_x, axis=1)\n",
    "    score1 = accuracy_score(y[valid_index], oof_y)\n",
    "    # print('accuracy_score',score1)\n",
    "    score = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[valid_index], oof_y)) / oof_y.shape[0]\n",
    "    print('accuracy_score', score1, 'acc_combo', score)\n",
    "    acc_scores.append(score1)\n",
    "    combo_scores.append(score)\n",
    "\n",
    "print(\"5kflod mean acc score:{}\".format(np.mean(acc_scores)))\n",
    "print(\"5kflod mean combo score:{}\".format(np.mean(combo_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('data/提交结果示例.csv')\n",
    "sub.behavior_id = np.argmax(proba_t, axis=1)\n",
    "sub.to_csv('result/dnn_acc{}_combo{}.csv'.format(round(np.mean(acc_scores), 5), round(np.mean(combo_scores), 5)), index=False)\n",
    "\n",
    "\n",
    "# acc 654 up 668\n",
    "# 5kflod mean acc score:0.6907564889795235  691\n",
    "# 5kflod mean acc score:0.7309383787869814  714\n",
    "# 5kflod mean acc score:0.747531569342551   725\n",
    "# 5kflod mean acc score:0.7619320409435404  727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(proba_t, columns=['pred_{}'.format(i) for i in range(19)]).to_csv('result/har_proba_t_{}.csv'.format(round(np.mean(acc_scores), 5)), index=False)\n",
    "pd.DataFrame(proba_xx, columns=['pred_{}'.format(i) for i in range(19)]).to_csv('result/har_proba_x_{}.csv'.format(round(np.mean(acc_scores), 5)), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(proba_xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_py3",
   "language": "python",
   "name": "conda_tensorflow2_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
