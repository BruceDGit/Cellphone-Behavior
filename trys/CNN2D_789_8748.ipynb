{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "from scipy.signal import welch\n",
    "from scipy.interpolate import  interp1d as interp\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "class Preprocessing(object):\n",
    "    \"\"\"\n",
    "    对序列进行提取特征等预处理\n",
    "    为了可能处理变长序列,因此输入为list\n",
    "    \"\"\"\n",
    "    def __init__(self,with_label=True):\n",
    "        self.with_label=with_label\n",
    "    def __call__(self,sequence):\n",
    "        for i in range(len(sequence)):\n",
    "            time_point=np.expand_dims(sequence[i][0],axis=0)\n",
    "            if self.with_label:\n",
    "                label=np.expand_dims(sequence[i][-1],axis=0)\n",
    "                new_sequence=self.for_each(sequence[i][1:-1])\n",
    "                sequence[i]=np.concatenate([time_point,new_sequence,label],axis=0)\n",
    "            else:\n",
    "                new_sequence=self.for_each(sequence[i][1:])\n",
    "                sequence[i]=np.concatenate([time_point,new_sequence],axis=0)\n",
    "        sequence=self.for_all(sequence)\n",
    "        return sequence\n",
    "    def smooth(self,array,decay_rate=0.9):\n",
    "        _smooth=np.zeros(array.shape)\n",
    "        for i in range(1,len(array)-1):\n",
    "            decay=min(decay_rate,(i+1)/(i+10))\n",
    "            _smooth[i]=_smooth[i-1]*decay+(1-decay)*array[i]\n",
    "        return _smooth\n",
    "    def for_all(self,sequence):#变长序列可以numpy吗#\n",
    "        return sequence\n",
    "        mean=np.zeros(shape=sequence[0].shape[0])\n",
    "        std=np.zeros(shape=sequence[0].shape[0])\n",
    "        lenth=len(sequence)\n",
    "        for index in range(lenth):\n",
    "            if self.with_label:\n",
    "                _range=range(1,sequence[index].shape[0]-1)\n",
    "            else:\n",
    "                _range=range(1,sequence[index].shape[0])\n",
    "            for i in _range:\n",
    "                mean[i]+=sequence[index][i].mean()/lenth\n",
    "                std[i]+=sequence[index][i].std()/lenth\n",
    "        for index in range(lenth):\n",
    "            if self.with_label:\n",
    "                _range=range(1,sequence[index].shape[0]-1)\n",
    "            else:\n",
    "                _range=range(1,sequence[index].shape[0])\n",
    "            for i in _range:\n",
    "                sequence[index][i]=(sequence[index][i]-mean[i])/std[i]\n",
    "        return sequence\n",
    "    def for_each(self,sequence):\n",
    "        acc=(sequence[0]**2+sequence[1]**2+sequence[2]**2)**0.5\n",
    "        acc=np.expand_dims(acc,axis=0)\n",
    "        acc_g=(sequence[3]**2+sequence[4]**2+sequence[5]**2)**0.5\n",
    "        acc_g=np.expand_dims(acc_g,axis=0)\n",
    "        sequence=np.concatenate([sequence,acc,acc_g],axis=0)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据\n",
    "class DatasetLoader(object):\n",
    "    def __init__(self,csv_file,with_label=True,num_classes=19):\n",
    "        self.csv_file=csv_file\n",
    "        self.with_label=with_label\n",
    "        self.format=\"channel_last\"\n",
    "        self.split=False\n",
    "        self.names=self.get_feature_names()\n",
    "        self.num_classes=num_classes\n",
    "        self.data_split=False\n",
    "    def get_feature_names(self):\n",
    "         with open(self.csv_file) as f:\n",
    "            examples={}\n",
    "            names=f.readline().split(',')[1:]\n",
    "            names[-1]=names[-1][:-1]\n",
    "            return names\n",
    "    def make_numpy(self,num_interpolation=200,with_label=True):\n",
    "        '''将数据读取并保存为Numpy数组\n",
    "               Args:\n",
    "                 num_interpolation:差值法采样点个数\n",
    "                 with_label：是否带标签\n",
    "               Returns:\n",
    "                 A list,shape=[num_examples,keys,length]\n",
    "        '''\n",
    "        #数据读取\n",
    "        if self.csv_file is None:\n",
    "            raise ValueError(\"sub dataset cannnot get numpy data\")\n",
    "        print(\"Loading date...\")\n",
    "        line={}\n",
    "        with open(self.csv_file) as f:\n",
    "            examples={}\n",
    "            names=f.readline().split(',')[1:]\n",
    "            names[-1]=names[-1][:-1]\n",
    "            while True:\n",
    "                try:\n",
    "                    line=f.readline().split(\",\")\n",
    "                    if line is None:\n",
    "                        break\n",
    "                    for i in range(len(line)):\n",
    "                        line[i]=eval(line[i])\n",
    "                    if not line[0] in examples:\n",
    "                        examples[line[0]]=[]\n",
    "                    examples[line[0]].append(line[1:])\n",
    "                except:\n",
    "                    break\n",
    "        print(\"done\")\n",
    "        #格式转换            \n",
    "        for i in range(len(examples)):\n",
    "            examples[i]=np.array(examples[i]).transpose([1,0])\n",
    "        self.examples=examples=list(examples.values())\n",
    "        return self\n",
    "    def resample(self,num_interpolation=200):\n",
    "        examples=self.examples\n",
    "        print(\"interpolate\")\n",
    "        bar=Progbar(len(examples))#进度条\n",
    "        if num_interpolation and num_interpolation is not None:\n",
    "            for i in range(len(examples)):\n",
    "                range_len=examples[i][0][-1]-examples[i][0][0]\n",
    "                range_start=examples[i][0][0]\n",
    "                range_interval=range_len/num_interpolation\n",
    "                interp_x=[range_start+range_interval*i for i in range(num_interpolation)]\n",
    "                interp_data=[interp_x]\n",
    "                for feature_id in range(1,len(self.names)):\n",
    "                    try:\n",
    "                        interp_f=interp(examples[i][0],examples[i][feature_id],kind=\"cubic\")\n",
    "                        interp_data.append([interp_f(x)for x in interp_x])\n",
    "                    except:\n",
    "                        raise ValueError(\"%d %d\"%(i,feature_id),examples[i])\n",
    "                bar.update(i)\n",
    "                examples[i]=np.array(interp_data)\n",
    "        print(\"\\ndone\")\n",
    "        #数据预处理\n",
    "        preprocession=Preprocessing(with_label=self.with_label)\n",
    "        examples=preprocession(examples)\n",
    "        self.examples=np.array(examples,dtype=\"float32\")    \n",
    "        if self.with_label:\n",
    "            self.y=self.examples[::,-1,0].tolist()\n",
    "            self.x=self.examples[::,1:-1,::]\n",
    "        else:\n",
    "            self.x=self.examples[::,1:,::]\n",
    "        return self\n",
    "    def apply_class_weights(self):\n",
    "        weights=np.zeros([self.num_classes])\n",
    "        for i in range(len(weights)):\n",
    "            weights[i]=(self.examples[::,-1:,0]==i).sum()\n",
    "        return weights/weights.sum()\n",
    "    def data_format(string=\"channel_last\"):\n",
    "        if not string in [\"channel_first\",\"channel_last\"]:\n",
    "            raise ValueError(\"either channel_last or channel_first are supported\")\n",
    "        self.format=string\n",
    "    def apply_data(self):\n",
    "        if self.with_label:\n",
    "            if self.split:\n",
    "                if self.format==\"channel_first\":\n",
    "                    return self.x_train,self.y_train,self.x_test,self.y_test\n",
    "                else:\n",
    "                    return self.x_train.transpose([0,2,1]),self.y_train,self.x_test.transpose([0,2,1]),self.y_test\n",
    "                    \n",
    "            else:\n",
    "                if self.format==\"channel_first\":\n",
    "                    return self.x,self.y\n",
    "                else:\n",
    "                    return self.x.transpose([0,2,1]),self.y\n",
    "        else:\n",
    "            if self.format==\"channel_first\":\n",
    "                return self.x\n",
    "            else:\n",
    "                return self.x.transpose([0,2,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "def single_score(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "def py_score(y_true,y_pred):\n",
    "    y_true=np.argmax(y_true,axis=-1)\n",
    "    y_pred=np.argmax(y_pred,axis=-1)\n",
    "    scores=[]\n",
    "    for i in range(len(y_true)):\n",
    "        scores.append(single_score(y_true[i],y_pred[i]))\n",
    "    mean_score=np.array(scores,dtype=\"float32\").mean()\n",
    "    return mean_score,mean_score\n",
    "def score(y_true,y_pred):\n",
    "    \"\"\"线上评测所使用的评测方法\n",
    "    Args:\n",
    "      y_true:one_hot编码的标签\n",
    "      y_pred:网络类别置信度预测\n",
    "    Returns:\n",
    "      Tensor标量\n",
    "    \"\"\"\n",
    "    mean_score=tf.py_func(py_score,[y_true,y_pred],[tf.float32,tf.float32])[0]\n",
    "    return tf.reshape(mean_score,shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#网络结构\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from extend import WeightDecayScheduler,AdamW\n",
    "class SoftThreshold(tf.keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(SoftThreshold,self).__init__(**kwargs)\n",
    "    def call(self,inputs):\n",
    "        tensor=inputs[0]\n",
    "        threshold=inputs[0]*inputs[1]\n",
    "        threshold=tf.abs(threshold)\n",
    "        less=tf.cast(tf.less(tensor,-threshold),tf.float32)\n",
    "        greater=tf.cast(tf.greater(tensor,threshold),tf.float32)\n",
    "        tensor=(tensor-threshold)*greater+(tensor+threshold)*less\n",
    "        return tensor\n",
    "class SKNet(object):\n",
    "    def __init__(self,filters,kernel_size,activation,dilation_rates=[1,1,1,1]):\n",
    "        self.filters=filters\n",
    "        self.kernel_size=kernel_size\n",
    "        self.dilation_rates=dilation_rates\n",
    "        self.activation=activation\n",
    "        self.sk_rate=8\n",
    "    def branch(self,tensor,dilation_rate):\n",
    "        x=Conv2D(self.filters,self.kernel_size,1,padding=\"same\",kernel_regularizer=l2(0.00),\n",
    "                 activation=self.activation,dilation_rate=dilation_rate)(tensor)\n",
    "        x=BatchNormalization()(x)\n",
    "        x=Activation(self.activation)(x)\n",
    "        return x\n",
    "    def attention(self,tensor):\n",
    "        add=tf.keras.layers.add(tensor)\n",
    "        shape=add.shape.as_list()\n",
    "        squeeze=GlobalAveragePooling2D()(add)\n",
    "        squeeze=Dense(shape[-1]//self.sk_rate,activation=tf.nn.relu)(squeeze)\n",
    "        extract=Dense(shape[-1],activation=tf.nn.softmax)(squeeze)\n",
    "        extract=tf.expand_dims(extract,axis=1)\n",
    "        extract=tf.expand_dims(extract,axis=1)\n",
    "        output=[]\n",
    "        for t in tensor:\n",
    "            output.append(SoftThreshold()([t,extract]))\n",
    "        return output\n",
    "    def pool(self,tensor):\n",
    "        tensor=Conv2D(self.filters,self.kernel_size,1,padding=\"same\",kernel_regularizer=l2(0.00))(tensor)\n",
    "        shape=tensor.shape.as_list()#[batch,lenth,channels]\n",
    "        scaler=AveragePooling2D((shape[1],shape[2]))(tensor)#[batch,channels]\n",
    "        scaler=UpSampling2D((shape[1],shape[2]))(scaler)\n",
    "        return scaler\n",
    "    def __call__(self,tensor):\n",
    "        x=Conv2D(self.filters,1,1,padding=\"same\",\n",
    "                 activation=self.activation,kernel_regularizer=l2(0.00))(tensor)\n",
    "        x=BatchNormalization()(x)\n",
    "        x=Activation(None)(x)\n",
    "        output=[x]\n",
    "        output.append(self.pool(tensor))\n",
    "        for rate in self.dilation_rates:\n",
    "            output.append(self.branch(tensor,rate))\n",
    "        output=self.attention(output)\n",
    "        output=tf.keras.layers.add(output)\n",
    "        output=Conv2D(self.filters,3,1,padding=\"same\",kernel_regularizer=l2(0.00))(output)\n",
    "        output=BatchNormalization()(output)\n",
    "        output=Activation(self.activation)(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shijunfeng\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "def CNN(inputs,num_classes):\n",
    "    x=Conv2D(32,3,1,padding=\"same\",kernel_regularizer=l2(0.00))(inputs)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation(tf.nn.relu)(x)\n",
    "    \n",
    "    x=Conv2D(64,3,1,padding=\"same\",kernel_regularizer=l2(0.00))(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation(tf.nn.relu)(x)\n",
    "    \n",
    "    x=Conv2D(128,3,1,padding=\"same\",kernel_regularizer=l2(0.00))(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation(tf.nn.relu)(x)\n",
    "    \n",
    "    x=MaxPooling2D(2,padding=\"same\")(x)\n",
    "    x=Dropout(0.5)(x)\n",
    "    \n",
    "    x=Conv2D(192,3,1,padding=\"same\",kernel_regularizer=l2(0.00))(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation(tf.nn.relu)(x)\n",
    "    x=Dropout(0.5)(x)\n",
    "    \n",
    "    x=SKNet(384,3,activation=tf.nn.relu)(x)\n",
    "    \n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "    x=Dropout(0.5)(x)\n",
    "\n",
    "    x=Dense(num_classes,kernel_regularizer=l2(0))(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Softmax()(x)\n",
    "    return x\n",
    "#inputs=Input(shape=[64,8,1])\n",
    "#outputs=CNN(inputs,num_classes=19)\n",
    "#tf.keras.utils.plot_model(Model(inputs,outputs),show_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading date...\n",
      "done\n",
      "interpolate\n",
      "7288/7292 [============================>.] - ETA: 0s\n",
      "done\n",
      "Loading date...\n",
      "done\n",
      "interpolate\n",
      "7495/7500 [============================>.] - ETA: 0s\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7292, 64, 8, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载数据\n",
    "tf.reset_default_graph()\n",
    "csv_file=\"data\\\\sensor_train.csv\"\n",
    "test_csv_file=\"data\\\\sensor_test.csv\"\n",
    "filepath='best_weights_aspp_raw'\n",
    "batch_size=64\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess=tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "if True:\n",
    "    dataset=DatasetLoader(csv_file,with_label=True,num_classes=19)\n",
    "    dataset=dataset.make_numpy()\n",
    "    #dataset=dataset.reverse()\n",
    "    dataset=dataset.resample(num_interpolation=64)\n",
    "    x,y=dataset.apply_data()\n",
    "    class_weight=dataset.apply_class_weights()\n",
    "    dataset=DatasetLoader(test_csv_file,with_label=False)\n",
    "    dataset=dataset.make_numpy()\n",
    "    #dataset=dataset.reverse()\n",
    "    dataset=dataset.resample(num_interpolation=64)\n",
    "    x_val=dataset.apply_data()\n",
    "    x=np.expand_dims(x,axis=-1)\n",
    "    x_val=np.expand_dims(x_val,axis=-1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-6214e7eb0d92>:38: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "Train on 5825 samples, validate on 1467 samples\n",
      "0 1.0000000474974513\n",
      "Epoch 1/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 2.2133 - acc: 0.2806 - score: 0.3701\n",
      "Epoch 00001: val_score improved from -inf to 0.20179, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 11s 2ms/sample - loss: 2.2147 - acc: 0.2798 - score: 0.3655 - val_loss: 4.2331 - val_acc: 0.1009 - val_score: 0.2018\n",
      "Epoch 2/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.9136 - acc: 0.3982 - score: 0.4782\n",
      "Epoch 00002: val_score improved from 0.20179 to 0.25114, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 6s 968us/sample - loss: 1.9138 - acc: 0.3981 - score: 0.4730 - val_loss: 2.8175 - val_acc: 0.1486 - val_score: 0.2511\n",
      "Epoch 3/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.7957 - acc: 0.4360 - score: 0.5150\n",
      "Epoch 00003: val_score improved from 0.25114 to 0.40309, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 6s 988us/sample - loss: 1.7959 - acc: 0.4359 - score: 0.5094 - val_loss: 2.1802 - val_acc: 0.3115 - val_score: 0.4031\n",
      "Epoch 4/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.7105 - acc: 0.4662 - score: 0.5417\n",
      "Epoch 00004: val_score improved from 0.40309 to 0.50827, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 6s 957us/sample - loss: 1.7107 - acc: 0.4661 - score: 0.5358 - val_loss: 1.8213 - val_acc: 0.4240 - val_score: 0.5083\n",
      "Epoch 5/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.6513 - acc: 0.4854 - score: 0.5554\n",
      "Epoch 00005: val_score did not improve from 0.50827\n",
      "5825/5825 [==============================] - 6s 953us/sample - loss: 1.6515 - acc: 0.4855 - score: 0.5602 - val_loss: 2.0542 - val_acc: 0.3170 - val_score: 0.3846\n",
      "Epoch 6/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.6165 - acc: 0.4974 - score: 0.5700\n",
      "Epoch 00006: val_score improved from 0.50827 to 0.52797, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 6s 991us/sample - loss: 1.6168 - acc: 0.4973 - score: 0.5638 - val_loss: 1.7849 - val_acc: 0.4560 - val_score: 0.5280\n",
      "Epoch 7/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.5897 - acc: 0.5079 - score: 0.5795\n",
      "Epoch 00007: val_score improved from 0.52797 to 0.53803, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 6s 983us/sample - loss: 1.5899 - acc: 0.5078 - score: 0.5732 - val_loss: 1.6777 - val_acc: 0.4533 - val_score: 0.5380\n",
      "Epoch 8/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.5477 - acc: 0.5189 - score: 0.5876\n",
      "Epoch 00008: val_score did not improve from 0.53803\n",
      "5825/5825 [==============================] - 6s 958us/sample - loss: 1.5480 - acc: 0.5188 - score: 0.5812 - val_loss: 1.7561 - val_acc: 0.4554 - val_score: 0.5246\n",
      "Epoch 9/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.5336 - acc: 0.5282 - score: 0.5949\n",
      "Epoch 00009: val_score did not improve from 0.53803\n",
      "5825/5825 [==============================] - 6s 967us/sample - loss: 1.5339 - acc: 0.5281 - score: 0.5884 - val_loss: 1.7122 - val_acc: 0.4526 - val_score: 0.5164\n",
      "Epoch 10/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.4838 - acc: 0.5421 - score: 0.6077\n",
      "Epoch 00010: val_score did not improve from 0.53803\n",
      "5825/5825 [==============================] - 6s 961us/sample - loss: 1.4841 - acc: 0.5420 - score: 0.6026 - val_loss: 1.7242 - val_acc: 0.4628 - val_score: 0.5347\n",
      "Epoch 11/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.4733 - acc: 0.5482 - score: 0.6142\n",
      "Epoch 00011: val_score did not improve from 0.53803\n",
      "5825/5825 [==============================] - 6s 968us/sample - loss: 1.4735 - acc: 0.5482 - score: 0.6075 - val_loss: 1.8518 - val_acc: 0.4410 - val_score: 0.5158\n",
      "Epoch 12/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.4288 - acc: 0.5647 - score: 0.6263\n",
      "Epoch 00012: val_score did not improve from 0.53803\n",
      "5825/5825 [==============================] - 6s 962us/sample - loss: 1.4290 - acc: 0.5646 - score: 0.6210 - val_loss: 2.0790 - val_acc: 0.3763 - val_score: 0.4475\n",
      "Epoch 13/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.4195 - acc: 0.5611 - score: 0.6244\n",
      "Epoch 00013: val_score improved from 0.53803 to 0.57215, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 6s 990us/sample - loss: 1.4197 - acc: 0.5610 - score: 0.6213 - val_loss: 1.5694 - val_acc: 0.5031 - val_score: 0.5721\n",
      "Epoch 14/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.4010 - acc: 0.5656 - score: 0.6281- ETA\n",
      "Epoch 00014: val_score did not improve from 0.57215\n",
      "5825/5825 [==============================] - 6s 958us/sample - loss: 1.4012 - acc: 0.5655 - score: 0.6212 - val_loss: 1.7799 - val_acc: 0.4438 - val_score: 0.5121\n",
      "Epoch 15/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.3684 - acc: 0.5816 - score: 0.6433\n",
      "Epoch 00015: val_score did not improve from 0.57215\n",
      "5825/5825 [==============================] - 5s 899us/sample - loss: 1.3687 - acc: 0.5815 - score: 0.6363 - val_loss: 1.7454 - val_acc: 0.4683 - val_score: 0.5416\n",
      "Epoch 16/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.3642 - acc: 0.5872 - score: 0.6444\n",
      "Epoch 00016: val_score improved from 0.57215 to 0.61619, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 922us/sample - loss: 1.3644 - acc: 0.5871 - score: 0.6410 - val_loss: 1.5312 - val_acc: 0.5556 - val_score: 0.6162\n",
      "Epoch 17/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.3128 - acc: 0.6042 - score: 0.6606\n",
      "Epoch 00017: val_score did not improve from 0.61619\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 1.3128 - acc: 0.6048 - score: 0.6576 - val_loss: 2.2785 - val_acc: 0.3620 - val_score: 0.4438\n",
      "Epoch 18/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.2963 - acc: 0.6107 - score: 0.6672\n",
      "Epoch 00018: val_score did not improve from 0.61619\n",
      "5825/5825 [==============================] - 5s 894us/sample - loss: 1.2966 - acc: 0.6106 - score: 0.6615 - val_loss: 1.5231 - val_acc: 0.5365 - val_score: 0.6022\n",
      "Epoch 19/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.2806 - acc: 0.6149 - score: 0.6691\n",
      "Epoch 00019: val_score improved from 0.61619 to 0.63260, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 919us/sample - loss: 1.2810 - acc: 0.6146 - score: 0.6617 - val_loss: 1.3857 - val_acc: 0.5678 - val_score: 0.6326\n",
      "Epoch 20/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.2546 - acc: 0.6241 - score: 0.6786\n",
      "Epoch 00020: val_score did not improve from 0.63260\n",
      "5825/5825 [==============================] - 5s 873us/sample - loss: 1.2549 - acc: 0.6240 - score: 0.6749 - val_loss: 3.4870 - val_acc: 0.2597 - val_score: 0.3482\n",
      "Epoch 21/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.2272 - acc: 0.6403 - score: 0.6909\n",
      "Epoch 00021: val_score did not improve from 0.63260\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 1.2270 - acc: 0.6405 - score: 0.6837 - val_loss: 1.9345 - val_acc: 0.4185 - val_score: 0.5039\n",
      "Epoch 22/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.2134 - acc: 0.6410 - score: 0.6916\n",
      "Epoch 00022: val_score did not improve from 0.63260\n",
      "5825/5825 [==============================] - 5s 894us/sample - loss: 1.2137 - acc: 0.6409 - score: 0.6840 - val_loss: 1.7524 - val_acc: 0.4431 - val_score: 0.5213\n",
      "Epoch 23/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.1971 - acc: 0.6458 - score: 0.6964\n",
      "Epoch 00023: val_score did not improve from 0.63260\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 1.1974 - acc: 0.6457 - score: 0.6889 - val_loss: 1.6033 - val_acc: 0.4799 - val_score: 0.5582\n",
      "Epoch 24/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.1673 - acc: 0.6600 - score: 0.7089\n",
      "Epoch 00024: val_score did not improve from 0.63260\n",
      "5825/5825 [==============================] - 5s 896us/sample - loss: 1.1677 - acc: 0.6599 - score: 0.7012 - val_loss: 2.1373 - val_acc: 0.3681 - val_score: 0.4487\n",
      "Epoch 25/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.1525 - acc: 0.6609 - score: 0.7106\n",
      "Epoch 00025: val_score did not improve from 0.63260\n",
      "5825/5825 [==============================] - 5s 882us/sample - loss: 1.1504 - acc: 0.6618 - score: 0.7053 - val_loss: 2.8428 - val_acc: 0.3456 - val_score: 0.4177\n",
      "Epoch 26/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.1257 - acc: 0.6741 - score: 0.7216\n",
      "Epoch 00026: val_score improved from 0.63260 to 0.65505, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 912us/sample - loss: 1.1260 - acc: 0.6740 - score: 0.7138 - val_loss: 1.3673 - val_acc: 0.5910 - val_score: 0.6551\n",
      "Epoch 27/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.1082 - acc: 0.6833 - score: 0.7284\n",
      "Epoch 00027: val_score did not improve from 0.65505\n",
      "5825/5825 [==============================] - 5s 890us/sample - loss: 1.1080 - acc: 0.6833 - score: 0.7221 - val_loss: 1.7505 - val_acc: 0.4908 - val_score: 0.5563\n",
      "Epoch 28/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.1021 - acc: 0.6798 - score: 0.7273\n",
      "Epoch 00028: val_score did not improve from 0.65505\n",
      "5825/5825 [==============================] - 5s 890us/sample - loss: 1.1024 - acc: 0.6797 - score: 0.7210 - val_loss: 1.6093 - val_acc: 0.4881 - val_score: 0.5555\n",
      "Epoch 29/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.0765 - acc: 0.6935 - score: 0.7383\n",
      "Epoch 00029: val_score did not improve from 0.65505\n",
      "5825/5825 [==============================] - 5s 890us/sample - loss: 1.0768 - acc: 0.6934 - score: 0.7319 - val_loss: 2.3583 - val_acc: 0.3776 - val_score: 0.4624\n",
      "Epoch 30/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.0746 - acc: 0.6932 - score: 0.7383\n",
      "Epoch 00030: val_score improved from 0.65505 to 0.72376, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 906us/sample - loss: 1.0757 - acc: 0.6929 - score: 0.7314 - val_loss: 1.1006 - val_acc: 0.6748 - val_score: 0.7238\n",
      "Epoch 31/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.0384 - acc: 0.6957 - score: 0.7405\n",
      "Epoch 00031: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 883us/sample - loss: 1.0381 - acc: 0.6963 - score: 0.7331 - val_loss: 2.1331 - val_acc: 0.4417 - val_score: 0.5168\n",
      "Epoch 32/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.0401 - acc: 0.7030 - score: 0.7458\n",
      "Epoch 00032: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 1.0404 - acc: 0.7028 - score: 0.7414 - val_loss: 1.7815 - val_acc: 0.4915 - val_score: 0.5559\n",
      "Epoch 33/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.0249 - acc: 0.7057 - score: 0.7476\n",
      "Epoch 00033: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 932us/sample - loss: 1.0265 - acc: 0.7061 - score: 0.7506 - val_loss: 1.6735 - val_acc: 0.4922 - val_score: 0.5664\n",
      "Epoch 34/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.0011 - acc: 0.7145 - score: 0.7565\n",
      "Epoch 00034: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 913us/sample - loss: 1.0015 - acc: 0.7143 - score: 0.7483 - val_loss: 1.8586 - val_acc: 0.4669 - val_score: 0.5433\n",
      "Epoch 35/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.9912 - acc: 0.7174 - score: 0.7594\n",
      "Epoch 00035: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 885us/sample - loss: 0.9915 - acc: 0.7173 - score: 0.7547 - val_loss: 2.8562 - val_acc: 0.3183 - val_score: 0.4064\n",
      "Epoch 36/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.9830 - acc: 0.7217 - score: 0.7624\n",
      "Epoch 00036: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.9830 - acc: 0.7214 - score: 0.7554 - val_loss: 2.7232 - val_acc: 0.3374 - val_score: 0.4196\n",
      "Epoch 37/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.9726 - acc: 0.7299 - score: 0.7705- ETA: 2s - loss:\n",
      "Epoch 00037: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 883us/sample - loss: 0.9729 - acc: 0.7298 - score: 0.7657 - val_loss: 1.5834 - val_acc: 0.5337 - val_score: 0.5946\n",
      "Epoch 38/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.9690 - acc: 0.7234 - score: 0.7654\n",
      "Epoch 00038: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 889us/sample - loss: 0.9693 - acc: 0.7233 - score: 0.7571 - val_loss: 1.6904 - val_acc: 0.4976 - val_score: 0.5692\n",
      "Epoch 39/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.9458 - acc: 0.7424 - score: 0.7796\n",
      "Epoch 00039: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 886us/sample - loss: 0.9460 - acc: 0.7423 - score: 0.7747 - val_loss: 1.5654 - val_acc: 0.5474 - val_score: 0.6042\n",
      "Epoch 40/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.9478 - acc: 0.7368 - score: 0.7765\n",
      "Epoch 00040: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.9481 - acc: 0.7367 - score: 0.7696 - val_loss: 1.1098 - val_acc: 0.6646 - val_score: 0.7127\n",
      "Epoch 41/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.9198 - acc: 0.7507 - score: 0.7877\n",
      "Epoch 00041: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 0.9201 - acc: 0.7506 - score: 0.7828 - val_loss: 1.9755 - val_acc: 0.4547 - val_score: 0.5348\n",
      "Epoch 42/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.9278 - acc: 0.7477 - score: 0.7843\n",
      "Epoch 00042: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.9286 - acc: 0.7475 - score: 0.7756 - val_loss: 2.1322 - val_acc: 0.4594 - val_score: 0.5309\n",
      "Epoch 43/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.9338 - acc: 0.7426 - score: 0.7819\n",
      "Epoch 00043: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 885us/sample - loss: 0.9342 - acc: 0.7425 - score: 0.7749 - val_loss: 1.5958 - val_acc: 0.5269 - val_score: 0.5925\n",
      "Epoch 44/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.8999 - acc: 0.7573 - score: 0.7935\n",
      "Epoch 00044: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 887us/sample - loss: 0.8984 - acc: 0.7574 - score: 0.7851 - val_loss: 1.8830 - val_acc: 0.5037 - val_score: 0.5666\n",
      "Epoch 45/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.8782 - acc: 0.7649 - score: 0.7989\n",
      "Epoch 00045: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 894us/sample - loss: 0.8785 - acc: 0.7648 - score: 0.7938 - val_loss: 1.7421 - val_acc: 0.5044 - val_score: 0.5742\n",
      "Epoch 46/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.8962 - acc: 0.7575 - score: 0.7932\n",
      "Epoch 00046: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 0.8963 - acc: 0.7573 - score: 0.7844 - val_loss: 2.3329 - val_acc: 0.4247 - val_score: 0.4918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.8846 - acc: 0.7617 - score: 0.7972\n",
      "Epoch 00047: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 885us/sample - loss: 0.8850 - acc: 0.7615 - score: 0.7886 - val_loss: 2.1455 - val_acc: 0.4724 - val_score: 0.5421\n",
      "Epoch 48/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.8804 - acc: 0.7579 - score: 0.7942\n",
      "Epoch 00048: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.8808 - acc: 0.7578 - score: 0.7855 - val_loss: 1.9462 - val_acc: 0.4581 - val_score: 0.5285\n",
      "Epoch 49/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.8751 - acc: 0.7670 - score: 0.8008\n",
      "Epoch 00049: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.8755 - acc: 0.7669 - score: 0.7921 - val_loss: 1.2476 - val_acc: 0.6292 - val_score: 0.6803\n",
      "Epoch 50/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.8540 - acc: 0.7735 - score: 0.8068\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00050: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 899us/sample - loss: 0.8543 - acc: 0.7734 - score: 0.8017 - val_loss: 2.0388 - val_acc: 0.4513 - val_score: 0.5086\n",
      "50 0.4\n",
      "Epoch 51/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.7367 - acc: 0.8189 - score: 0.8450\n",
      "Epoch 00051: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 887us/sample - loss: 0.7356 - acc: 0.8191 - score: 0.8361 - val_loss: 1.5402 - val_acc: 0.5637 - val_score: 0.6227\n",
      "Epoch 52/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.7002 - acc: 0.8367 - score: 0.8619\n",
      "Epoch 00052: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 885us/sample - loss: 0.7005 - acc: 0.8367 - score: 0.8634 - val_loss: 1.6959 - val_acc: 0.5269 - val_score: 0.5844\n",
      "Epoch 53/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.6843 - acc: 0.8408 - score: 0.8642\n",
      "Epoch 00053: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 896us/sample - loss: 0.6856 - acc: 0.8403 - score: 0.8544 - val_loss: 2.0724 - val_acc: 0.4778 - val_score: 0.5414\n",
      "Epoch 54/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.6703 - acc: 0.8438 - score: 0.8680\n",
      "Epoch 00054: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 0.6707 - acc: 0.8436 - score: 0.8585 - val_loss: 1.6892 - val_acc: 0.5501 - val_score: 0.6103\n",
      "Epoch 55/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.6585 - acc: 0.8513 - score: 0.8739\n",
      "Epoch 00055: val_score did not improve from 0.72376\n",
      "5825/5825 [==============================] - 5s 884us/sample - loss: 0.6588 - acc: 0.8512 - score: 0.8681 - val_loss: 2.1672 - val_acc: 0.4867 - val_score: 0.5515\n",
      "Epoch 56/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.8520 - score: 0.8750\n",
      "Epoch 00056: val_score improved from 0.72376 to 0.80979, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 906us/sample - loss: 0.6601 - acc: 0.8518 - score: 0.8654 - val_loss: 0.7955 - val_acc: 0.7778 - val_score: 0.8098\n",
      "Epoch 57/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.6647 - acc: 0.8446 - score: 0.8678\n",
      "Epoch 00057: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 872us/sample - loss: 0.6669 - acc: 0.8438 - score: 0.8578 - val_loss: 0.9494 - val_acc: 0.7328 - val_score: 0.7679\n",
      "Epoch 58/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.6335 - acc: 0.8578 - score: 0.8786\n",
      "Epoch 00058: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 882us/sample - loss: 0.6339 - acc: 0.8577 - score: 0.8691 - val_loss: 1.9311 - val_acc: 0.5337 - val_score: 0.5941\n",
      "Epoch 59/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.6396 - acc: 0.8521 - score: 0.8742\n",
      "Epoch 00059: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 901us/sample - loss: 0.6380 - acc: 0.8525 - score: 0.8668 - val_loss: 0.8941 - val_acc: 0.7491 - val_score: 0.7857\n",
      "Epoch 60/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.6203 - acc: 0.8655 - score: 0.8850\n",
      "Epoch 00060: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 885us/sample - loss: 0.6191 - acc: 0.8663 - score: 0.8777 - val_loss: 1.9658 - val_acc: 0.5262 - val_score: 0.5846\n",
      "Epoch 61/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.6037 - acc: 0.8692 - score: 0.8896\n",
      "Epoch 00061: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 889us/sample - loss: 0.6041 - acc: 0.8690 - score: 0.8815 - val_loss: 1.4174 - val_acc: 0.6210 - val_score: 0.6704\n",
      "Epoch 62/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.6243 - acc: 0.8652 - score: 0.8857\n",
      "Epoch 00062: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.6247 - acc: 0.8651 - score: 0.8761 - val_loss: 1.7403 - val_acc: 0.5453 - val_score: 0.6044\n",
      "Epoch 63/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5948 - acc: 0.8666 - score: 0.8861\n",
      "Epoch 00063: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.5952 - acc: 0.8664 - score: 0.8780 - val_loss: 1.2705 - val_acc: 0.6496 - val_score: 0.6981\n",
      "Epoch 64/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5879 - acc: 0.8716 - score: 0.8908\n",
      "Epoch 00064: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 883us/sample - loss: 0.5884 - acc: 0.8714 - score: 0.8811 - val_loss: 1.3559 - val_acc: 0.6203 - val_score: 0.6723\n",
      "Epoch 65/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5866 - acc: 0.8748 - score: 0.8941\n",
      "Epoch 00065: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 886us/sample - loss: 0.5870 - acc: 0.8747 - score: 0.8843 - val_loss: 1.6485 - val_acc: 0.5944 - val_score: 0.6480\n",
      "Epoch 66/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5883 - acc: 0.8747 - score: 0.8934\n",
      "Epoch 00066: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 900us/sample - loss: 0.5888 - acc: 0.8745 - score: 0.8837 - val_loss: 1.0579 - val_acc: 0.6946 - val_score: 0.7354\n",
      "Epoch 67/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.5847 - acc: 0.8717 - score: 0.8908\n",
      "Epoch 00067: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 890us/sample - loss: 0.5847 - acc: 0.8716 - score: 0.8812 - val_loss: 1.4216 - val_acc: 0.6237 - val_score: 0.6716\n",
      "Epoch 68/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5805 - acc: 0.8781 - score: 0.8961\n",
      "Epoch 00068: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 889us/sample - loss: 0.5809 - acc: 0.8779 - score: 0.8864 - val_loss: 1.5836 - val_acc: 0.5801 - val_score: 0.6334\n",
      "Epoch 69/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5833 - acc: 0.8786 - score: 0.8963\n",
      "Epoch 00069: val_score did not improve from 0.80979\n",
      "5825/5825 [==============================] - 5s 880us/sample - loss: 0.5837 - acc: 0.8785 - score: 0.8902 - val_loss: 0.7595 - val_acc: 0.7778 - val_score: 0.8082\n",
      "Epoch 70/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5781 - acc: 0.8771 - score: 0.8951\n",
      "Epoch 00070: val_score improved from 0.80979 to 0.81855, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 916us/sample - loss: 0.5785 - acc: 0.8769 - score: 0.8869 - val_loss: 0.7445 - val_acc: 0.7914 - val_score: 0.8185\n",
      "Epoch 71/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5545 - acc: 0.8882 - score: 0.9054\n",
      "Epoch 00071: val_score did not improve from 0.81855\n",
      "5825/5825 [==============================] - 5s 879us/sample - loss: 0.5549 - acc: 0.8881 - score: 0.8991 - val_loss: 1.0117 - val_acc: 0.7198 - val_score: 0.7571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5545 - acc: 0.8896 - score: 0.9067\n",
      "Epoch 00072: val_score did not improve from 0.81855\n",
      "5825/5825 [==============================] - 5s 885us/sample - loss: 0.5549 - acc: 0.8894 - score: 0.8984 - val_loss: 0.9069 - val_acc: 0.7396 - val_score: 0.7745\n",
      "Epoch 73/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.8959 - score: 0.9117\n",
      "Epoch 00073: val_score did not improve from 0.81855\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.5338 - acc: 0.8960 - score: 0.9127 - val_loss: 1.2709 - val_acc: 0.6558 - val_score: 0.7007\n",
      "Epoch 74/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.8935 - score: 0.9095\n",
      "Epoch 00074: val_score did not improve from 0.81855\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.5314 - acc: 0.8934 - score: 0.9011 - val_loss: 1.5709 - val_acc: 0.5808 - val_score: 0.6334\n",
      "Epoch 75/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.8941 - score: 0.9107\n",
      "Epoch 00075: val_score did not improve from 0.81855\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.5273 - acc: 0.8939 - score: 0.9008 - val_loss: 1.1509 - val_acc: 0.6735 - val_score: 0.7188\n",
      "Epoch 76/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8930 - score: 0.9094\n",
      "Epoch 00076: val_score did not improve from 0.81855\n",
      "5825/5825 [==============================] - 5s 889us/sample - loss: 0.5378 - acc: 0.8930 - score: 0.9104 - val_loss: 0.9293 - val_acc: 0.7396 - val_score: 0.7761\n",
      "Epoch 77/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5461 - acc: 0.8889 - score: 0.9056\n",
      "Epoch 00077: val_score did not improve from 0.81855\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.5464 - acc: 0.8888 - score: 0.8994 - val_loss: 1.9568 - val_acc: 0.5501 - val_score: 0.6084\n",
      "Epoch 78/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.8987 - score: 0.9138\n",
      "Epoch 00078: val_score did not improve from 0.81855\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.5261 - acc: 0.8985 - score: 0.9054 - val_loss: 1.0434 - val_acc: 0.7048 - val_score: 0.7450\n",
      "Epoch 79/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.8936 - score: 0.9095\n",
      "Epoch 00079: val_score did not improve from 0.81855\n",
      "5825/5825 [==============================] - 5s 890us/sample - loss: 0.5310 - acc: 0.8939 - score: 0.9108 - val_loss: 0.8351 - val_acc: 0.7710 - val_score: 0.8026\n",
      "Epoch 80/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.8977 - score: 0.9123\n",
      "Epoch 00080: val_score improved from 0.81855 to 0.83301, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 906us/sample - loss: 0.5307 - acc: 0.8975 - score: 0.9023 - val_loss: 0.7187 - val_acc: 0.8030 - val_score: 0.8330\n",
      "Epoch 81/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.8956 - score: 0.9116\n",
      "Epoch 00081: val_score did not improve from 0.83301\n",
      "5825/5825 [==============================] - 5s 871us/sample - loss: 0.5217 - acc: 0.8955 - score: 0.9053 - val_loss: 1.8712 - val_acc: 0.5392 - val_score: 0.5982\n",
      "Epoch 82/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4975 - acc: 0.9047 - score: 0.9187\n",
      "Epoch 00082: val_score did not improve from 0.83301\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.4979 - acc: 0.9045 - score: 0.9103 - val_loss: 1.5422 - val_acc: 0.6230 - val_score: 0.6749\n",
      "Epoch 83/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5158 - acc: 0.8961 - score: 0.9120\n",
      "Epoch 00083: val_score did not improve from 0.83301\n",
      "5825/5825 [==============================] - 5s 881us/sample - loss: 0.5161 - acc: 0.8961 - score: 0.9130 - val_loss: 1.8055 - val_acc: 0.5590 - val_score: 0.6142\n",
      "Epoch 84/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.9087 - score: 0.9219\n",
      "Epoch 00084: val_score did not improve from 0.83301\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.4931 - acc: 0.9085 - score: 0.9134 - val_loss: 0.8539 - val_acc: 0.7628 - val_score: 0.7989\n",
      "Epoch 85/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5062 - acc: 0.9033 - score: 0.9178\n",
      "Epoch 00085: val_score did not improve from 0.83301\n",
      "5825/5825 [==============================] - 5s 885us/sample - loss: 0.5066 - acc: 0.9032 - score: 0.9094 - val_loss: 0.8596 - val_acc: 0.7444 - val_score: 0.7821\n",
      "Epoch 86/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.9052 - score: 0.9201\n",
      "Epoch 00086: val_score did not improve from 0.83301\n",
      "5825/5825 [==============================] - 5s 896us/sample - loss: 0.5040 - acc: 0.9051 - score: 0.9117 - val_loss: 1.3360 - val_acc: 0.6319 - val_score: 0.6810\n",
      "Epoch 87/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.9032 - score: 0.9184\n",
      "Epoch 00087: val_score improved from 0.83301 to 0.83320, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 910us/sample - loss: 0.5111 - acc: 0.9030 - score: 0.9099 - val_loss: 0.6868 - val_acc: 0.8044 - val_score: 0.8332\n",
      "Epoch 88/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.9052 - score: 0.9192\n",
      "Epoch 00088: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 885us/sample - loss: 0.4990 - acc: 0.9052 - score: 0.9201 - val_loss: 0.8729 - val_acc: 0.7587 - val_score: 0.7914\n",
      "Epoch 89/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4779 - acc: 0.9116 - score: 0.9253\n",
      "Epoch 00089: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 882us/sample - loss: 0.4784 - acc: 0.9114 - score: 0.9152 - val_loss: 1.5647 - val_acc: 0.5890 - val_score: 0.6449\n",
      "Epoch 90/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4840 - acc: 0.9116 - score: 0.9253\n",
      "Epoch 00090: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 0.4845 - acc: 0.9114 - score: 0.9153 - val_loss: 1.3964 - val_acc: 0.6421 - val_score: 0.6917\n",
      "Epoch 91/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4703 - acc: 0.9135 - score: 0.9272\n",
      "Epoch 00091: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 889us/sample - loss: 0.4707 - acc: 0.9133 - score: 0.9172 - val_loss: 1.1010 - val_acc: 0.6864 - val_score: 0.7350\n",
      "Epoch 92/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4754 - acc: 0.9100 - score: 0.9236\n",
      "Epoch 00092: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 901us/sample - loss: 0.4759 - acc: 0.9099 - score: 0.9136 - val_loss: 1.2630 - val_acc: 0.6449 - val_score: 0.6925\n",
      "Epoch 93/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4931 - acc: 0.9042 - score: 0.9196\n",
      "Epoch 00093: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 890us/sample - loss: 0.4936 - acc: 0.9040 - score: 0.9096 - val_loss: 1.1908 - val_acc: 0.6769 - val_score: 0.7195\n",
      "Epoch 94/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.9040 - score: 0.9190\n",
      "Epoch 00094: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.4904 - acc: 0.9039 - score: 0.9105 - val_loss: 1.3782 - val_acc: 0.6319 - val_score: 0.6829\n",
      "Epoch 95/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4813 - acc: 0.9147 - score: 0.9271\n",
      "Epoch 00095: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 900us/sample - loss: 0.4817 - acc: 0.9145 - score: 0.9206 - val_loss: 2.6019 - val_acc: 0.4485 - val_score: 0.5107\n",
      "Epoch 96/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4722 - acc: 0.9081 - score: 0.9220\n",
      "Epoch 00096: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 885us/sample - loss: 0.4727 - acc: 0.9080 - score: 0.9135 - val_loss: 1.4640 - val_acc: 0.6074 - val_score: 0.6585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5064 - acc: 0.8985 - score: 0.9146\n",
      "Epoch 00097: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 890us/sample - loss: 0.5067 - acc: 0.8984 - score: 0.9083 - val_loss: 0.9436 - val_acc: 0.7355 - val_score: 0.7716\n",
      "Epoch 98/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.9126 - score: 0.9256\n",
      "Epoch 00098: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.4736 - acc: 0.9126 - score: 0.9264 - val_loss: 0.8449 - val_acc: 0.7648 - val_score: 0.7983\n",
      "Epoch 99/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4642 - acc: 0.9198 - score: 0.9327- ETA: 2s - l\n",
      "Epoch 00099: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 0.4646 - acc: 0.9197 - score: 0.9241 - val_loss: 0.7502 - val_acc: 0.7969 - val_score: 0.8271\n",
      "Epoch 100/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.9183 - score: 0.9308\n",
      "Epoch 00100: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 897us/sample - loss: 0.4613 - acc: 0.9181 - score: 0.9223 - val_loss: 0.8541 - val_acc: 0.7648 - val_score: 0.8010\n",
      "Epoch 101/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.9229 - score: 0.9347\n",
      "Epoch 00101: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 902us/sample - loss: 0.4547 - acc: 0.9229 - score: 0.9354 - val_loss: 1.7848 - val_acc: 0.5481 - val_score: 0.6036\n",
      "Epoch 102/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4680 - acc: 0.9090 - score: 0.9230\n",
      "Epoch 00102: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 886us/sample - loss: 0.4684 - acc: 0.9088 - score: 0.9145 - val_loss: 1.4734 - val_acc: 0.5971 - val_score: 0.6544\n",
      "Epoch 103/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.9136 - score: 0.9270\n",
      "Epoch 00103: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 889us/sample - loss: 0.4623 - acc: 0.9135 - score: 0.9205 - val_loss: 0.9319 - val_acc: 0.7491 - val_score: 0.7817\n",
      "Epoch 104/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4770 - acc: 0.9095 - score: 0.9227\n",
      "Epoch 00104: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 898us/sample - loss: 0.4774 - acc: 0.9094 - score: 0.9143 - val_loss: 0.8851 - val_acc: 0.7485 - val_score: 0.7815\n",
      "Epoch 105/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.9158 - score: 0.9289\n",
      "Epoch 00105: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 903us/sample - loss: 0.4697 - acc: 0.9162 - score: 0.9208 - val_loss: 1.7674 - val_acc: 0.5658 - val_score: 0.6195\n",
      "Epoch 106/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4529 - acc: 0.9174 - score: 0.9302\n",
      "Epoch 00106: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.4533 - acc: 0.9173 - score: 0.9201 - val_loss: 1.4446 - val_acc: 0.6408 - val_score: 0.6873\n",
      "Epoch 107/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4688 - acc: 0.9097 - score: 0.9235\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "\n",
      "Epoch 00107: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.4691 - acc: 0.9097 - score: 0.9244 - val_loss: 0.9518 - val_acc: 0.7287 - val_score: 0.7669\n",
      "107 0.40000000727595725\n",
      "Epoch 108/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.9425 - score: 0.9517\n",
      "Epoch 00108: val_score did not improve from 0.83320\n",
      "5825/5825 [==============================] - 5s 897us/sample - loss: 0.3963 - acc: 0.9423 - score: 0.9413 - val_loss: 0.7774 - val_acc: 0.7771 - val_score: 0.8094\n",
      "Epoch 109/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.9530 - score: 0.9603\n",
      "Epoch 00109: val_score improved from 0.83320 to 0.85234, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 910us/sample - loss: 0.3612 - acc: 0.9526 - score: 0.9513 - val_loss: 0.6356 - val_acc: 0.8269 - val_score: 0.8523\n",
      "Epoch 110/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.9583 - score: 0.9647\n",
      "Epoch 00110: val_score did not improve from 0.85234\n",
      "5825/5825 [==============================] - 5s 884us/sample - loss: 0.3432 - acc: 0.9581 - score: 0.9542 - val_loss: 0.7195 - val_acc: 0.7996 - val_score: 0.8280\n",
      "Epoch 111/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3326 - acc: 0.9578 - score: 0.9644\n",
      "Epoch 00111: val_score did not improve from 0.85234\n",
      "5825/5825 [==============================] - 5s 883us/sample - loss: 0.3330 - acc: 0.9576 - score: 0.9539 - val_loss: 0.6995 - val_acc: 0.8050 - val_score: 0.8342\n",
      "Epoch 112/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.9663 - score: 0.9721\n",
      "Epoch 00112: val_score did not improve from 0.85234\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.3324 - acc: 0.9662 - score: 0.9615 - val_loss: 0.8959 - val_acc: 0.7485 - val_score: 0.7840\n",
      "Epoch 113/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.9561 - score: 0.9625- ETA: 1s - loss: 0.3458 - acc: 0\n",
      "Epoch 00113: val_score did not improve from 0.85234\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.3455 - acc: 0.9559 - score: 0.9535 - val_loss: 0.7001 - val_acc: 0.8119 - val_score: 0.8386\n",
      "Epoch 114/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3423 - acc: 0.9583 - score: 0.9647\n",
      "Epoch 00114: val_score did not improve from 0.85234\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.3427 - acc: 0.9581 - score: 0.9542 - val_loss: 1.1628 - val_acc: 0.6878 - val_score: 0.7293\n",
      "Epoch 115/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.9615 - score: 0.9680\n",
      "Epoch 00115: val_score did not improve from 0.85234\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.3294 - acc: 0.9614 - score: 0.9591 - val_loss: 1.1507 - val_acc: 0.6933 - val_score: 0.7355\n",
      "Epoch 116/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.9649 - score: 0.9704\n",
      "Epoch 00116: val_score did not improve from 0.85234\n",
      "5825/5825 [==============================] - 5s 900us/sample - loss: 0.3311 - acc: 0.9643 - score: 0.9609 - val_loss: 1.1397 - val_acc: 0.6892 - val_score: 0.7334\n",
      "Epoch 117/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.9574 - score: 0.9641\n",
      "Epoch 00117: val_score did not improve from 0.85234\n",
      "5825/5825 [==============================] - 5s 899us/sample - loss: 0.3430 - acc: 0.9573 - score: 0.9573 - val_loss: 1.1198 - val_acc: 0.6973 - val_score: 0.7387\n",
      "Epoch 118/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.9614 - score: 0.9674\n",
      "Epoch 00118: val_score improved from 0.85234 to 0.85745, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 911us/sample - loss: 0.3361 - acc: 0.9612 - score: 0.9605 - val_loss: 0.6212 - val_acc: 0.8303 - val_score: 0.8574\n",
      "Epoch 119/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.9581 - score: 0.9653\n",
      "Epoch 00119: val_score did not improve from 0.85745\n",
      "5825/5825 [==============================] - 5s 887us/sample - loss: 0.3307 - acc: 0.9579 - score: 0.9584 - val_loss: 0.6292 - val_acc: 0.8330 - val_score: 0.8564\n",
      "Epoch 120/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.9625 - score: 0.9684\n",
      "Epoch 00120: val_score did not improve from 0.85745\n",
      "5825/5825 [==============================] - 5s 896us/sample - loss: 0.3270 - acc: 0.9624 - score: 0.9579 - val_loss: 0.7140 - val_acc: 0.7914 - val_score: 0.8226\n",
      "Epoch 121/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.9606 - score: 0.9670\n",
      "Epoch 00121: val_score did not improve from 0.85745\n",
      "5825/5825 [==============================] - 5s 900us/sample - loss: 0.3235 - acc: 0.9605 - score: 0.9565 - val_loss: 0.8103 - val_acc: 0.7757 - val_score: 0.8086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3188 - acc: 0.9648 - score: 0.9707\n",
      "Epoch 00122: val_score did not improve from 0.85745\n",
      "5825/5825 [==============================] - 5s 898us/sample - loss: 0.3193 - acc: 0.9646 - score: 0.9602 - val_loss: 0.6917 - val_acc: 0.8064 - val_score: 0.8337\n",
      "Epoch 123/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3318 - acc: 0.9603 - score: 0.9664\n",
      "Epoch 00123: val_score did not improve from 0.85745\n",
      "5825/5825 [==============================] - 5s 894us/sample - loss: 0.3323 - acc: 0.9602 - score: 0.9559 - val_loss: 0.7285 - val_acc: 0.8016 - val_score: 0.8293\n",
      "Epoch 124/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9657 - score: 0.9712\n",
      "Epoch 00124: val_score did not improve from 0.85745\n",
      "5825/5825 [==============================] - 5s 897us/sample - loss: 0.3140 - acc: 0.9655 - score: 0.9622 - val_loss: 0.8706 - val_acc: 0.7716 - val_score: 0.8033\n",
      "Epoch 125/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9706 - score: 0.9747\n",
      "Epoch 00125: val_score did not improve from 0.85745\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.3060 - acc: 0.9705 - score: 0.9677 - val_loss: 0.8396 - val_acc: 0.7716 - val_score: 0.8040\n",
      "Epoch 126/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.9634 - score: 0.9693\n",
      "Epoch 00126: val_score did not improve from 0.85745\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 0.3250 - acc: 0.9633 - score: 0.9603 - val_loss: 0.7615 - val_acc: 0.7907 - val_score: 0.8207\n",
      "Epoch 127/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.9632 - score: 0.9687\n",
      "Epoch 00127: val_score did not improve from 0.85745\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.3185 - acc: 0.9626 - score: 0.9578 - val_loss: 0.8246 - val_acc: 0.7628 - val_score: 0.7968\n",
      "Epoch 128/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.9686 - score: 0.9738\n",
      "Epoch 00128: val_score did not improve from 0.85745\n",
      "5825/5825 [==============================] - 5s 889us/sample - loss: 0.3103 - acc: 0.9684 - score: 0.9633 - val_loss: 0.7538 - val_acc: 0.7900 - val_score: 0.8205\n",
      "Epoch 129/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.9639 - score: 0.9694\n",
      "Epoch 00129: val_score improved from 0.85745 to 0.85849, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 907us/sample - loss: 0.3209 - acc: 0.9638 - score: 0.9604 - val_loss: 0.6356 - val_acc: 0.8316 - val_score: 0.8585\n",
      "Epoch 130/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.9605 - score: 0.9667\n",
      "Epoch 00130: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 882us/sample - loss: 0.3219 - acc: 0.9603 - score: 0.9562 - val_loss: 0.6444 - val_acc: 0.8282 - val_score: 0.8548\n",
      "Epoch 131/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.9688 - score: 0.9742\n",
      "Epoch 00131: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 898us/sample - loss: 0.3140 - acc: 0.9682 - score: 0.9670 - val_loss: 0.6905 - val_acc: 0.8085 - val_score: 0.8379\n",
      "Epoch 132/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9691 - score: 0.9740\n",
      "Epoch 00132: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 898us/sample - loss: 0.3035 - acc: 0.9681 - score: 0.9735 - val_loss: 0.7630 - val_acc: 0.7955 - val_score: 0.8242\n",
      "Epoch 133/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3076 - acc: 0.9693 - score: 0.9741\n",
      "Epoch 00133: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 901us/sample - loss: 0.3078 - acc: 0.9693 - score: 0.9637 - val_loss: 0.6707 - val_acc: 0.8228 - val_score: 0.8488\n",
      "Epoch 134/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.9608 - score: 0.9668\n",
      "Epoch 00134: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.3191 - acc: 0.9607 - score: 0.9579 - val_loss: 0.7284 - val_acc: 0.8071 - val_score: 0.8363\n",
      "Epoch 135/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9688 - score: 0.9732\n",
      "Epoch 00135: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 902us/sample - loss: 0.3045 - acc: 0.9689 - score: 0.9737 - val_loss: 0.6834 - val_acc: 0.8146 - val_score: 0.8431\n",
      "Epoch 136/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9669 - score: 0.9721\n",
      "Epoch 00136: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 0.3139 - acc: 0.9667 - score: 0.9615 - val_loss: 0.6802 - val_acc: 0.8234 - val_score: 0.8497\n",
      "Epoch 137/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9679 - score: 0.9732\n",
      "Epoch 00137: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.3085 - acc: 0.9677 - score: 0.9626 - val_loss: 0.8381 - val_acc: 0.7716 - val_score: 0.8051\n",
      "Epoch 138/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.9662 - score: 0.9716\n",
      "Epoch 00138: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.3013 - acc: 0.9660 - score: 0.9610 - val_loss: 0.7358 - val_acc: 0.8010 - val_score: 0.8298\n",
      "Epoch 139/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9698 - score: 0.9743\n",
      "Epoch 00139: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.2975 - acc: 0.9696 - score: 0.9674 - val_loss: 0.6398 - val_acc: 0.8262 - val_score: 0.8511\n",
      "Epoch 140/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.9713 - score: 0.9758\n",
      "Epoch 00140: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.3005 - acc: 0.9712 - score: 0.9688 - val_loss: 0.6181 - val_acc: 0.8323 - val_score: 0.8571\n",
      "Epoch 141/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9675 - score: 0.9729\n",
      "Epoch 00141: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.3110 - acc: 0.9674 - score: 0.9624 - val_loss: 0.7174 - val_acc: 0.8003 - val_score: 0.8293\n",
      "Epoch 142/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9651 - score: 0.9705\n",
      "Epoch 00142: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 888us/sample - loss: 0.3074 - acc: 0.9650 - score: 0.9599 - val_loss: 0.9577 - val_acc: 0.7471 - val_score: 0.7852\n",
      "Epoch 143/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9677 - score: 0.9733\n",
      "Epoch 00143: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 897us/sample - loss: 0.3042 - acc: 0.9676 - score: 0.9642 - val_loss: 0.7659 - val_acc: 0.7887 - val_score: 0.8197\n",
      "Epoch 144/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3072 - acc: 0.9651 - score: 0.9705\n",
      "Epoch 00144: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 898us/sample - loss: 0.3076 - acc: 0.9650 - score: 0.9599 - val_loss: 1.4788 - val_acc: 0.6128 - val_score: 0.6632\n",
      "Epoch 145/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9672 - score: 0.9724- ETA: 1s - loss: 0.3082 - acc: 0.9676\n",
      "Epoch 00145: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.3106 - acc: 0.9670 - score: 0.9654 - val_loss: 0.6957 - val_acc: 0.8125 - val_score: 0.8415\n",
      "Epoch 146/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9677 - score: 0.9728\n",
      "Epoch 00146: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 896us/sample - loss: 0.2956 - acc: 0.9676 - score: 0.9622 - val_loss: 0.6633 - val_acc: 0.8228 - val_score: 0.8504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9682 - score: 0.9732\n",
      "Epoch 00147: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 902us/sample - loss: 0.2965 - acc: 0.9682 - score: 0.9735 - val_loss: 0.7212 - val_acc: 0.8078 - val_score: 0.8383\n",
      "Epoch 148/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9631 - score: 0.9693\n",
      "Epoch 00148: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.3074 - acc: 0.9631 - score: 0.9696 - val_loss: 0.6842 - val_acc: 0.8078 - val_score: 0.8359\n",
      "Epoch 149/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9665 - score: 0.9718\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "\n",
      "Epoch 00149: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.3006 - acc: 0.9664 - score: 0.9612 - val_loss: 0.7018 - val_acc: 0.8105 - val_score: 0.8396\n",
      "149 0.40000001818989284\n",
      "Epoch 150/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2601 - acc: 0.9803 - score: 0.9837\n",
      "Epoch 00150: val_score did not improve from 0.85849\n",
      "5825/5825 [==============================] - 5s 896us/sample - loss: 0.2606 - acc: 0.9801 - score: 0.9730 - val_loss: 0.6433 - val_acc: 0.8316 - val_score: 0.8571\n",
      "Epoch 151/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9812 - score: 0.9845\n",
      "Epoch 00151: val_score improved from 0.85849 to 0.86780, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 913us/sample - loss: 0.2574 - acc: 0.9813 - score: 0.9755 - val_loss: 0.6006 - val_acc: 0.8432 - val_score: 0.8678\n",
      "Epoch 152/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9804 - score: 0.9832\n",
      "Epoch 00152: val_score improved from 0.86780 to 0.87881, saving model to aspp_baseline0.h5\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.2553 - acc: 0.9803 - score: 0.9761 - val_loss: 0.5942 - val_acc: 0.8562 - val_score: 0.8788\n",
      "Epoch 153/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9830 - score: 0.9857\n",
      "Epoch 00153: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 883us/sample - loss: 0.2521 - acc: 0.9830 - score: 0.9859 - val_loss: 0.6288 - val_acc: 0.8248 - val_score: 0.8504\n",
      "Epoch 154/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9852 - score: 0.9880\n",
      "Epoch 00154: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.2453 - acc: 0.9851 - score: 0.9809 - val_loss: 0.8083 - val_acc: 0.7798 - val_score: 0.8120\n",
      "Epoch 155/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9847 - score: 0.9870\n",
      "Epoch 00155: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.2481 - acc: 0.9845 - score: 0.9778 - val_loss: 0.6118 - val_acc: 0.8432 - val_score: 0.8680\n",
      "Epoch 156/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9845 - score: 0.9870\n",
      "Epoch 00156: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 890us/sample - loss: 0.2409 - acc: 0.9844 - score: 0.9763 - val_loss: 0.6293 - val_acc: 0.8357 - val_score: 0.8611\n",
      "Epoch 157/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9833 - score: 0.9859\n",
      "Epoch 00157: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.2455 - acc: 0.9833 - score: 0.9860 - val_loss: 0.6323 - val_acc: 0.8371 - val_score: 0.8617\n",
      "Epoch 158/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9823 - score: 0.9852\n",
      "Epoch 00158: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 894us/sample - loss: 0.2446 - acc: 0.9823 - score: 0.9746 - val_loss: 0.6186 - val_acc: 0.8425 - val_score: 0.8667\n",
      "Epoch 159/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9828 - score: 0.9854\n",
      "Epoch 00159: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 899us/sample - loss: 0.2613 - acc: 0.9825 - score: 0.9782 - val_loss: 0.5802 - val_acc: 0.8507 - val_score: 0.8743\n",
      "Epoch 160/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9857 - score: 0.9880\n",
      "Epoch 00160: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.2445 - acc: 0.9856 - score: 0.9788 - val_loss: 0.6474 - val_acc: 0.8255 - val_score: 0.8508\n",
      "Epoch 161/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9847 - score: 0.9873\n",
      "Epoch 00161: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.2406 - acc: 0.9847 - score: 0.9767 - val_loss: 0.6119 - val_acc: 0.8446 - val_score: 0.8694\n",
      "Epoch 162/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9842 - score: 0.9869\n",
      "Epoch 00162: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 904us/sample - loss: 0.2436 - acc: 0.9842 - score: 0.9870 - val_loss: 0.6428 - val_acc: 0.8269 - val_score: 0.8532\n",
      "Epoch 163/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9854 - score: 0.9874- ETA: 1s - loss: 0.2432 - acc: \n",
      "Epoch 00163: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.2458 - acc: 0.9852 - score: 0.9803 - val_loss: 0.6405 - val_acc: 0.8262 - val_score: 0.8506\n",
      "Epoch 164/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9856 - score: 0.9880\n",
      "Epoch 00164: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 0.2432 - acc: 0.9854 - score: 0.9773 - val_loss: 0.8367 - val_acc: 0.7703 - val_score: 0.8050\n",
      "Epoch 165/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9856 - score: 0.9879\n",
      "Epoch 00165: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 885us/sample - loss: 0.2386 - acc: 0.9854 - score: 0.9772 - val_loss: 0.8422 - val_acc: 0.7655 - val_score: 0.8009\n",
      "Epoch 166/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9837 - score: 0.9865\n",
      "Epoch 00166: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 896us/sample - loss: 0.2440 - acc: 0.9837 - score: 0.9866 - val_loss: 0.6708 - val_acc: 0.8166 - val_score: 0.8447\n",
      "Epoch 167/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9856 - score: 0.9881\n",
      "Epoch 00167: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 899us/sample - loss: 0.2455 - acc: 0.9856 - score: 0.9811 - val_loss: 0.6107 - val_acc: 0.8459 - val_score: 0.8697\n",
      "Epoch 168/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9866 - score: 0.9885\n",
      "Epoch 00168: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 901us/sample - loss: 0.2311 - acc: 0.9864 - score: 0.9814 - val_loss: 0.6391 - val_acc: 0.8364 - val_score: 0.8617\n",
      "Epoch 169/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9866 - score: 0.9885\n",
      "Epoch 00169: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.2461 - acc: 0.9864 - score: 0.9778 - val_loss: 0.5977 - val_acc: 0.8398 - val_score: 0.8629\n",
      "Epoch 170/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9835 - score: 0.9860\n",
      "Epoch 00170: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 0.2445 - acc: 0.9833 - score: 0.9789 - val_loss: 0.6107 - val_acc: 0.8337 - val_score: 0.8595\n",
      "Epoch 171/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9856 - score: 0.9879\n",
      "Epoch 00171: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 896us/sample - loss: 0.2461 - acc: 0.9854 - score: 0.9771 - val_loss: 0.6017 - val_acc: 0.8398 - val_score: 0.8655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9875 - score: 0.9896\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.\n",
      "\n",
      "Epoch 00172: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 897us/sample - loss: 0.2366 - acc: 0.9873 - score: 0.9788 - val_loss: 0.6374 - val_acc: 0.8241 - val_score: 0.8524\n",
      "172 0.39999998863131747\n",
      "Epoch 173/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9885 - score: 0.9902\n",
      "Epoch 00173: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 899us/sample - loss: 0.2251 - acc: 0.9883 - score: 0.9830 - val_loss: 0.5895 - val_acc: 0.8419 - val_score: 0.8662\n",
      "Epoch 174/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9894 - score: 0.9911\n",
      "Epoch 00174: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 899us/sample - loss: 0.2190 - acc: 0.9892 - score: 0.9818 - val_loss: 0.5685 - val_acc: 0.8494 - val_score: 0.8723\n",
      "Epoch 175/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9872 - score: 0.9890\n",
      "Epoch 00175: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 898us/sample - loss: 0.2273 - acc: 0.9871 - score: 0.9784 - val_loss: 0.6044 - val_acc: 0.8398 - val_score: 0.8633\n",
      "Epoch 176/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9890 - score: 0.9908\n",
      "Epoch 00176: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 897us/sample - loss: 0.2269 - acc: 0.9890 - score: 0.9909 - val_loss: 0.5718 - val_acc: 0.8419 - val_score: 0.8681\n",
      "Epoch 177/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9914 - score: 0.9927\n",
      "Epoch 00177: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.2238 - acc: 0.9912 - score: 0.9856 - val_loss: 0.5798 - val_acc: 0.8439 - val_score: 0.8691\n",
      "Epoch 178/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9898 - score: 0.9912- ETA: 0s - loss: 0.2231 - acc: 0.9901 - score\n",
      "Epoch 00178: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 901us/sample - loss: 0.2232 - acc: 0.9895 - score: 0.9804 - val_loss: 0.5861 - val_acc: 0.8378 - val_score: 0.8625\n",
      "Epoch 179/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9864 - score: 0.9881\n",
      "Epoch 00179: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 900us/sample - loss: 0.2283 - acc: 0.9863 - score: 0.9789 - val_loss: 0.5872 - val_acc: 0.8425 - val_score: 0.8664\n",
      "Epoch 180/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9894 - score: 0.9907\n",
      "Epoch 00180: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 892us/sample - loss: 0.2229 - acc: 0.9892 - score: 0.9799 - val_loss: 0.5746 - val_acc: 0.8534 - val_score: 0.8768\n",
      "Epoch 181/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9928 - score: 0.9938\n",
      "Epoch 00181: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 899us/sample - loss: 0.2124 - acc: 0.9926 - score: 0.9866 - val_loss: 0.5718 - val_acc: 0.8453 - val_score: 0.8694\n",
      "Epoch 182/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9897 - score: 0.9912\n",
      "Epoch 00182: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.2207 - acc: 0.9895 - score: 0.9840 - val_loss: 0.5759 - val_acc: 0.8473 - val_score: 0.8722\n",
      "Epoch 183/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9933 - score: 0.9944\n",
      "Epoch 00183: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 900us/sample - loss: 0.2161 - acc: 0.9933 - score: 0.9945 - val_loss: 0.5922 - val_acc: 0.8405 - val_score: 0.8663\n",
      "Epoch 184/800\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9903 - score: 0.9919\n",
      "Epoch 00184: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 894us/sample - loss: 0.2192 - acc: 0.9900 - score: 0.9811 - val_loss: 0.5747 - val_acc: 0.8541 - val_score: 0.8773\n",
      "Epoch 185/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9918 - score: 0.9932\n",
      "Epoch 00185: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 897us/sample - loss: 0.2154 - acc: 0.9916 - score: 0.9824 - val_loss: 0.5778 - val_acc: 0.8453 - val_score: 0.8690\n",
      "Epoch 186/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9897 - score: 0.9915\n",
      "Epoch 00186: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 897us/sample - loss: 0.2212 - acc: 0.9895 - score: 0.9843 - val_loss: 0.5766 - val_acc: 0.8500 - val_score: 0.8739\n",
      "Epoch 187/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9897 - score: 0.9914\n",
      "Epoch 00187: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 908us/sample - loss: 0.2216 - acc: 0.9895 - score: 0.9822 - val_loss: 0.5725 - val_acc: 0.8534 - val_score: 0.8769\n",
      "Epoch 188/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9923 - score: 0.9936\n",
      "Epoch 00188: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 898us/sample - loss: 0.2193 - acc: 0.9921 - score: 0.9864 - val_loss: 0.5711 - val_acc: 0.8521 - val_score: 0.8751\n",
      "Epoch 189/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9907 - score: 0.9922\n",
      "Epoch 00189: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 889us/sample - loss: 0.2180 - acc: 0.9906 - score: 0.9850 - val_loss: 0.5954 - val_acc: 0.8432 - val_score: 0.8674\n",
      "Epoch 190/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9897 - score: 0.9913\n",
      "Epoch 00190: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 908us/sample - loss: 0.2227 - acc: 0.9895 - score: 0.9806 - val_loss: 0.5777 - val_acc: 0.8473 - val_score: 0.8715\n",
      "Epoch 191/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9909 - score: 0.9922\n",
      "Epoch 00191: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 896us/sample - loss: 0.2222 - acc: 0.9907 - score: 0.9850 - val_loss: 0.5764 - val_acc: 0.8480 - val_score: 0.8717\n",
      "Epoch 192/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9919 - score: 0.9932\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.\n",
      "\n",
      "Epoch 00192: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 895us/sample - loss: 0.2142 - acc: 0.9918 - score: 0.9824 - val_loss: 0.5685 - val_acc: 0.8548 - val_score: 0.8779\n",
      "192 0.4\n",
      "Epoch 193/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9907 - score: 0.9922\n",
      "Epoch 00193: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 897us/sample - loss: 0.2198 - acc: 0.9906 - score: 0.9815 - val_loss: 0.5771 - val_acc: 0.8500 - val_score: 0.8733\n",
      "Epoch 194/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9926 - score: 0.9938\n",
      "Epoch 00194: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 900us/sample - loss: 0.2214 - acc: 0.9926 - score: 0.9938 - val_loss: 0.5627 - val_acc: 0.8534 - val_score: 0.8758\n",
      "Epoch 195/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9899 - score: 0.9915\n",
      "Epoch 00195: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.2212 - acc: 0.9899 - score: 0.9916 - val_loss: 0.5702 - val_acc: 0.8514 - val_score: 0.8750\n",
      "Epoch 196/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9907 - score: 0.9922\n",
      "Epoch 00196: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 898us/sample - loss: 0.2154 - acc: 0.9906 - score: 0.9814 - val_loss: 0.5651 - val_acc: 0.8548 - val_score: 0.8775\n",
      "Epoch 197/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9914 - score: 0.9927\n",
      "Epoch 00197: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 893us/sample - loss: 0.2143 - acc: 0.9912 - score: 0.9835 - val_loss: 0.5658 - val_acc: 0.8528 - val_score: 0.8757\n",
      "Epoch 198/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9924 - score: 0.9934\n",
      "Epoch 00198: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 900us/sample - loss: 0.2117 - acc: 0.9923 - score: 0.9826 - val_loss: 0.5622 - val_acc: 0.8466 - val_score: 0.8710\n",
      "Epoch 199/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9930 - score: 0.9940\n",
      "Epoch 00199: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 891us/sample - loss: 0.2171 - acc: 0.9928 - score: 0.9832 - val_loss: 0.5637 - val_acc: 0.8534 - val_score: 0.8762\n",
      "Epoch 200/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9907 - score: 0.9924\n",
      "Epoch 00200: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 897us/sample - loss: 0.2160 - acc: 0.9906 - score: 0.9853 - val_loss: 0.5729 - val_acc: 0.8473 - val_score: 0.8715\n",
      "Epoch 201/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9918 - score: 0.9933\n",
      "Epoch 00201: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 899us/sample - loss: 0.2189 - acc: 0.9916 - score: 0.9825 - val_loss: 0.5872 - val_acc: 0.8439 - val_score: 0.8681\n",
      "Epoch 202/800\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9935 - score: 0.9946\n",
      "Epoch 00202: val_score did not improve from 0.87881\n",
      "5825/5825 [==============================] - 5s 905us/sample - loss: 0.2081 - acc: 0.9933 - score: 0.9837 - val_loss: 0.5705 - val_acc: 0.8507 - val_score: 0.8743\n",
      "Epoch 00202: early stopping\n",
      "Train on 5831 samples, validate on 1461 samples\n",
      "0 1.0000000474974513\n",
      "Epoch 1/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 2.2152 - acc: 0.2893 - score: 0.3752\n",
      "Epoch 00001: val_score improved from -inf to 0.22061, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 9s 1ms/sample - loss: 2.2162 - acc: 0.2891 - score: 0.3734 - val_loss: 3.5104 - val_acc: 0.1095 - val_score: 0.2206\n",
      "Epoch 2/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.9350 - acc: 0.3855 - score: 0.4698\n",
      "Epoch 00002: val_score improved from 0.22061 to 0.25938, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 888us/sample - loss: 1.9356 - acc: 0.3855 - score: 0.4696 - val_loss: 2.6020 - val_acc: 0.1677 - val_score: 0.2594\n",
      "Epoch 3/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.8135 - acc: 0.4360 - score: 0.5135\n",
      "Epoch 00003: val_score improved from 0.25938 to 0.27648, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 919us/sample - loss: 1.8140 - acc: 0.4359 - score: 0.5133 - val_loss: 2.4510 - val_acc: 0.1937 - val_score: 0.2765\n",
      "Epoch 4/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.7479 - acc: 0.4554 - score: 0.5302\n",
      "Epoch 00004: val_score improved from 0.27648 to 0.39633, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 915us/sample - loss: 1.7485 - acc: 0.4553 - score: 0.5300 - val_loss: 2.1019 - val_acc: 0.3142 - val_score: 0.3963\n",
      "Epoch 5/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.6860 - acc: 0.4816 - score: 0.5546\n",
      "Epoch 00005: val_score improved from 0.39633 to 0.41116, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 918us/sample - loss: 1.6868 - acc: 0.4810 - score: 0.5543 - val_loss: 2.3085 - val_acc: 0.3217 - val_score: 0.4112\n",
      "Epoch 6/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.6476 - acc: 0.4842 - score: 0.5568\n",
      "Epoch 00006: val_score improved from 0.41116 to 0.47913, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 914us/sample - loss: 1.6477 - acc: 0.4841 - score: 0.5565 - val_loss: 1.9199 - val_acc: 0.4011 - val_score: 0.4791\n",
      "Epoch 7/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.6109 - acc: 0.4990 - score: 0.5692\n",
      "Epoch 00007: val_score did not improve from 0.47913\n",
      "5831/5831 [==============================] - 5s 892us/sample - loss: 1.6113 - acc: 0.4987 - score: 0.5679 - val_loss: 1.9428 - val_acc: 0.3860 - val_score: 0.4645\n",
      "Epoch 8/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.6032 - acc: 0.4974 - score: 0.5685\n",
      "Epoch 00008: val_score improved from 0.47913 to 0.49037, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 921us/sample - loss: 1.6032 - acc: 0.4975 - score: 0.5696 - val_loss: 1.8410 - val_acc: 0.4120 - val_score: 0.4904\n",
      "Epoch 9/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.5549 - acc: 0.5201 - score: 0.5886\n",
      "Epoch 00009: val_score improved from 0.49037 to 0.58963, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 1.5556 - acc: 0.5198 - score: 0.5855 - val_loss: 1.6282 - val_acc: 0.5168 - val_score: 0.5896\n",
      "Epoch 10/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.5184 - acc: 0.5333 - score: 0.5990\n",
      "Epoch 00010: val_score did not improve from 0.58963\n",
      "5831/5831 [==============================] - 5s 892us/sample - loss: 1.5185 - acc: 0.5325 - score: 0.5944 - val_loss: 1.7933 - val_acc: 0.4319 - val_score: 0.4920\n",
      "Epoch 11/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.4975 - acc: 0.5444 - score: 0.6089\n",
      "Epoch 00011: val_score did not improve from 0.58963\n",
      "5831/5831 [==============================] - 5s 890us/sample - loss: 1.4957 - acc: 0.5447 - score: 0.6060 - val_loss: 1.6093 - val_acc: 0.5065 - val_score: 0.5731\n",
      "Epoch 12/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.4742 - acc: 0.5503 - score: 0.6148\n",
      "Epoch 00012: val_score did not improve from 0.58963\n",
      "5831/5831 [==============================] - 5s 893us/sample - loss: 1.4742 - acc: 0.5502 - score: 0.6135 - val_loss: 1.6111 - val_acc: 0.5051 - val_score: 0.5706\n",
      "Epoch 13/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.4402 - acc: 0.5602 - score: 0.6224\n",
      "Epoch 00013: val_score did not improve from 0.58963\n",
      "5831/5831 [==============================] - 5s 896us/sample - loss: 1.4404 - acc: 0.5603 - score: 0.6196 - val_loss: 1.7876 - val_acc: 0.4264 - val_score: 0.5061\n",
      "Epoch 14/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.4430 - acc: 0.5640 - score: 0.6253\n",
      "Epoch 00014: val_score improved from 0.58963 to 0.59654, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 912us/sample - loss: 1.4442 - acc: 0.5639 - score: 0.6247 - val_loss: 1.5380 - val_acc: 0.5250 - val_score: 0.5965\n",
      "Epoch 15/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.4117 - acc: 0.5659 - score: 0.6274\n",
      "Epoch 00015: val_score did not improve from 0.59654\n",
      "5831/5831 [==============================] - 5s 891us/sample - loss: 1.4125 - acc: 0.5656 - score: 0.6249 - val_loss: 1.7423 - val_acc: 0.4702 - val_score: 0.5460\n",
      "Epoch 16/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.3882 - acc: 0.5786 - score: 0.6383\n",
      "Epoch 00016: val_score did not improve from 0.59654\n",
      "5831/5831 [==============================] - 5s 902us/sample - loss: 1.3885 - acc: 0.5785 - score: 0.6370 - val_loss: 1.8529 - val_acc: 0.4271 - val_score: 0.5073\n",
      "Epoch 17/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.3604 - acc: 0.5898 - score: 0.6473\n",
      "Epoch 00017: val_score did not improve from 0.59654\n",
      "5831/5831 [==============================] - 5s 897us/sample - loss: 1.3609 - acc: 0.5900 - score: 0.6447 - val_loss: 1.5998 - val_acc: 0.4942 - val_score: 0.5665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.3379 - acc: 0.5962 - score: 0.6528\n",
      "Epoch 00018: val_score did not improve from 0.59654\n",
      "5831/5831 [==============================] - 5s 901us/sample - loss: 1.3382 - acc: 0.5960 - score: 0.6511 - val_loss: 1.7012 - val_acc: 0.4716 - val_score: 0.5415\n",
      "Epoch 19/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.3050 - acc: 0.6137 - score: 0.6676\n",
      "Epoch 00019: val_score improved from 0.59654 to 0.61370, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 921us/sample - loss: 1.3051 - acc: 0.6138 - score: 0.6689 - val_loss: 1.4964 - val_acc: 0.5489 - val_score: 0.6137\n",
      "Epoch 20/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.3008 - acc: 0.6095 - score: 0.6653\n",
      "Epoch 00020: val_score did not improve from 0.61370\n",
      "5831/5831 [==============================] - 5s 881us/sample - loss: 1.3011 - acc: 0.6097 - score: 0.6660 - val_loss: 1.6668 - val_acc: 0.5284 - val_score: 0.5945\n",
      "Epoch 21/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.2817 - acc: 0.6174 - score: 0.6719\n",
      "Epoch 00021: val_score improved from 0.61370 to 0.62556, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 921us/sample - loss: 1.2827 - acc: 0.6172 - score: 0.6700 - val_loss: 1.4018 - val_acc: 0.5633 - val_score: 0.6256\n",
      "Epoch 22/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.2608 - acc: 0.6267 - score: 0.6779\n",
      "Epoch 00022: val_score did not improve from 0.62556\n",
      "5831/5831 [==============================] - 5s 904us/sample - loss: 1.2618 - acc: 0.6265 - score: 0.6763 - val_loss: 1.5617 - val_acc: 0.5270 - val_score: 0.5915\n",
      "Epoch 23/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.2430 - acc: 0.6348 - score: 0.6875\n",
      "Epoch 00023: val_score improved from 0.62556 to 0.67504, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 915us/sample - loss: 1.2434 - acc: 0.6344 - score: 0.6841 - val_loss: 1.3267 - val_acc: 0.6242 - val_score: 0.6750\n",
      "Epoch 24/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.2235 - acc: 0.6349 - score: 0.6871\n",
      "Epoch 00024: val_score did not improve from 0.67504\n",
      "5831/5831 [==============================] - 5s 892us/sample - loss: 1.2230 - acc: 0.6357 - score: 0.6871 - val_loss: 1.7064 - val_acc: 0.4695 - val_score: 0.5339\n",
      "Epoch 25/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.2031 - acc: 0.6477 - score: 0.6982\n",
      "Epoch 00025: val_score did not improve from 0.67504\n",
      "5831/5831 [==============================] - 5s 903us/sample - loss: 1.2047 - acc: 0.6477 - score: 0.6935 - val_loss: 1.3330 - val_acc: 0.5948 - val_score: 0.6504\n",
      "Epoch 26/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.1676 - acc: 0.6645 - score: 0.7123\n",
      "Epoch 00026: val_score did not improve from 0.67504\n",
      "5831/5831 [==============================] - 5s 896us/sample - loss: 1.1683 - acc: 0.6642 - score: 0.7095 - val_loss: 1.9502 - val_acc: 0.4470 - val_score: 0.5188\n",
      "Epoch 27/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.1620 - acc: 0.6669 - score: 0.7145\n",
      "Epoch 00027: val_score did not improve from 0.67504\n",
      "5831/5831 [==============================] - 5s 900us/sample - loss: 1.1637 - acc: 0.6663 - score: 0.7088 - val_loss: 1.8149 - val_acc: 0.4825 - val_score: 0.5526\n",
      "Epoch 28/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.1646 - acc: 0.6564 - score: 0.7058\n",
      "Epoch 00028: val_score did not improve from 0.67504\n",
      "5831/5831 [==============================] - 5s 895us/sample - loss: 1.1654 - acc: 0.6563 - score: 0.7046 - val_loss: 1.4617 - val_acc: 0.5462 - val_score: 0.6102\n",
      "Epoch 29/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.1329 - acc: 0.6708 - score: 0.7189\n",
      "Epoch 00029: val_score improved from 0.67504 to 0.68186, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 928us/sample - loss: 1.1340 - acc: 0.6702 - score: 0.7144 - val_loss: 1.2750 - val_acc: 0.6304 - val_score: 0.6819\n",
      "Epoch 30/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.0935 - acc: 0.6875 - score: 0.7321\n",
      "Epoch 00030: val_score did not improve from 0.68186\n",
      "5831/5831 [==============================] - 5s 880us/sample - loss: 1.0939 - acc: 0.6872 - score: 0.7290 - val_loss: 1.5749 - val_acc: 0.5421 - val_score: 0.6090\n",
      "Epoch 31/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.0697 - acc: 0.6951 - score: 0.7390\n",
      "Epoch 00031: val_score did not improve from 0.68186\n",
      "5831/5831 [==============================] - 5s 901us/sample - loss: 1.0709 - acc: 0.6944 - score: 0.7339 - val_loss: 1.5246 - val_acc: 0.5387 - val_score: 0.6040\n",
      "Epoch 32/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.0723 - acc: 0.6945 - score: 0.7388\n",
      "Epoch 00032: val_score did not improve from 0.68186\n",
      "5831/5831 [==============================] - 5s 896us/sample - loss: 1.0732 - acc: 0.6942 - score: 0.7362 - val_loss: 1.3272 - val_acc: 0.6126 - val_score: 0.6640\n",
      "Epoch 33/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.0797 - acc: 0.6854 - score: 0.7323\n",
      "Epoch 00033: val_score improved from 0.68186 to 0.70442, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 917us/sample - loss: 1.0805 - acc: 0.6851 - score: 0.7302 - val_loss: 1.1837 - val_acc: 0.6537 - val_score: 0.7044\n",
      "Epoch 34/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.0652 - acc: 0.6930 - score: 0.7361\n",
      "Epoch 00034: val_score did not improve from 0.70442\n",
      "5831/5831 [==============================] - 5s 886us/sample - loss: 1.0655 - acc: 0.6927 - score: 0.7335 - val_loss: 1.3320 - val_acc: 0.5989 - val_score: 0.6500\n",
      "Epoch 35/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.0371 - acc: 0.7056 - score: 0.7476\n",
      "Epoch 00035: val_score did not improve from 0.70442\n",
      "5831/5831 [==============================] - 5s 904us/sample - loss: 1.0358 - acc: 0.7062 - score: 0.7477 - val_loss: 1.7382 - val_acc: 0.4846 - val_score: 0.5532\n",
      "Epoch 36/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.0212 - acc: 0.7121 - score: 0.7532\n",
      "Epoch 00036: val_score did not improve from 0.70442\n",
      "5831/5831 [==============================] - 5s 897us/sample - loss: 1.0215 - acc: 0.7122 - score: 0.7545 - val_loss: 1.3973 - val_acc: 0.5626 - val_score: 0.6280\n",
      "Epoch 37/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.9854 - acc: 0.7280 - score: 0.7670\n",
      "Epoch 00037: val_score did not improve from 0.70442\n",
      "5831/5831 [==============================] - 5s 890us/sample - loss: 0.9860 - acc: 0.7278 - score: 0.7651 - val_loss: 2.1886 - val_acc: 0.4586 - val_score: 0.5326\n",
      "Epoch 38/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.0074 - acc: 0.7212 - score: 0.7611\n",
      "Epoch 00038: val_score did not improve from 0.70442\n",
      "5831/5831 [==============================] - 5s 902us/sample - loss: 1.0080 - acc: 0.7208 - score: 0.7587 - val_loss: 2.0493 - val_acc: 0.4545 - val_score: 0.5231\n",
      "Epoch 39/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.9864 - acc: 0.7304 - score: 0.7688- ETA: 1s - loss: 0.9888 - a\n",
      "Epoch 00039: val_score did not improve from 0.70442\n",
      "5831/5831 [==============================] - 5s 904us/sample - loss: 0.9870 - acc: 0.7297 - score: 0.7642 - val_loss: 2.9477 - val_acc: 0.3600 - val_score: 0.4377\n",
      "Epoch 40/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.9661 - acc: 0.7312 - score: 0.7708\n",
      "Epoch 00040: val_score did not improve from 0.70442\n",
      "5831/5831 [==============================] - 5s 897us/sample - loss: 0.9673 - acc: 0.7306 - score: 0.7680 - val_loss: 2.0689 - val_acc: 0.4524 - val_score: 0.5229\n",
      "Epoch 41/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.9484 - acc: 0.7382 - score: 0.7750- ETA: 0s - loss: 0.9432 - acc: 0.7395 - score\n",
      "Epoch 00041: val_score improved from 0.70442 to 0.72486, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 926us/sample - loss: 0.9489 - acc: 0.7381 - score: 0.7748 - val_loss: 1.0896 - val_acc: 0.6783 - val_score: 0.7249\n",
      "Epoch 42/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.9524 - acc: 0.7445 - score: 0.7815\n",
      "Epoch 00042: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 886us/sample - loss: 0.9530 - acc: 0.7443 - score: 0.7797 - val_loss: 1.6120 - val_acc: 0.5154 - val_score: 0.5791\n",
      "Epoch 43/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.9420 - acc: 0.7464 - score: 0.7827\n",
      "Epoch 00043: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 895us/sample - loss: 0.9432 - acc: 0.7455 - score: 0.7781 - val_loss: 1.2907 - val_acc: 0.6153 - val_score: 0.6705\n",
      "Epoch 44/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.9416 - acc: 0.7425 - score: 0.7783\n",
      "Epoch 00044: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 893us/sample - loss: 0.9430 - acc: 0.7422 - score: 0.7767 - val_loss: 2.1441 - val_acc: 0.4305 - val_score: 0.5062\n",
      "Epoch 45/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8972 - acc: 0.7577 - score: 0.7943\n",
      "Epoch 00045: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 897us/sample - loss: 0.8986 - acc: 0.7568 - score: 0.7866 - val_loss: 2.6099 - val_acc: 0.3922 - val_score: 0.4684\n",
      "Epoch 46/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.9107 - acc: 0.7552 - score: 0.7898- ETA: 3s - \n",
      "Epoch 00046: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 903us/sample - loss: 0.9110 - acc: 0.7551 - score: 0.7892 - val_loss: 2.2429 - val_acc: 0.4648 - val_score: 0.5359\n",
      "Epoch 47/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8836 - acc: 0.7612 - score: 0.7960\n",
      "Epoch 00047: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 900us/sample - loss: 0.8844 - acc: 0.7609 - score: 0.7941 - val_loss: 1.1822 - val_acc: 0.6427 - val_score: 0.6962\n",
      "Epoch 48/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.9037 - acc: 0.7535 - score: 0.7899\n",
      "Epoch 00048: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 887us/sample - loss: 0.9059 - acc: 0.7527 - score: 0.7885 - val_loss: 2.0210 - val_acc: 0.4695 - val_score: 0.5375\n",
      "Epoch 49/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.8731 - acc: 0.7743 - score: 0.8074\n",
      "Epoch 00049: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 900us/sample - loss: 0.8759 - acc: 0.7731 - score: 0.8021 - val_loss: 2.1130 - val_acc: 0.4552 - val_score: 0.5262\n",
      "Epoch 50/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8959 - acc: 0.7594 - score: 0.7934\n",
      "Epoch 00050: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 900us/sample - loss: 0.8964 - acc: 0.7592 - score: 0.7914 - val_loss: 1.3397 - val_acc: 0.6078 - val_score: 0.6664\n",
      "Epoch 51/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8718 - acc: 0.7709 - score: 0.8036\n",
      "Epoch 00051: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 912us/sample - loss: 0.8721 - acc: 0.7711 - score: 0.8042 - val_loss: 1.8626 - val_acc: 0.5120 - val_score: 0.5824\n",
      "Epoch 52/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.8595 - acc: 0.7753 - score: 0.8069\n",
      "Epoch 00052: val_score did not improve from 0.72486\n",
      "5831/5831 [==============================] - 5s 896us/sample - loss: 0.8616 - acc: 0.7748 - score: 0.8044 - val_loss: 1.2303 - val_acc: 0.6167 - val_score: 0.6690\n",
      "Epoch 53/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.8514 - acc: 0.7818 - score: 0.8128\n",
      "Epoch 00053: val_score improved from 0.72486 to 0.72497, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 920us/sample - loss: 0.8513 - acc: 0.7820 - score: 0.8092 - val_loss: 1.0827 - val_acc: 0.6790 - val_score: 0.7250\n",
      "Epoch 54/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8567 - acc: 0.7747 - score: 0.8078\n",
      "Epoch 00054: val_score improved from 0.72497 to 0.75202, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 912us/sample - loss: 0.8569 - acc: 0.7747 - score: 0.8073 - val_loss: 1.0183 - val_acc: 0.7091 - val_score: 0.7520\n",
      "Epoch 55/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.8631 - acc: 0.7710 - score: 0.8038\n",
      "Epoch 00055: val_score did not improve from 0.75202\n",
      "5831/5831 [==============================] - 5s 883us/sample - loss: 0.8635 - acc: 0.7711 - score: 0.8006 - val_loss: 1.1870 - val_acc: 0.6489 - val_score: 0.6988\n",
      "Epoch 56/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8663 - acc: 0.7696 - score: 0.8033\n",
      "Epoch 00056: val_score did not improve from 0.75202\n",
      "5831/5831 [==============================] - 5s 899us/sample - loss: 0.8672 - acc: 0.7692 - score: 0.8001 - val_loss: 1.1660 - val_acc: 0.6564 - val_score: 0.7079\n",
      "Epoch 57/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8652 - acc: 0.7711 - score: 0.8034\n",
      "Epoch 00057: val_score did not improve from 0.75202\n",
      "5831/5831 [==============================] - 5s 910us/sample - loss: 0.8661 - acc: 0.7707 - score: 0.8003 - val_loss: 1.1308 - val_acc: 0.6612 - val_score: 0.7085\n",
      "Epoch 58/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.8164 - acc: 0.7899 - score: 0.8197\n",
      "Epoch 00058: val_score did not improve from 0.75202\n",
      "5831/5831 [==============================] - 5s 895us/sample - loss: 0.8182 - acc: 0.7892 - score: 0.8157 - val_loss: 1.1494 - val_acc: 0.6626 - val_score: 0.7112\n",
      "Epoch 59/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8350 - acc: 0.7816 - score: 0.8129\n",
      "Epoch 00059: val_score did not improve from 0.75202\n",
      "5831/5831 [==============================] - 5s 899us/sample - loss: 0.8361 - acc: 0.7812 - score: 0.8090 - val_loss: 1.3974 - val_acc: 0.6119 - val_score: 0.6680\n",
      "Epoch 60/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.8309 - acc: 0.7833 - score: 0.8145\n",
      "Epoch 00060: val_score improved from 0.75202 to 0.75548, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 919us/sample - loss: 0.8330 - acc: 0.7829 - score: 0.8112 - val_loss: 1.0009 - val_acc: 0.7125 - val_score: 0.7555\n",
      "Epoch 61/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7983 - acc: 0.7953 - score: 0.8247- ETA: 0s - loss: 0.7975 - acc: 0.7958 \n",
      "Epoch 00061: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 882us/sample - loss: 0.7989 - acc: 0.7952 - score: 0.8243 - val_loss: 1.8693 - val_acc: 0.5257 - val_score: 0.5910\n",
      "Epoch 62/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8178 - acc: 0.7900 - score: 0.8209- ETA: 2s - loss: 0\n",
      "Epoch 00062: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 914us/sample - loss: 0.8190 - acc: 0.7897 - score: 0.8189 - val_loss: 1.4517 - val_acc: 0.5838 - val_score: 0.6402\n",
      "Epoch 63/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8465 - acc: 0.7849 - score: 0.8163\n",
      "Epoch 00063: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 935us/sample - loss: 0.8476 - acc: 0.7846 - score: 0.8143 - val_loss: 2.9212 - val_acc: 0.3621 - val_score: 0.4280\n",
      "Epoch 64/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8327 - acc: 0.7802 - score: 0.8123\n",
      "Epoch 00064: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 935us/sample - loss: 0.8337 - acc: 0.7800 - score: 0.8102 - val_loss: 1.1561 - val_acc: 0.6502 - val_score: 0.7009\n",
      "Epoch 65/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8281 - acc: 0.7831 - score: 0.8144\n",
      "Epoch 00065: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 943us/sample - loss: 0.8295 - acc: 0.7827 - score: 0.8111 - val_loss: 1.5176 - val_acc: 0.5661 - val_score: 0.6250\n",
      "Epoch 66/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7954 - acc: 0.7972 - score: 0.8280\n",
      "Epoch 00066: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 6s 952us/sample - loss: 0.7967 - acc: 0.7966 - score: 0.8229 - val_loss: 2.1247 - val_acc: 0.5175 - val_score: 0.5846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7913 - acc: 0.7960 - score: 0.8247- ETA: 2\n",
      "Epoch 00067: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 943us/sample - loss: 0.7921 - acc: 0.7957 - score: 0.8229 - val_loss: 1.5012 - val_acc: 0.5832 - val_score: 0.6411\n",
      "Epoch 68/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8149 - acc: 0.7878 - score: 0.8180\n",
      "Epoch 00068: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 938us/sample - loss: 0.8156 - acc: 0.7875 - score: 0.8158 - val_loss: 2.4127 - val_acc: 0.4298 - val_score: 0.5045\n",
      "Epoch 69/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8024 - acc: 0.7873 - score: 0.8166\n",
      "Epoch 00069: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 935us/sample - loss: 0.8028 - acc: 0.7870 - score: 0.8145 - val_loss: 1.4040 - val_acc: 0.5893 - val_score: 0.6410\n",
      "Epoch 70/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7730 - acc: 0.8036 - score: 0.8324\n",
      "Epoch 00070: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 943us/sample - loss: 0.7742 - acc: 0.8033 - score: 0.8298 - val_loss: 2.2979 - val_acc: 0.3888 - val_score: 0.4629\n",
      "Epoch 71/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7834 - acc: 0.7981 - score: 0.8269\n",
      "Epoch 00071: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 938us/sample - loss: 0.7843 - acc: 0.7978 - score: 0.8257 - val_loss: 1.0491 - val_acc: 0.6920 - val_score: 0.7368\n",
      "Epoch 72/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.7750 - acc: 0.8050 - score: 0.8333\n",
      "Epoch 00072: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 6s 948us/sample - loss: 0.7761 - acc: 0.8048 - score: 0.8293 - val_loss: 1.2092 - val_acc: 0.6502 - val_score: 0.6978\n",
      "Epoch 73/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7935 - acc: 0.7929 - score: 0.8227\n",
      "Epoch 00073: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 942us/sample - loss: 0.7942 - acc: 0.7928 - score: 0.8223 - val_loss: 1.1344 - val_acc: 0.6502 - val_score: 0.6987\n",
      "Epoch 74/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7477 - acc: 0.8053 - score: 0.8336\n",
      "Epoch 00074: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 936us/sample - loss: 0.7484 - acc: 0.8050 - score: 0.8310 - val_loss: 1.6874 - val_acc: 0.5517 - val_score: 0.6163\n",
      "Epoch 75/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7698 - acc: 0.8003 - score: 0.8293\n",
      "Epoch 00075: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 939us/sample - loss: 0.7708 - acc: 0.8000 - score: 0.8275 - val_loss: 1.0831 - val_acc: 0.6831 - val_score: 0.7238\n",
      "Epoch 76/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7790 - acc: 0.8001 - score: 0.8277\n",
      "Epoch 00076: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 938us/sample - loss: 0.7795 - acc: 0.8000 - score: 0.8265 - val_loss: 1.3885 - val_acc: 0.5866 - val_score: 0.6434\n",
      "Epoch 77/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7793 - acc: 0.7988 - score: 0.8273\n",
      "Epoch 00077: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 6s 951us/sample - loss: 0.7799 - acc: 0.7987 - score: 0.8263 - val_loss: 1.5409 - val_acc: 0.5489 - val_score: 0.6110\n",
      "Epoch 78/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7586 - acc: 0.8034 - score: 0.8322\n",
      "Epoch 00078: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 941us/sample - loss: 0.7596 - acc: 0.8031 - score: 0.8298 - val_loss: 1.0729 - val_acc: 0.6762 - val_score: 0.7201\n",
      "Epoch 79/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7716 - acc: 0.8051 - score: 0.8328- ETA: 2s - loss: 0.7\n",
      "Epoch 00079: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 939us/sample - loss: 0.7719 - acc: 0.8052 - score: 0.8331 - val_loss: 1.6654 - val_acc: 0.5530 - val_score: 0.6128\n",
      "Epoch 80/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7646 - acc: 0.8077 - score: 0.8356\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00080: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 6s 955us/sample - loss: 0.7657 - acc: 0.8072 - score: 0.8319 - val_loss: 2.1451 - val_acc: 0.3997 - val_score: 0.4811\n",
      "80 0.4\n",
      "Epoch 81/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.6301 - acc: 0.8573 - score: 0.8779\n",
      "Epoch 00081: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 930us/sample - loss: 0.6307 - acc: 0.8570 - score: 0.8746 - val_loss: 1.0854 - val_acc: 0.6838 - val_score: 0.7316\n",
      "Epoch 82/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5895 - acc: 0.8771 - score: 0.8951\n",
      "Epoch 00082: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 938us/sample - loss: 0.5903 - acc: 0.8769 - score: 0.8934 - val_loss: 1.2531 - val_acc: 0.6311 - val_score: 0.6844\n",
      "Epoch 83/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5841 - acc: 0.8796 - score: 0.8976\n",
      "Epoch 00083: val_score did not improve from 0.75548\n",
      "5831/5831 [==============================] - 5s 940us/sample - loss: 0.5853 - acc: 0.8793 - score: 0.8948 - val_loss: 1.1160 - val_acc: 0.6783 - val_score: 0.7257\n",
      "Epoch 84/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5872 - acc: 0.8789 - score: 0.8964\n",
      "Epoch 00084: val_score improved from 0.75548 to 0.78068, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 6s 967us/sample - loss: 0.5885 - acc: 0.8781 - score: 0.8897 - val_loss: 0.8829 - val_acc: 0.7413 - val_score: 0.7807\n",
      "Epoch 85/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.8893 - score: 0.9063\n",
      "Epoch 00085: val_score did not improve from 0.78068\n",
      "5831/5831 [==============================] - 5s 926us/sample - loss: 0.5543 - acc: 0.8889 - score: 0.9033 - val_loss: 1.5123 - val_acc: 0.6057 - val_score: 0.6597\n",
      "Epoch 86/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8958 - score: 0.9116\n",
      "Epoch 00086: val_score did not improve from 0.78068\n",
      "5831/5831 [==============================] - 5s 942us/sample - loss: 0.5382 - acc: 0.8957 - score: 0.9115 - val_loss: 1.2453 - val_acc: 0.6331 - val_score: 0.6858\n",
      "Epoch 87/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.8886 - score: 0.9053\n",
      "Epoch 00087: val_score did not improve from 0.78068\n",
      "5831/5831 [==============================] - 6s 964us/sample - loss: 0.5524 - acc: 0.8885 - score: 0.9048 - val_loss: 1.5595 - val_acc: 0.5722 - val_score: 0.6320\n",
      "Epoch 88/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5432 - acc: 0.8906 - score: 0.9069\n",
      "Epoch 00088: val_score did not improve from 0.78068\n",
      "5831/5831 [==============================] - 6s 962us/sample - loss: 0.5447 - acc: 0.8901 - score: 0.9027 - val_loss: 1.4496 - val_acc: 0.5818 - val_score: 0.6431\n",
      "Epoch 89/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5645 - acc: 0.8853 - score: 0.9022\n",
      "Epoch 00089: val_score did not improve from 0.78068\n",
      "5831/5831 [==============================] - 6s 960us/sample - loss: 0.5649 - acc: 0.8851 - score: 0.9004 - val_loss: 2.3564 - val_acc: 0.4470 - val_score: 0.5215\n",
      "Epoch 90/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8959 - score: 0.9121\n",
      "Epoch 00090: val_score did not improve from 0.78068\n",
      "5831/5831 [==============================] - 6s 960us/sample - loss: 0.5335 - acc: 0.8956 - score: 0.9086 - val_loss: 1.7274 - val_acc: 0.5305 - val_score: 0.5947\n",
      "Epoch 91/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.8951 - score: 0.9106\n",
      "Epoch 00091: val_score did not improve from 0.78068\n",
      "5831/5831 [==============================] - 6s 965us/sample - loss: 0.5411 - acc: 0.8949 - score: 0.9089 - val_loss: 1.5994 - val_acc: 0.5743 - val_score: 0.6370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.8973 - score: 0.9127\n",
      "Epoch 00092: val_score improved from 0.78068 to 0.79239, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 6s 980us/sample - loss: 0.5228 - acc: 0.8973 - score: 0.9123 - val_loss: 0.8536 - val_acc: 0.7550 - val_score: 0.7924\n",
      "Epoch 93/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.8989 - score: 0.9135\n",
      "Epoch 00093: val_score did not improve from 0.79239\n",
      "5831/5831 [==============================] - 6s 956us/sample - loss: 0.5368 - acc: 0.8985 - score: 0.9107 - val_loss: 0.8988 - val_acc: 0.7481 - val_score: 0.7894\n",
      "Epoch 94/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.8954 - score: 0.9105\n",
      "Epoch 00094: val_score did not improve from 0.79239\n",
      "5831/5831 [==============================] - 6s 954us/sample - loss: 0.5255 - acc: 0.8952 - score: 0.9091 - val_loss: 0.9021 - val_acc: 0.7372 - val_score: 0.7740\n",
      "Epoch 95/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.9023 - score: 0.9170- ET\n",
      "Epoch 00095: val_score did not improve from 0.79239\n",
      "5831/5831 [==============================] - 6s 954us/sample - loss: 0.5057 - acc: 0.9022 - score: 0.9169 - val_loss: 0.8853 - val_acc: 0.7420 - val_score: 0.7791\n",
      "Epoch 96/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5007 - acc: 0.9088 - score: 0.9221\n",
      "Epoch 00096: val_score improved from 0.79239 to 0.80995, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 6s 976us/sample - loss: 0.5016 - acc: 0.9086 - score: 0.9200 - val_loss: 0.7743 - val_acc: 0.7762 - val_score: 0.8099\n",
      "Epoch 97/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4977 - acc: 0.9081 - score: 0.9216\n",
      "Epoch 00097: val_score did not improve from 0.80995\n",
      "5831/5831 [==============================] - 6s 949us/sample - loss: 0.4980 - acc: 0.9081 - score: 0.9211 - val_loss: 1.1989 - val_acc: 0.6585 - val_score: 0.7129\n",
      "Epoch 98/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.9080 - score: 0.9219- ETA: 1s - loss: 0.5002 - acc: 0.9\n",
      "Epoch 00098: val_score did not improve from 0.80995\n",
      "5831/5831 [==============================] - 6s 961us/sample - loss: 0.5020 - acc: 0.9077 - score: 0.9199 - val_loss: 1.3640 - val_acc: 0.6270 - val_score: 0.6790\n",
      "Epoch 99/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.9095 - score: 0.9225\n",
      "Epoch 00099: val_score did not improve from 0.80995\n",
      "5831/5831 [==============================] - 6s 955us/sample - loss: 0.4885 - acc: 0.9091 - score: 0.9191 - val_loss: 1.5791 - val_acc: 0.5619 - val_score: 0.6269\n",
      "Epoch 100/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.9052 - score: 0.9187\n",
      "Epoch 00100: val_score improved from 0.80995 to 0.81446, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 6s 986us/sample - loss: 0.4981 - acc: 0.9045 - score: 0.9127 - val_loss: 0.7879 - val_acc: 0.7837 - val_score: 0.8145\n",
      "Epoch 101/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.9121 - score: 0.9259\n",
      "Epoch 00101: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 947us/sample - loss: 0.4907 - acc: 0.9117 - score: 0.9225 - val_loss: 1.3600 - val_acc: 0.6126 - val_score: 0.6645\n",
      "Epoch 102/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.9047 - score: 0.9193\n",
      "Epoch 00102: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 958us/sample - loss: 0.5095 - acc: 0.9043 - score: 0.9160 - val_loss: 2.0872 - val_acc: 0.4908 - val_score: 0.5568\n",
      "Epoch 103/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5042 - acc: 0.9037 - score: 0.9185\n",
      "Epoch 00103: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 946us/sample - loss: 0.5049 - acc: 0.9033 - score: 0.9152 - val_loss: 0.9262 - val_acc: 0.7454 - val_score: 0.7824\n",
      "Epoch 104/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.4721 - acc: 0.9128 - score: 0.9261\n",
      "Epoch 00104: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 960us/sample - loss: 0.4729 - acc: 0.9129 - score: 0.9245 - val_loss: 0.8018 - val_acc: 0.7700 - val_score: 0.8053\n",
      "Epoch 105/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.9162 - score: 0.9290\n",
      "Epoch 00105: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 948us/sample - loss: 0.4774 - acc: 0.9161 - score: 0.9285 - val_loss: 1.5179 - val_acc: 0.5756 - val_score: 0.6378\n",
      "Epoch 106/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4991 - acc: 0.9035 - score: 0.9184\n",
      "Epoch 00106: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 962us/sample - loss: 0.4999 - acc: 0.9031 - score: 0.9149 - val_loss: 1.7228 - val_acc: 0.5400 - val_score: 0.5958\n",
      "Epoch 107/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.9033 - score: 0.9177\n",
      "Epoch 00107: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 955us/sample - loss: 0.5026 - acc: 0.9031 - score: 0.9159 - val_loss: 0.8223 - val_acc: 0.7693 - val_score: 0.8054\n",
      "Epoch 108/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4841 - acc: 0.9076 - score: 0.9216\n",
      "Epoch 00108: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 961us/sample - loss: 0.4849 - acc: 0.9074 - score: 0.9198 - val_loss: 1.7665 - val_acc: 0.5325 - val_score: 0.5931\n",
      "Epoch 109/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4629 - acc: 0.9174 - score: 0.9297\n",
      "Epoch 00109: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 953us/sample - loss: 0.4641 - acc: 0.9168 - score: 0.9250 - val_loss: 0.7820 - val_acc: 0.7796 - val_score: 0.8117\n",
      "Epoch 110/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.4658 - acc: 0.9191 - score: 0.9300\n",
      "Epoch 00110: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 949us/sample - loss: 0.4689 - acc: 0.9179 - score: 0.9262 - val_loss: 1.0907 - val_acc: 0.6831 - val_score: 0.7263\n",
      "Epoch 111/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4849 - acc: 0.9153 - score: 0.928 - ETA: 0s - loss: 0.4857 - acc: 0.9150 - score: 0.9284\n",
      "Epoch 00111: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 959us/sample - loss: 0.4864 - acc: 0.9149 - score: 0.9281 - val_loss: 0.8110 - val_acc: 0.7700 - val_score: 0.8029\n",
      "Epoch 112/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4671 - acc: 0.9133 - score: 0.9254\n",
      "Epoch 00112: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 950us/sample - loss: 0.4678 - acc: 0.9132 - score: 0.9247 - val_loss: 2.2330 - val_acc: 0.4593 - val_score: 0.5199\n",
      "Epoch 113/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4587 - acc: 0.9205 - score: 0.9321\n",
      "Epoch 00113: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 954us/sample - loss: 0.4596 - acc: 0.9203 - score: 0.9308 - val_loss: 0.9414 - val_acc: 0.7290 - val_score: 0.7706\n",
      "Epoch 114/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.9166 - score: 0.9291\n",
      "Epoch 00114: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 965us/sample - loss: 0.4632 - acc: 0.9167 - score: 0.9298 - val_loss: 0.9339 - val_acc: 0.7385 - val_score: 0.7782\n",
      "Epoch 115/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.9248 - score: 0.9360\n",
      "Epoch 00115: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 955us/sample - loss: 0.4506 - acc: 0.9245 - score: 0.9336 - val_loss: 0.7768 - val_acc: 0.7700 - val_score: 0.8052\n",
      "Epoch 116/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.9166 - score: 0.9293\n",
      "Epoch 00116: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 960us/sample - loss: 0.4655 - acc: 0.9160 - score: 0.9251 - val_loss: 0.8711 - val_acc: 0.7433 - val_score: 0.7802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.9148 - score: 0.9273\n",
      "Epoch 00117: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 964us/sample - loss: 0.4685 - acc: 0.9149 - score: 0.9281 - val_loss: 0.8427 - val_acc: 0.7522 - val_score: 0.7901\n",
      "Epoch 118/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.9126 - score: 0.9246\n",
      "Epoch 00118: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 977us/sample - loss: 0.4619 - acc: 0.9122 - score: 0.9208 - val_loss: 0.9173 - val_acc: 0.7255 - val_score: 0.7685\n",
      "Epoch 119/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.9198 - score: 0.9318\n",
      "Epoch 00119: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 971us/sample - loss: 0.4629 - acc: 0.9196 - score: 0.9299 - val_loss: 1.0928 - val_acc: 0.6954 - val_score: 0.7399\n",
      "Epoch 120/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.9150 - score: 0.9276\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "\n",
      "Epoch 00120: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 979us/sample - loss: 0.4591 - acc: 0.9148 - score: 0.9253 - val_loss: 0.9071 - val_acc: 0.7447 - val_score: 0.7831\n",
      "120 0.40000000727595725\n",
      "Epoch 121/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.9444 - score: 0.9530\n",
      "Epoch 00121: val_score did not improve from 0.81446\n",
      "5831/5831 [==============================] - 6s 966us/sample - loss: 0.3882 - acc: 0.9439 - score: 0.9498 - val_loss: 0.8464 - val_acc: 0.7652 - val_score: 0.8015\n",
      "Epoch 122/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.9485 - score: 0.9565\n",
      "Epoch 00122: val_score improved from 0.81446 to 0.84293, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 6s 985us/sample - loss: 0.3692 - acc: 0.9484 - score: 0.9557 - val_loss: 0.7090 - val_acc: 0.8138 - val_score: 0.8429\n",
      "Epoch 123/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.3573 - acc: 0.9528 - score: 0.9596\n",
      "Epoch 00123: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 6s 946us/sample - loss: 0.3584 - acc: 0.9525 - score: 0.9587 - val_loss: 0.9042 - val_acc: 0.7529 - val_score: 0.7905\n",
      "Epoch 124/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.9590 - score: 0.9647\n",
      "Epoch 00124: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 6s 953us/sample - loss: 0.3471 - acc: 0.9587 - score: 0.9625 - val_loss: 0.8621 - val_acc: 0.7495 - val_score: 0.7865\n",
      "Epoch 125/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.9598 - score: 0.9655- ETA: 1s - loss: 0.3318 - acc: 0.962\n",
      "Epoch 00125: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 6s 951us/sample - loss: 0.3395 - acc: 0.9588 - score: 0.9580 - val_loss: 0.8543 - val_acc: 0.7652 - val_score: 0.7995\n",
      "Epoch 126/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.9600 - score: 0.9655\n",
      "Epoch 00126: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 6s 964us/sample - loss: 0.3530 - acc: 0.9599 - score: 0.9643 - val_loss: 0.7345 - val_acc: 0.7912 - val_score: 0.8239\n",
      "Epoch 127/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.9584 - score: 0.9647\n",
      "Epoch 00127: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 6s 962us/sample - loss: 0.3504 - acc: 0.9582 - score: 0.9620 - val_loss: 1.0179 - val_acc: 0.7228 - val_score: 0.7640\n",
      "Epoch 128/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.9579 - score: 0.9643\n",
      "Epoch 00128: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 6s 951us/sample - loss: 0.3451 - acc: 0.9575 - score: 0.9602 - val_loss: 1.4343 - val_acc: 0.6092 - val_score: 0.6654\n",
      "Epoch 129/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.9675 - score: 0.9725\n",
      "Epoch 00129: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 6s 960us/sample - loss: 0.3263 - acc: 0.9676 - score: 0.9728 - val_loss: 0.8816 - val_acc: 0.7563 - val_score: 0.7922\n",
      "Epoch 130/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.9667 - score: 0.9718\n",
      "Epoch 00130: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 5s 920us/sample - loss: 0.3374 - acc: 0.9667 - score: 0.9710 - val_loss: 0.6807 - val_acc: 0.8104 - val_score: 0.8412\n",
      "Epoch 131/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.9643 - score: 0.9695\n",
      "Epoch 00131: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 5s 908us/sample - loss: 0.3327 - acc: 0.9642 - score: 0.9688 - val_loss: 1.2401 - val_acc: 0.6626 - val_score: 0.7123\n",
      "Epoch 132/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.9598 - score: 0.9653\n",
      "Epoch 00132: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 5s 911us/sample - loss: 0.3405 - acc: 0.9597 - score: 0.9644 - val_loss: 2.0266 - val_acc: 0.4839 - val_score: 0.5502\n",
      "Epoch 133/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.9593 - score: 0.9648\n",
      "Epoch 00133: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 5s 908us/sample - loss: 0.3363 - acc: 0.9587 - score: 0.9594 - val_loss: 0.9004 - val_acc: 0.7474 - val_score: 0.7857\n",
      "Epoch 134/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.9606 - score: 0.9668\n",
      "Epoch 00134: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 5s 915us/sample - loss: 0.3401 - acc: 0.9606 - score: 0.9657 - val_loss: 1.0992 - val_acc: 0.7036 - val_score: 0.7460\n",
      "Epoch 135/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.9638 - score: 0.9696\n",
      "Epoch 00135: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 5s 908us/sample - loss: 0.3255 - acc: 0.9635 - score: 0.9673 - val_loss: 1.1484 - val_acc: 0.6872 - val_score: 0.7337\n",
      "Epoch 136/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.9592 - score: 0.9657\n",
      "Epoch 00136: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 5s 916us/sample - loss: 0.3288 - acc: 0.9594 - score: 0.9638 - val_loss: 0.8052 - val_acc: 0.7803 - val_score: 0.8134\n",
      "Epoch 137/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3386 - acc: 0.9567 - score: 0.9630\n",
      "Epoch 00137: val_score did not improve from 0.84293\n",
      "5831/5831 [==============================] - 5s 904us/sample - loss: 0.3395 - acc: 0.9563 - score: 0.9594 - val_loss: 0.7313 - val_acc: 0.8056 - val_score: 0.8348\n",
      "Epoch 138/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.9645 - score: 0.9700\n",
      "Epoch 00138: val_score improved from 0.84293 to 0.84596, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 924us/sample - loss: 0.3271 - acc: 0.9642 - score: 0.9675 - val_loss: 0.6796 - val_acc: 0.8193 - val_score: 0.8460\n",
      "Epoch 139/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3188 - acc: 0.9657 - score: 0.9708\n",
      "Epoch 00139: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 912us/sample - loss: 0.3194 - acc: 0.9655 - score: 0.9701 - val_loss: 0.8477 - val_acc: 0.7584 - val_score: 0.7932\n",
      "Epoch 140/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3273 - acc: 0.9639 - score: 0.9697\n",
      "Epoch 00140: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 906us/sample - loss: 0.3281 - acc: 0.9636 - score: 0.9674 - val_loss: 0.7846 - val_acc: 0.7844 - val_score: 0.8157\n",
      "Epoch 141/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.9646 - score: 0.9702\n",
      "Epoch 00141: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 914us/sample - loss: 0.3309 - acc: 0.9642 - score: 0.9666 - val_loss: 0.8884 - val_acc: 0.7481 - val_score: 0.7865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.9609 - score: 0.9666\n",
      "Epoch 00142: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 0.3420 - acc: 0.9606 - score: 0.9646 - val_loss: 0.7060 - val_acc: 0.7995 - val_score: 0.8295\n",
      "Epoch 143/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.9669 - score: 0.9722\n",
      "Epoch 00143: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 908us/sample - loss: 0.3191 - acc: 0.9666 - score: 0.9697 - val_loss: 0.6995 - val_acc: 0.8172 - val_score: 0.8456\n",
      "Epoch 144/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.9650 - score: 0.9695\n",
      "Epoch 00144: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 0.3237 - acc: 0.9645 - score: 0.9654 - val_loss: 0.6944 - val_acc: 0.8063 - val_score: 0.8358\n",
      "Epoch 145/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.9648 - score: 0.9698\n",
      "Epoch 00145: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 912us/sample - loss: 0.3217 - acc: 0.9647 - score: 0.9688 - val_loss: 0.7773 - val_acc: 0.7892 - val_score: 0.8211\n",
      "Epoch 146/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.9710 - score: 0.9754\n",
      "Epoch 00146: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 906us/sample - loss: 0.3138 - acc: 0.9707 - score: 0.9730 - val_loss: 0.7638 - val_acc: 0.7912 - val_score: 0.8232\n",
      "Epoch 147/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.9674 - score: 0.9718\n",
      "Epoch 00147: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 908us/sample - loss: 0.3096 - acc: 0.9676 - score: 0.9722 - val_loss: 0.7009 - val_acc: 0.8049 - val_score: 0.8352\n",
      "Epoch 148/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.9688 - score: 0.9732- ETA: 1s - loss: 0.3144 - ac\n",
      "Epoch 00148: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 903us/sample - loss: 0.3122 - acc: 0.9686 - score: 0.9720 - val_loss: 0.7032 - val_acc: 0.8029 - val_score: 0.8332\n",
      "Epoch 149/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9668 - score: 0.9718\n",
      "Epoch 00149: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 912us/sample - loss: 0.3115 - acc: 0.9664 - score: 0.9703 - val_loss: 1.1833 - val_acc: 0.6769 - val_score: 0.7242\n",
      "Epoch 150/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.9691 - score: 0.9741\n",
      "Epoch 00150: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 914us/sample - loss: 0.3114 - acc: 0.9686 - score: 0.9722 - val_loss: 1.4302 - val_acc: 0.6167 - val_score: 0.6709\n",
      "Epoch 151/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.9552 - score: 0.9623\n",
      "Epoch 00151: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 0.3505 - acc: 0.9551 - score: 0.9613 - val_loss: 0.7074 - val_acc: 0.8084 - val_score: 0.8395\n",
      "Epoch 152/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9681 - score: 0.9729\n",
      "Epoch 00152: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 908us/sample - loss: 0.3123 - acc: 0.9676 - score: 0.9700 - val_loss: 0.9618 - val_acc: 0.7228 - val_score: 0.7636\n",
      "Epoch 153/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.9700 - score: 0.9747\n",
      "Epoch 00153: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 908us/sample - loss: 0.3037 - acc: 0.9698 - score: 0.9736 - val_loss: 0.8119 - val_acc: 0.7769 - val_score: 0.8129\n",
      "Epoch 154/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.3170 - acc: 0.9655 - score: 0.9706\n",
      "Epoch 00154: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 902us/sample - loss: 0.3179 - acc: 0.9650 - score: 0.9694 - val_loss: 0.7673 - val_acc: 0.7953 - val_score: 0.8278\n",
      "Epoch 155/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9681 - score: 0.9730\n",
      "Epoch 00155: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 917us/sample - loss: 0.3028 - acc: 0.9679 - score: 0.9723 - val_loss: 0.7165 - val_acc: 0.8104 - val_score: 0.8385\n",
      "Epoch 156/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9682 - score: 0.9729\n",
      "Epoch 00156: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 922us/sample - loss: 0.3033 - acc: 0.9681 - score: 0.9719 - val_loss: 0.8709 - val_acc: 0.7563 - val_score: 0.7948\n",
      "Epoch 157/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.9715 - score: 0.9755\n",
      "Epoch 00157: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 925us/sample - loss: 0.3014 - acc: 0.9712 - score: 0.9728 - val_loss: 0.8494 - val_acc: 0.7618 - val_score: 0.7972\n",
      "Epoch 158/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9712 - score: 0.9754\n",
      "Epoch 00158: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "\n",
      "Epoch 00158: val_score did not improve from 0.84596\n",
      "5831/5831 [==============================] - 5s 915us/sample - loss: 0.2951 - acc: 0.9702 - score: 0.9671 - val_loss: 0.7179 - val_acc: 0.7960 - val_score: 0.8279\n",
      "158 0.40000001818989284\n",
      "Epoch 159/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9787 - score: 0.9816\n",
      "Epoch 00159: val_score improved from 0.84596 to 0.85684, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 928us/sample - loss: 0.2779 - acc: 0.9787 - score: 0.9818 - val_loss: 0.6473 - val_acc: 0.8303 - val_score: 0.8568\n",
      "Epoch 160/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2711 - acc: 0.9780 - score: 0.9816- ETA: 2s - lo\n",
      "Epoch 00160: val_score did not improve from 0.85684\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 0.2724 - acc: 0.9777 - score: 0.9791 - val_loss: 0.6576 - val_acc: 0.8248 - val_score: 0.8522\n",
      "Epoch 161/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9813 - score: 0.9843\n",
      "Epoch 00161: val_score did not improve from 0.85684\n",
      "5831/5831 [==============================] - 5s 914us/sample - loss: 0.2711 - acc: 0.9811 - score: 0.9829 - val_loss: 0.7650 - val_acc: 0.7912 - val_score: 0.8216\n",
      "Epoch 162/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9804 - score: 0.9836\n",
      "Epoch 00162: val_score did not improve from 0.85684\n",
      "5831/5831 [==============================] - 5s 902us/sample - loss: 0.2738 - acc: 0.9803 - score: 0.9822 - val_loss: 0.6343 - val_acc: 0.8275 - val_score: 0.8554\n",
      "Epoch 163/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9823 - score: 0.9848\n",
      "Epoch 00163: val_score improved from 0.85684 to 0.85900, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 933us/sample - loss: 0.2632 - acc: 0.9820 - score: 0.9833 - val_loss: 0.6470 - val_acc: 0.8316 - val_score: 0.8590\n",
      "Epoch 164/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9812 - score: 0.9844\n",
      "Epoch 00164: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 903us/sample - loss: 0.2685 - acc: 0.9811 - score: 0.9845 - val_loss: 0.6625 - val_acc: 0.8241 - val_score: 0.8528\n",
      "Epoch 165/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9832 - score: 0.9855\n",
      "Epoch 00165: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 905us/sample - loss: 0.2613 - acc: 0.9829 - score: 0.9826 - val_loss: 0.6320 - val_acc: 0.8316 - val_score: 0.8577\n",
      "Epoch 166/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2680 - acc: 0.9806 - score: 0.9837\n",
      "Epoch 00166: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 906us/sample - loss: 0.2687 - acc: 0.9803 - score: 0.9808 - val_loss: 0.6679 - val_acc: 0.8166 - val_score: 0.8444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2680 - acc: 0.9797 - score: 0.9827\n",
      "Epoch 00167: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 912us/sample - loss: 0.2690 - acc: 0.9794 - score: 0.9801 - val_loss: 0.6543 - val_acc: 0.8248 - val_score: 0.8522\n",
      "Epoch 168/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9833 - score: 0.9862\n",
      "Epoch 00168: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 914us/sample - loss: 0.2580 - acc: 0.9832 - score: 0.9848 - val_loss: 0.6734 - val_acc: 0.8220 - val_score: 0.8497\n",
      "Epoch 169/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9845 - score: 0.9868\n",
      "Epoch 00169: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 900us/sample - loss: 0.2592 - acc: 0.9844 - score: 0.9854 - val_loss: 0.6527 - val_acc: 0.8220 - val_score: 0.8507\n",
      "Epoch 170/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.9839 - score: 0.9866\n",
      "Epoch 00170: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 899us/sample - loss: 0.2638 - acc: 0.9834 - score: 0.9863 - val_loss: 0.7766 - val_acc: 0.7878 - val_score: 0.8194\n",
      "Epoch 171/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2556 - acc: 0.9839 - score: 0.9864\n",
      "Epoch 00171: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 914us/sample - loss: 0.2564 - acc: 0.9837 - score: 0.9850 - val_loss: 0.7069 - val_acc: 0.8118 - val_score: 0.8406\n",
      "Epoch 172/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9833 - score: 0.9859\n",
      "Epoch 00172: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 909us/sample - loss: 0.2568 - acc: 0.9827 - score: 0.9808 - val_loss: 0.6526 - val_acc: 0.8220 - val_score: 0.8513\n",
      "Epoch 173/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9835 - score: 0.9860\n",
      "Epoch 00173: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 0.2603 - acc: 0.9835 - score: 0.9862 - val_loss: 0.6421 - val_acc: 0.8296 - val_score: 0.8556\n",
      "Epoch 174/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9844 - score: 0.9869\n",
      "Epoch 00174: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 911us/sample - loss: 0.2506 - acc: 0.9846 - score: 0.9872 - val_loss: 0.6478 - val_acc: 0.8248 - val_score: 0.8527\n",
      "Epoch 175/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.9863 - score: 0.9883\n",
      "Epoch 00175: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 904us/sample - loss: 0.2510 - acc: 0.9863 - score: 0.9884 - val_loss: 0.6917 - val_acc: 0.8227 - val_score: 0.8509\n",
      "Epoch 176/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9865 - score: 0.9883\n",
      "Epoch 00176: val_score did not improve from 0.85900\n",
      "5831/5831 [==============================] - 5s 908us/sample - loss: 0.2478 - acc: 0.9861 - score: 0.9868 - val_loss: 0.6508 - val_acc: 0.8214 - val_score: 0.8491\n",
      "Epoch 177/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9828 - score: 0.9853\n",
      "Epoch 00177: val_score improved from 0.85900 to 0.86113, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 930us/sample - loss: 0.2584 - acc: 0.9825 - score: 0.9838 - val_loss: 0.6246 - val_acc: 0.8357 - val_score: 0.8611\n",
      "Epoch 178/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9821 - score: 0.9850\n",
      "Epoch 00178: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 890us/sample - loss: 0.2638 - acc: 0.9820 - score: 0.9841 - val_loss: 0.6932 - val_acc: 0.8049 - val_score: 0.8347\n",
      "Epoch 179/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9859 - score: 0.9882\n",
      "Epoch 00179: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 905us/sample - loss: 0.2466 - acc: 0.9854 - score: 0.9844 - val_loss: 0.7140 - val_acc: 0.8077 - val_score: 0.8361\n",
      "Epoch 180/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.9808 - score: 0.9836\n",
      "Epoch 00180: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 0.2639 - acc: 0.9801 - score: 0.9775 - val_loss: 0.6875 - val_acc: 0.8118 - val_score: 0.8412\n",
      "Epoch 181/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9839 - score: 0.9862\n",
      "Epoch 00181: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 910us/sample - loss: 0.2509 - acc: 0.9834 - score: 0.9828 - val_loss: 0.6689 - val_acc: 0.8255 - val_score: 0.8518\n",
      "Epoch 182/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9713 - score: 0.9761\n",
      "Epoch 00182: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 0.2893 - acc: 0.9712 - score: 0.9751 - val_loss: 0.6624 - val_acc: 0.8186 - val_score: 0.8466\n",
      "Epoch 183/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9816 - score: 0.9841\n",
      "Epoch 00183: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 905us/sample - loss: 0.2538 - acc: 0.9810 - score: 0.9814 - val_loss: 0.6505 - val_acc: 0.8261 - val_score: 0.8535\n",
      "Epoch 184/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9812 - score: 0.9842\n",
      "Epoch 00184: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 909us/sample - loss: 0.2578 - acc: 0.9810 - score: 0.9817 - val_loss: 0.6861 - val_acc: 0.8111 - val_score: 0.8398\n",
      "Epoch 185/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9868 - score: 0.9889- ETA: 1s - loss: 0.2508 - acc\n",
      "Epoch 00185: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 915us/sample - loss: 0.2482 - acc: 0.9866 - score: 0.9877 - val_loss: 0.6493 - val_acc: 0.8255 - val_score: 0.8533\n",
      "Epoch 186/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9845 - score: 0.9869\n",
      "Epoch 00186: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 913us/sample - loss: 0.2463 - acc: 0.9842 - score: 0.9847 - val_loss: 0.6755 - val_acc: 0.8261 - val_score: 0.8526\n",
      "Epoch 187/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9839 - score: 0.9864\n",
      "Epoch 00187: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 0.2542 - acc: 0.9834 - score: 0.9826 - val_loss: 0.7070 - val_acc: 0.8084 - val_score: 0.8367\n",
      "Epoch 188/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9863 - score: 0.9882\n",
      "Epoch 00188: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 910us/sample - loss: 0.2522 - acc: 0.9863 - score: 0.9883 - val_loss: 0.6524 - val_acc: 0.8282 - val_score: 0.8553\n",
      "Epoch 189/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9825 - score: 0.9852\n",
      "Epoch 00189: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 910us/sample - loss: 0.2589 - acc: 0.9825 - score: 0.9853 - val_loss: 0.7008 - val_acc: 0.8186 - val_score: 0.8462\n",
      "Epoch 190/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9845 - score: 0.9870\n",
      "Epoch 00190: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 910us/sample - loss: 0.2492 - acc: 0.9844 - score: 0.9858 - val_loss: 0.6642 - val_acc: 0.8330 - val_score: 0.8595\n",
      "Epoch 191/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9825 - score: 0.9853\n",
      "Epoch 00191: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 906us/sample - loss: 0.2550 - acc: 0.9823 - score: 0.9839 - val_loss: 0.6482 - val_acc: 0.8275 - val_score: 0.8538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9861 - score: 0.9883\n",
      "Epoch 00192: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 911us/sample - loss: 0.2405 - acc: 0.9856 - score: 0.9847 - val_loss: 0.6639 - val_acc: 0.8179 - val_score: 0.8458\n",
      "Epoch 193/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9840 - score: 0.9866\n",
      "Epoch 00193: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 911us/sample - loss: 0.2491 - acc: 0.9839 - score: 0.9852 - val_loss: 0.6678 - val_acc: 0.8241 - val_score: 0.8527\n",
      "Epoch 194/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9863 - score: 0.9883\n",
      "Epoch 00194: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 910us/sample - loss: 0.2470 - acc: 0.9858 - score: 0.9845 - val_loss: 0.6689 - val_acc: 0.8200 - val_score: 0.8481\n",
      "Epoch 195/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9856 - score: 0.9877\n",
      "Epoch 00195: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 903us/sample - loss: 0.2484 - acc: 0.9854 - score: 0.9865 - val_loss: 0.6582 - val_acc: 0.8241 - val_score: 0.8515\n",
      "Epoch 196/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9849 - score: 0.9873\n",
      "Epoch 00196: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 915us/sample - loss: 0.2514 - acc: 0.9849 - score: 0.9875 - val_loss: 0.6515 - val_acc: 0.8207 - val_score: 0.8484\n",
      "Epoch 197/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9830 - score: 0.9859\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.\n",
      "\n",
      "Epoch 00197: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 909us/sample - loss: 0.2475 - acc: 0.9830 - score: 0.9860 - val_loss: 0.6648 - val_acc: 0.8303 - val_score: 0.8578\n",
      "197 0.39999998863131747\n",
      "Epoch 198/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9876 - score: 0.9895\n",
      "Epoch 00198: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 903us/sample - loss: 0.2339 - acc: 0.9875 - score: 0.9885 - val_loss: 0.6659 - val_acc: 0.8275 - val_score: 0.8541\n",
      "Epoch 199/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9868 - score: 0.9888\n",
      "Epoch 00199: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 0.2366 - acc: 0.9861 - score: 0.9830 - val_loss: 0.6722 - val_acc: 0.8186 - val_score: 0.8478\n",
      "Epoch 200/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9897 - score: 0.9913\n",
      "Epoch 00200: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 907us/sample - loss: 0.2333 - acc: 0.9897 - score: 0.9914 - val_loss: 0.6386 - val_acc: 0.8207 - val_score: 0.8495\n",
      "Epoch 201/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9909 - score: 0.9923\n",
      "Epoch 00201: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 914us/sample - loss: 0.2292 - acc: 0.9906 - score: 0.9900 - val_loss: 0.6330 - val_acc: 0.8309 - val_score: 0.8583\n",
      "Epoch 202/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9894 - score: 0.9909\n",
      "Epoch 00202: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 911us/sample - loss: 0.2350 - acc: 0.9890 - score: 0.9886 - val_loss: 0.6330 - val_acc: 0.8275 - val_score: 0.8535\n",
      "Epoch 203/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9911 - score: 0.9924\n",
      "Epoch 00203: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 906us/sample - loss: 0.2267 - acc: 0.9907 - score: 0.9896 - val_loss: 0.6233 - val_acc: 0.8296 - val_score: 0.8564\n",
      "Epoch 204/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9904 - score: 0.9918\n",
      "Epoch 00204: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 911us/sample - loss: 0.2332 - acc: 0.9904 - score: 0.9919 - val_loss: 0.6488 - val_acc: 0.8227 - val_score: 0.8502\n",
      "Epoch 205/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9899 - score: 0.9912\n",
      "Epoch 00205: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 904us/sample - loss: 0.2288 - acc: 0.9897 - score: 0.9900 - val_loss: 0.6400 - val_acc: 0.8316 - val_score: 0.8581\n",
      "Epoch 206/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9892 - score: 0.9909\n",
      "Epoch 00206: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 900us/sample - loss: 0.2300 - acc: 0.9890 - score: 0.9894 - val_loss: 0.6323 - val_acc: 0.8303 - val_score: 0.8579\n",
      "Epoch 207/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9888 - score: 0.9905\n",
      "Epoch 00207: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 916us/sample - loss: 0.2303 - acc: 0.9889 - score: 0.9906 - val_loss: 0.6372 - val_acc: 0.8309 - val_score: 0.8575\n",
      "Epoch 208/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9887 - score: 0.9905\n",
      "Epoch 00208: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 911us/sample - loss: 0.2311 - acc: 0.9883 - score: 0.9877 - val_loss: 0.6274 - val_acc: 0.8275 - val_score: 0.8544\n",
      "Epoch 209/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9902 - score: 0.9916\n",
      "Epoch 00209: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 901us/sample - loss: 0.2265 - acc: 0.9897 - score: 0.9883 - val_loss: 0.6312 - val_acc: 0.8350 - val_score: 0.8608\n",
      "Epoch 210/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9895 - score: 0.9907\n",
      "Epoch 00210: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 908us/sample - loss: 0.2276 - acc: 0.9895 - score: 0.9908 - val_loss: 0.6412 - val_acc: 0.8323 - val_score: 0.8588\n",
      "Epoch 211/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9906 - score: 0.9919\n",
      "Epoch 00211: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 917us/sample - loss: 0.2274 - acc: 0.9899 - score: 0.9865 - val_loss: 0.6247 - val_acc: 0.8316 - val_score: 0.8578\n",
      "Epoch 212/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9906 - score: 0.9919\n",
      "Epoch 00212: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 908us/sample - loss: 0.2301 - acc: 0.9902 - score: 0.9891 - val_loss: 0.6279 - val_acc: 0.8337 - val_score: 0.8601\n",
      "Epoch 213/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9885 - score: 0.9903\n",
      "Epoch 00213: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 904us/sample - loss: 0.2298 - acc: 0.9883 - score: 0.9891 - val_loss: 0.6304 - val_acc: 0.8303 - val_score: 0.8563\n",
      "Epoch 214/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9892 - score: 0.9911\n",
      "Epoch 00214: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 911us/sample - loss: 0.2329 - acc: 0.9892 - score: 0.9912 - val_loss: 0.6256 - val_acc: 0.8289 - val_score: 0.8564\n",
      "Epoch 215/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9902 - score: 0.9916\n",
      "Epoch 00215: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 916us/sample - loss: 0.2222 - acc: 0.9902 - score: 0.9917 - val_loss: 0.6903 - val_acc: 0.8084 - val_score: 0.8383\n",
      "Epoch 216/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9907 - score: 0.9920\n",
      "Epoch 00216: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 910us/sample - loss: 0.2251 - acc: 0.9906 - score: 0.9910 - val_loss: 0.6381 - val_acc: 0.8303 - val_score: 0.8563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9897 - score: 0.9915\n",
      "Epoch 00217: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.\n",
      "\n",
      "Epoch 00217: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 904us/sample - loss: 0.2285 - acc: 0.9894 - score: 0.9892 - val_loss: 0.6241 - val_acc: 0.8289 - val_score: 0.8555\n",
      "217 0.4\n",
      "Epoch 218/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9921 - score: 0.9934\n",
      "Epoch 00218: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 917us/sample - loss: 0.2243 - acc: 0.9916 - score: 0.9891 - val_loss: 0.6226 - val_acc: 0.8303 - val_score: 0.8571\n",
      "Epoch 219/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9921 - score: 0.9933\n",
      "Epoch 00219: val_score did not improve from 0.86113\n",
      "5831/5831 [==============================] - 5s 910us/sample - loss: 0.2203 - acc: 0.9919 - score: 0.9924 - val_loss: 0.6191 - val_acc: 0.8316 - val_score: 0.8578\n",
      "Epoch 220/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9875 - score: 0.9891\n",
      "Epoch 00220: val_score improved from 0.86113 to 0.86128, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 930us/sample - loss: 0.2263 - acc: 0.9866 - score: 0.9814 - val_loss: 0.6196 - val_acc: 0.8350 - val_score: 0.8613\n",
      "Epoch 221/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9916 - score: 0.9929\n",
      "Epoch 00221: val_score did not improve from 0.86128\n",
      "5831/5831 [==============================] - 5s 898us/sample - loss: 0.2207 - acc: 0.9916 - score: 0.9930 - val_loss: 0.6204 - val_acc: 0.8323 - val_score: 0.8585\n",
      "Epoch 222/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9909 - score: 0.9922\n",
      "Epoch 00222: val_score did not improve from 0.86128\n",
      "5831/5831 [==============================] - 5s 906us/sample - loss: 0.2207 - acc: 0.9906 - score: 0.9896 - val_loss: 0.6293 - val_acc: 0.8316 - val_score: 0.8586\n",
      "Epoch 223/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9916 - score: 0.9928\n",
      "Epoch 00223: val_score did not improve from 0.86128\n",
      "5831/5831 [==============================] - 5s 911us/sample - loss: 0.2192 - acc: 0.9916 - score: 0.9929 - val_loss: 0.6237 - val_acc: 0.8344 - val_score: 0.8609\n",
      "Epoch 224/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9916 - score: 0.9933\n",
      "Epoch 00224: val_score improved from 0.86128 to 0.86144, saving model to aspp_baseline1.h5\n",
      "5831/5831 [==============================] - 5s 928us/sample - loss: 0.2272 - acc: 0.9914 - score: 0.9918 - val_loss: 0.6204 - val_acc: 0.8357 - val_score: 0.8614\n",
      "Epoch 225/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9905 - score: 0.9920\n",
      "Epoch 00225: val_score did not improve from 0.86144\n",
      "5831/5831 [==============================] - 5s 905us/sample - loss: 0.2220 - acc: 0.9902 - score: 0.9893 - val_loss: 0.6260 - val_acc: 0.8309 - val_score: 0.8587\n",
      "Epoch 226/800\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9910 - score: 0.9923\n",
      "Epoch 00226: val_score did not improve from 0.86144\n",
      "5831/5831 [==============================] - 5s 910us/sample - loss: 0.2194 - acc: 0.9911 - score: 0.9925 - val_loss: 0.6207 - val_acc: 0.8330 - val_score: 0.8594\n",
      "Epoch 227/800\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9909 - score: 0.9925\n",
      "Epoch 00227: val_score did not improve from 0.86144\n",
      "5831/5831 [==============================] - 5s 912us/sample - loss: 0.2242 - acc: 0.9904 - score: 0.9879 - val_loss: 0.6213 - val_acc: 0.8344 - val_score: 0.8592\n",
      "Epoch 00227: early stopping\n",
      "Train on 5834 samples, validate on 1458 samples\n",
      "0 1.0000000474974513\n",
      "Epoch 1/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 2.2149 - acc: 0.2816 - score: 0.3677\n",
      "Epoch 00001: val_score improved from -inf to 0.19736, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 9s 1ms/sample - loss: 2.2143 - acc: 0.2818 - score: 0.3689 - val_loss: 4.1150 - val_acc: 0.0933 - val_score: 0.1974\n",
      "Epoch 2/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.9358 - acc: 0.3915 - score: 0.4701\n",
      "Epoch 00002: val_score improved from 0.19736 to 0.26157, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 888us/sample - loss: 1.9359 - acc: 0.3910 - score: 0.4669 - val_loss: 2.9454 - val_acc: 0.1523 - val_score: 0.2616\n",
      "Epoch 3/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.8138 - acc: 0.4293 - score: 0.5082\n",
      "Epoch 00003: val_score improved from 0.26157 to 0.31496, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 926us/sample - loss: 1.8147 - acc: 0.4289 - score: 0.5059 - val_loss: 2.4703 - val_acc: 0.2154 - val_score: 0.3150\n",
      "Epoch 4/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.7542 - acc: 0.4547 - score: 0.5293\n",
      "Epoch 00004: val_score improved from 0.31496 to 0.48299, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 923us/sample - loss: 1.7539 - acc: 0.4549 - score: 0.5303 - val_loss: 1.9141 - val_acc: 0.3951 - val_score: 0.4830\n",
      "Epoch 5/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.6783 - acc: 0.4808 - score: 0.5537\n",
      "Epoch 00005: val_score did not improve from 0.48299\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 1.6782 - acc: 0.4808 - score: 0.5537 - val_loss: 1.9747 - val_acc: 0.3690 - val_score: 0.4478\n",
      "Epoch 6/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.6492 - acc: 0.4866 - score: 0.5588\n",
      "Epoch 00006: val_score improved from 0.48299 to 0.54795, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 936us/sample - loss: 1.6478 - acc: 0.4868 - score: 0.5578 - val_loss: 1.7421 - val_acc: 0.4636 - val_score: 0.5480\n",
      "Epoch 7/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.6033 - acc: 0.5089 - score: 0.5774\n",
      "Epoch 00007: val_score did not improve from 0.54795\n",
      "5834/5834 [==============================] - 5s 904us/sample - loss: 1.6041 - acc: 0.5086 - score: 0.5754 - val_loss: 1.9212 - val_acc: 0.3669 - val_score: 0.4562\n",
      "Epoch 8/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.5576 - acc: 0.5314 - score: 0.5973\n",
      "Epoch 00008: val_score improved from 0.54795 to 0.55302, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 928us/sample - loss: 1.5584 - acc: 0.5312 - score: 0.5966 - val_loss: 1.7049 - val_acc: 0.4787 - val_score: 0.5530\n",
      "Epoch 9/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.5090 - acc: 0.5367 - score: 0.6019\n",
      "Epoch 00009: val_score improved from 0.55302 to 0.58289, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 924us/sample - loss: 1.5090 - acc: 0.5369 - score: 0.6031 - val_loss: 1.6885 - val_acc: 0.5082 - val_score: 0.5829\n",
      "Epoch 10/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.5007 - acc: 0.5405 - score: 0.6044\n",
      "Epoch 00010: val_score did not improve from 0.58289\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 1.5018 - acc: 0.5399 - score: 0.6020 - val_loss: 1.7991 - val_acc: 0.4540 - val_score: 0.5211\n",
      "Epoch 11/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.4673 - acc: 0.5467 - score: 0.6100\n",
      "Epoch 00011: val_score did not improve from 0.58289\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 1.4697 - acc: 0.5453 - score: 0.6086 - val_loss: 1.6278 - val_acc: 0.4781 - val_score: 0.5561\n",
      "Epoch 12/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.4355 - acc: 0.5661 - score: 0.6283\n",
      "Epoch 00012: val_score did not improve from 0.58289\n",
      "5834/5834 [==============================] - 5s 938us/sample - loss: 1.4367 - acc: 0.5656 - score: 0.6254 - val_loss: 1.8852 - val_acc: 0.4005 - val_score: 0.4817\n",
      "Epoch 13/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.4071 - acc: 0.5725 - score: 0.6320\n",
      "Epoch 00013: val_score improved from 0.58289 to 0.60311, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 6s 963us/sample - loss: 1.4075 - acc: 0.5723 - score: 0.6318 - val_loss: 1.5350 - val_acc: 0.5364 - val_score: 0.6031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.3810 - acc: 0.5840 - score: 0.6445\n",
      "Epoch 00014: val_score did not improve from 0.60311\n",
      "5834/5834 [==============================] - 5s 932us/sample - loss: 1.3816 - acc: 0.5840 - score: 0.6444 - val_loss: 2.7783 - val_acc: 0.3176 - val_score: 0.3941\n",
      "Epoch 15/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.3581 - acc: 0.5927 - score: 0.6511\n",
      "Epoch 00015: val_score did not improve from 0.60311\n",
      "5834/5834 [==============================] - 5s 933us/sample - loss: 1.3586 - acc: 0.5924 - score: 0.6494 - val_loss: 1.8262 - val_acc: 0.4486 - val_score: 0.5261\n",
      "Epoch 16/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.3366 - acc: 0.5994 - score: 0.6554\n",
      "Epoch 00016: val_score improved from 0.60311 to 0.63302, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 931us/sample - loss: 1.3374 - acc: 0.5991 - score: 0.6530 - val_loss: 1.4459 - val_acc: 0.5645 - val_score: 0.6330\n",
      "Epoch 17/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.3138 - acc: 0.6056 - score: 0.6610\n",
      "Epoch 00017: val_score did not improve from 0.63302\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 1.3143 - acc: 0.6056 - score: 0.6608 - val_loss: 2.7096 - val_acc: 0.3560 - val_score: 0.4314\n",
      "Epoch 18/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.2862 - acc: 0.6205 - score: 0.6741\n",
      "Epoch 00018: val_score did not improve from 0.63302\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 1.2876 - acc: 0.6200 - score: 0.6706 - val_loss: 1.5817 - val_acc: 0.4959 - val_score: 0.5739\n",
      "Epoch 19/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.3033 - acc: 0.6075 - score: 0.6608- ETA: 2s - loss: 1.2\n",
      "Epoch 00019: val_score did not improve from 0.63302\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 1.3041 - acc: 0.6073 - score: 0.6597 - val_loss: 1.8863 - val_acc: 0.4664 - val_score: 0.5360\n",
      "Epoch 20/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.2463 - acc: 0.6341 - score: 0.6860\n",
      "Epoch 00020: val_score did not improve from 0.63302\n",
      "5834/5834 [==============================] - 5s 905us/sample - loss: 1.2477 - acc: 0.6337 - score: 0.6835 - val_loss: 1.5737 - val_acc: 0.5110 - val_score: 0.5905\n",
      "Epoch 21/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.2206 - acc: 0.6427 - score: 0.6929\n",
      "Epoch 00021: val_score improved from 0.63302 to 0.63825, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 926us/sample - loss: 1.2213 - acc: 0.6426 - score: 0.6925 - val_loss: 1.3840 - val_acc: 0.5713 - val_score: 0.6382\n",
      "Epoch 22/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.2059 - acc: 0.6442 - score: 0.6941\n",
      "Epoch 00022: val_score did not improve from 0.63825\n",
      "5834/5834 [==============================] - 5s 890us/sample - loss: 1.2064 - acc: 0.6440 - score: 0.6926 - val_loss: 2.2325 - val_acc: 0.3861 - val_score: 0.4673\n",
      "Epoch 23/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.1664 - acc: 0.6608 - score: 0.7096\n",
      "Epoch 00023: val_score improved from 0.63825 to 0.67752, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 920us/sample - loss: 1.1673 - acc: 0.6592 - score: 0.7042 - val_loss: 1.3013 - val_acc: 0.6214 - val_score: 0.6775\n",
      "Epoch 24/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.1876 - acc: 0.6463 - score: 0.6973\n",
      "Epoch 00024: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 893us/sample - loss: 1.1880 - acc: 0.6460 - score: 0.6960 - val_loss: 1.5431 - val_acc: 0.5267 - val_score: 0.5950\n",
      "Epoch 25/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.1396 - acc: 0.6669 - score: 0.7135\n",
      "Epoch 00025: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 1.1403 - acc: 0.6663 - score: 0.7104 - val_loss: 1.5757 - val_acc: 0.5247 - val_score: 0.6018\n",
      "Epoch 26/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.1302 - acc: 0.6722 - score: 0.7181\n",
      "Epoch 00026: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 1.1304 - acc: 0.6724 - score: 0.7190 - val_loss: 1.5256 - val_acc: 0.5309 - val_score: 0.5992\n",
      "Epoch 27/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.0980 - acc: 0.6877 - score: 0.7309\n",
      "Epoch 00027: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 1.0989 - acc: 0.6875 - score: 0.7298 - val_loss: 1.4894 - val_acc: 0.5322 - val_score: 0.6010\n",
      "Epoch 28/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.0926 - acc: 0.6882 - score: 0.7327\n",
      "Epoch 00028: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 890us/sample - loss: 1.0924 - acc: 0.6880 - score: 0.7317 - val_loss: 1.3556 - val_acc: 0.5878 - val_score: 0.6481\n",
      "Epoch 29/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.0809 - acc: 0.6899 - score: 0.7327- ETA: 2s - loss: 1.0712 \n",
      "Epoch 00029: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 1.0809 - acc: 0.6899 - score: 0.7328 - val_loss: 1.9608 - val_acc: 0.4362 - val_score: 0.5162\n",
      "Epoch 30/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.0481 - acc: 0.7047 - score: 0.7474\n",
      "Epoch 00030: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 1.0493 - acc: 0.7041 - score: 0.7442 - val_loss: 1.4945 - val_acc: 0.5617 - val_score: 0.6244\n",
      "Epoch 31/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.0635 - acc: 0.6948 - score: 0.7384\n",
      "Epoch 00031: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 1.0669 - acc: 0.6940 - score: 0.7377 - val_loss: 1.4278 - val_acc: 0.5658 - val_score: 0.6290\n",
      "Epoch 32/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.0528 - acc: 0.7004 - score: 0.7438\n",
      "Epoch 00032: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 889us/sample - loss: 1.0536 - acc: 0.7000 - score: 0.7413 - val_loss: 1.1879 - val_acc: 0.6235 - val_score: 0.6718\n",
      "Epoch 33/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.0064 - acc: 0.7251 - score: 0.7656\n",
      "Epoch 00033: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 1.0073 - acc: 0.7249 - score: 0.7643 - val_loss: 2.5555 - val_acc: 0.3834 - val_score: 0.4626\n",
      "Epoch 34/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.0029 - acc: 0.7213 - score: 0.7611\n",
      "Epoch 00034: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 1.0031 - acc: 0.7213 - score: 0.7615 - val_loss: 1.6857 - val_acc: 0.5267 - val_score: 0.5938\n",
      "Epoch 35/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9869 - acc: 0.7284 - score: 0.7675\n",
      "Epoch 00035: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 0.9878 - acc: 0.7276 - score: 0.7632 - val_loss: 1.3180 - val_acc: 0.5995 - val_score: 0.6563\n",
      "Epoch 36/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.0058 - acc: 0.7170 - score: 0.7563\n",
      "Epoch 00036: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 1.0071 - acc: 0.7168 - score: 0.7551 - val_loss: 1.7989 - val_acc: 0.5089 - val_score: 0.5771\n",
      "Epoch 37/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9683 - acc: 0.7337 - score: 0.7712\n",
      "Epoch 00037: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.9696 - acc: 0.7335 - score: 0.7702 - val_loss: 1.6626 - val_acc: 0.5165 - val_score: 0.5839\n",
      "Epoch 38/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9491 - acc: 0.7454 - score: 0.7822\n",
      "Epoch 00038: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.9509 - acc: 0.7446 - score: 0.7782 - val_loss: 1.6701 - val_acc: 0.5171 - val_score: 0.5789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.9475 - acc: 0.7497 - score: 0.7880\n",
      "Epoch 00039: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 906us/sample - loss: 0.9471 - acc: 0.7499 - score: 0.7854 - val_loss: 1.9279 - val_acc: 0.4575 - val_score: 0.5264\n",
      "Epoch 40/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9326 - acc: 0.7522 - score: 0.7873\n",
      "Epoch 00040: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 889us/sample - loss: 0.9329 - acc: 0.7521 - score: 0.7874 - val_loss: 1.7539 - val_acc: 0.4938 - val_score: 0.5540\n",
      "Epoch 41/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9481 - acc: 0.7385 - score: 0.7767\n",
      "Epoch 00041: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.9489 - acc: 0.7384 - score: 0.7763 - val_loss: 1.2749 - val_acc: 0.5878 - val_score: 0.6516\n",
      "Epoch 42/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9293 - acc: 0.7531 - score: 0.7891\n",
      "Epoch 00042: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.9295 - acc: 0.7532 - score: 0.7895 - val_loss: 2.2144 - val_acc: 0.4575 - val_score: 0.5255\n",
      "Epoch 43/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.8894 - acc: 0.7639 - score: 0.7985\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00043: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 911us/sample - loss: 0.8903 - acc: 0.7633 - score: 0.7949 - val_loss: 2.2547 - val_acc: 0.4438 - val_score: 0.5155\n",
      "43 0.4\n",
      "Epoch 44/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.7836 - acc: 0.8007 - score: 0.8284\n",
      "Epoch 00044: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.7838 - acc: 0.8008 - score: 0.8296 - val_loss: 1.4455 - val_acc: 0.5816 - val_score: 0.6441\n",
      "Epoch 45/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.7464 - acc: 0.8243 - score: 0.8498\n",
      "Epoch 00045: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.7472 - acc: 0.8241 - score: 0.8486 - val_loss: 1.7415 - val_acc: 0.5171 - val_score: 0.5800\n",
      "Epoch 46/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.7285 - acc: 0.8302 - score: 0.8545- ETA: 1s - loss: 0.7188 - a\n",
      "Epoch 00046: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 905us/sample - loss: 0.7293 - acc: 0.8300 - score: 0.8537 - val_loss: 1.4237 - val_acc: 0.6015 - val_score: 0.6579\n",
      "Epoch 47/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.7186 - acc: 0.8276 - score: 0.8529\n",
      "Epoch 00047: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.7202 - acc: 0.8277 - score: 0.8526 - val_loss: 1.3593 - val_acc: 0.6132 - val_score: 0.6690\n",
      "Epoch 48/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.8412 - score: 0.8647\n",
      "Epoch 00048: val_score did not improve from 0.67752\n",
      "5834/5834 [==============================] - 5s 889us/sample - loss: 0.6928 - acc: 0.8408 - score: 0.8627 - val_loss: 1.5380 - val_acc: 0.5700 - val_score: 0.6284\n",
      "Epoch 49/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6892 - acc: 0.8431 - score: 0.8672\n",
      "Epoch 00049: val_score improved from 0.67752 to 0.69756, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 921us/sample - loss: 0.6895 - acc: 0.8428 - score: 0.8661 - val_loss: 1.2529 - val_acc: 0.6502 - val_score: 0.6976\n",
      "Epoch 50/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.6702 - acc: 0.8481 - score: 0.8708\n",
      "Epoch 00050: val_score improved from 0.69756 to 0.77140, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 907us/sample - loss: 0.6710 - acc: 0.8481 - score: 0.8697 - val_loss: 0.8987 - val_acc: 0.7298 - val_score: 0.7714\n",
      "Epoch 51/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6708 - acc: 0.8467 - score: 0.8694\n",
      "Epoch 00051: val_score improved from 0.77140 to 0.79417, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 0.6713 - acc: 0.8466 - score: 0.8688 - val_loss: 0.8310 - val_acc: 0.7579 - val_score: 0.7942\n",
      "Epoch 52/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6542 - acc: 0.8558 - score: 0.8764\n",
      "Epoch 00052: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 882us/sample - loss: 0.6552 - acc: 0.8553 - score: 0.8738 - val_loss: 2.1243 - val_acc: 0.4808 - val_score: 0.5463\n",
      "Epoch 53/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6723 - acc: 0.8489 - score: 0.8712\n",
      "Epoch 00053: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.6732 - acc: 0.8485 - score: 0.8687 - val_loss: 1.6728 - val_acc: 0.5782 - val_score: 0.6345\n",
      "Epoch 54/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.6569 - acc: 0.8490 - score: 0.8711\n",
      "Epoch 00054: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 890us/sample - loss: 0.6586 - acc: 0.8485 - score: 0.8703 - val_loss: 1.0312 - val_acc: 0.6907 - val_score: 0.7357\n",
      "Epoch 55/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.6486 - acc: 0.8561 - score: 0.8783\n",
      "Epoch 00055: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 0.6498 - acc: 0.8560 - score: 0.8784 - val_loss: 0.9540 - val_acc: 0.7298 - val_score: 0.7655\n",
      "Epoch 56/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6440 - acc: 0.8563 - score: 0.8775\n",
      "Epoch 00056: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 903us/sample - loss: 0.6455 - acc: 0.8558 - score: 0.8756 - val_loss: 1.2653 - val_acc: 0.6372 - val_score: 0.6861\n",
      "Epoch 57/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6456 - acc: 0.8592 - score: 0.8794\n",
      "Epoch 00057: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 890us/sample - loss: 0.6468 - acc: 0.8591 - score: 0.8786 - val_loss: 1.2342 - val_acc: 0.6276 - val_score: 0.6797\n",
      "Epoch 58/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6304 - acc: 0.8645 - score: 0.8848\n",
      "Epoch 00058: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.6314 - acc: 0.8642 - score: 0.8833 - val_loss: 1.1041 - val_acc: 0.6831 - val_score: 0.7279\n",
      "Epoch 59/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6079 - acc: 0.8728 - score: 0.8922\n",
      "Epoch 00059: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 903us/sample - loss: 0.6085 - acc: 0.8728 - score: 0.8924 - val_loss: 0.7964 - val_acc: 0.7586 - val_score: 0.7942\n",
      "Epoch 60/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.8705 - score: 0.8896\n",
      "Epoch 00060: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.6051 - acc: 0.8699 - score: 0.8860 - val_loss: 1.3340 - val_acc: 0.6255 - val_score: 0.6776\n",
      "Epoch 61/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6181 - acc: 0.8657 - score: 0.8853\n",
      "Epoch 00061: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 0.6188 - acc: 0.8656 - score: 0.8851 - val_loss: 1.1922 - val_acc: 0.6481 - val_score: 0.6957\n",
      "Epoch 62/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5955 - acc: 0.8733 - score: 0.8921\n",
      "Epoch 00062: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 886us/sample - loss: 0.5964 - acc: 0.8726 - score: 0.8885 - val_loss: 0.8407 - val_acc: 0.7586 - val_score: 0.7927\n",
      "Epoch 63/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5830 - acc: 0.8786 - score: 0.8965\n",
      "Epoch 00063: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.5862 - acc: 0.8778 - score: 0.8951 - val_loss: 0.9798 - val_acc: 0.7078 - val_score: 0.7456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6062 - acc: 0.8709 - score: 0.8894\n",
      "Epoch 00064: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.6079 - acc: 0.8701 - score: 0.8855 - val_loss: 1.4141 - val_acc: 0.6207 - val_score: 0.6688\n",
      "Epoch 65/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5965 - acc: 0.8764 - score: 0.8936\n",
      "Epoch 00065: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.5983 - acc: 0.8757 - score: 0.8911 - val_loss: 1.6836 - val_acc: 0.5652 - val_score: 0.6271\n",
      "Epoch 66/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.8686 - score: 0.8881\n",
      "Epoch 00066: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.6017 - acc: 0.8689 - score: 0.8877 - val_loss: 1.3439 - val_acc: 0.6427 - val_score: 0.6925\n",
      "Epoch 67/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.8759 - score: 0.8946\n",
      "Epoch 00067: val_score did not improve from 0.79417\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.5903 - acc: 0.8754 - score: 0.8923 - val_loss: 1.1637 - val_acc: 0.6776 - val_score: 0.7222\n",
      "Epoch 68/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5899 - acc: 0.8783 - score: 0.8966- ETA: 1s - loss: 0.5919 - acc: 0.8793\n",
      "Epoch 00068: val_score improved from 0.79417 to 0.81817, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 916us/sample - loss: 0.5905 - acc: 0.8780 - score: 0.8951 - val_loss: 0.7457 - val_acc: 0.7826 - val_score: 0.8182\n",
      "Epoch 69/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.8767 - score: 0.8945\n",
      "Epoch 00069: val_score did not improve from 0.81817\n",
      "5834/5834 [==============================] - 5s 882us/sample - loss: 0.5806 - acc: 0.8761 - score: 0.8903 - val_loss: 0.7816 - val_acc: 0.7833 - val_score: 0.8168\n",
      "Epoch 70/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5597 - acc: 0.8908 - score: 0.9074\n",
      "Epoch 00070: val_score did not improve from 0.81817\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 0.5616 - acc: 0.8900 - score: 0.9058 - val_loss: 0.8819 - val_acc: 0.7551 - val_score: 0.7930\n",
      "Epoch 71/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5623 - acc: 0.8898 - score: 0.9072\n",
      "Epoch 00071: val_score did not improve from 0.81817\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.5628 - acc: 0.8898 - score: 0.9073 - val_loss: 1.5293 - val_acc: 0.5933 - val_score: 0.6495\n",
      "Epoch 72/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5571 - acc: 0.8856 - score: 0.9022\n",
      "Epoch 00072: val_score did not improve from 0.81817\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.5576 - acc: 0.8855 - score: 0.9018 - val_loss: 1.0685 - val_acc: 0.7051 - val_score: 0.7452\n",
      "Epoch 73/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.8913 - score: 0.9073\n",
      "Epoch 00073: val_score did not improve from 0.81817\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.5507 - acc: 0.8906 - score: 0.9036 - val_loss: 1.6290 - val_acc: 0.5947 - val_score: 0.6503\n",
      "Epoch 74/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.8917 - score: 0.9081- E\n",
      "Epoch 00074: val_score did not improve from 0.81817\n",
      "5834/5834 [==============================] - 5s 892us/sample - loss: 0.5514 - acc: 0.8915 - score: 0.9073 - val_loss: 1.7102 - val_acc: 0.5761 - val_score: 0.6335\n",
      "Epoch 75/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.8953 - score: 0.9108\n",
      "Epoch 00075: val_score did not improve from 0.81817\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 0.5349 - acc: 0.8948 - score: 0.9080 - val_loss: 1.6667 - val_acc: 0.5713 - val_score: 0.6237\n",
      "Epoch 76/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.8898 - score: 0.9059\n",
      "Epoch 00076: val_score did not improve from 0.81817\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.5454 - acc: 0.8894 - score: 0.9040 - val_loss: 1.5428 - val_acc: 0.6008 - val_score: 0.6547\n",
      "Epoch 77/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.9033 - score: 0.9180\n",
      "Epoch 00077: val_score did not improve from 0.81817\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.5170 - acc: 0.9028 - score: 0.9146 - val_loss: 1.0556 - val_acc: 0.7023 - val_score: 0.7394\n",
      "Epoch 78/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.9032 - score: 0.9183\n",
      "Epoch 00078: val_score did not improve from 0.81817\n",
      "5834/5834 [==============================] - 5s 892us/sample - loss: 0.5164 - acc: 0.9026 - score: 0.9157 - val_loss: 2.3672 - val_acc: 0.4602 - val_score: 0.5309\n",
      "Epoch 79/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.8961 - score: 0.9101\n",
      "Epoch 00079: val_score improved from 0.81817 to 0.82234, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 915us/sample - loss: 0.5362 - acc: 0.8958 - score: 0.9085 - val_loss: 0.7702 - val_acc: 0.7929 - val_score: 0.8223\n",
      "Epoch 80/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.8941 - score: 0.9097\n",
      "Epoch 00080: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 885us/sample - loss: 0.5365 - acc: 0.8941 - score: 0.9098 - val_loss: 1.6427 - val_acc: 0.5789 - val_score: 0.6337\n",
      "Epoch 81/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.8980 - score: 0.9131- ET\n",
      "Epoch 00081: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.5288 - acc: 0.8975 - score: 0.9098 - val_loss: 1.2077 - val_acc: 0.6529 - val_score: 0.7016\n",
      "Epoch 82/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.8963 - score: 0.9123\n",
      "Epoch 00082: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.5307 - acc: 0.8958 - score: 0.9098 - val_loss: 1.0230 - val_acc: 0.7078 - val_score: 0.7463\n",
      "Epoch 83/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5319 - acc: 0.8893 - score: 0.9067\n",
      "Epoch 00083: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 0.5327 - acc: 0.8889 - score: 0.9049 - val_loss: 1.0130 - val_acc: 0.7195 - val_score: 0.7618\n",
      "Epoch 84/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5123 - acc: 0.8947 - score: 0.9107\n",
      "Epoch 00084: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 889us/sample - loss: 0.5135 - acc: 0.8942 - score: 0.9077 - val_loss: 1.0837 - val_acc: 0.6989 - val_score: 0.7394\n",
      "Epoch 85/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.9016 - score: 0.9165\n",
      "Epoch 00085: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.5127 - acc: 0.9014 - score: 0.9156 - val_loss: 1.5706 - val_acc: 0.6008 - val_score: 0.6529\n",
      "Epoch 86/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5057 - acc: 0.9026 - score: 0.9177\n",
      "Epoch 00086: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 888us/sample - loss: 0.5065 - acc: 0.9025 - score: 0.9169 - val_loss: 1.1367 - val_acc: 0.6852 - val_score: 0.7285\n",
      "Epoch 87/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5023 - acc: 0.9052 - score: 0.9205\n",
      "Epoch 00087: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 890us/sample - loss: 0.5033 - acc: 0.9050 - score: 0.9193 - val_loss: 1.4251 - val_acc: 0.6269 - val_score: 0.6746\n",
      "Epoch 88/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.9069 - score: 0.9212\n",
      "Epoch 00088: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 886us/sample - loss: 0.5051 - acc: 0.9062 - score: 0.9191 - val_loss: 0.9299 - val_acc: 0.7373 - val_score: 0.7711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.9100 - score: 0.9234\n",
      "Epoch 00089: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 0.4937 - acc: 0.9100 - score: 0.9232 - val_loss: 0.7673 - val_acc: 0.7846 - val_score: 0.8181\n",
      "Epoch 90/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4793 - acc: 0.9116 - score: 0.9252\n",
      "Epoch 00090: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 890us/sample - loss: 0.4805 - acc: 0.9110 - score: 0.9220 - val_loss: 0.7583 - val_acc: 0.7860 - val_score: 0.8178\n",
      "Epoch 91/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8979 - score: 0.9143\n",
      "Epoch 00091: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.5089 - acc: 0.8980 - score: 0.9134 - val_loss: 1.3229 - val_acc: 0.6529 - val_score: 0.7032\n",
      "Epoch 92/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.4958 - acc: 0.9080 - score: 0.9219\n",
      "Epoch 00092: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 0.4972 - acc: 0.9071 - score: 0.9178 - val_loss: 1.0902 - val_acc: 0.6955 - val_score: 0.7376\n",
      "Epoch 93/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5002 - acc: 0.9078 - score: 0.9220\n",
      "Epoch 00093: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.5011 - acc: 0.9076 - score: 0.9210 - val_loss: 1.2984 - val_acc: 0.6626 - val_score: 0.7107\n",
      "Epoch 94/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.9035 - score: 0.9175\n",
      "Epoch 00094: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.4937 - acc: 0.9035 - score: 0.9177 - val_loss: 1.6032 - val_acc: 0.5837 - val_score: 0.6419\n",
      "Epoch 95/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4835 - acc: 0.9119 - score: 0.9256\n",
      "Epoch 00095: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.4845 - acc: 0.9119 - score: 0.9253 - val_loss: 1.7106 - val_acc: 0.5521 - val_score: 0.6114\n",
      "Epoch 96/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.9112 - score: 0.9242\n",
      "Epoch 00096: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 892us/sample - loss: 0.4727 - acc: 0.9110 - score: 0.9232 - val_loss: 1.1905 - val_acc: 0.6626 - val_score: 0.7093\n",
      "Epoch 97/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.9102 - score: 0.9236\n",
      "Epoch 00097: val_score did not improve from 0.82234\n",
      "5834/5834 [==============================] - 5s 890us/sample - loss: 0.4828 - acc: 0.9104 - score: 0.9244 - val_loss: 0.7286 - val_acc: 0.7915 - val_score: 0.8206\n",
      "Epoch 98/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4806 - acc: 0.9129 - score: 0.9254\n",
      "Epoch 00098: val_score improved from 0.82234 to 0.83099, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 905us/sample - loss: 0.4816 - acc: 0.9124 - score: 0.9226 - val_loss: 0.7330 - val_acc: 0.8011 - val_score: 0.8310\n",
      "Epoch 99/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4763 - acc: 0.9147 - score: 0.9278\n",
      "Epoch 00099: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 888us/sample - loss: 0.4770 - acc: 0.9143 - score: 0.9258 - val_loss: 1.5288 - val_acc: 0.6084 - val_score: 0.6657\n",
      "Epoch 100/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.9090 - score: 0.9231\n",
      "Epoch 00100: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.4824 - acc: 0.9086 - score: 0.9209 - val_loss: 1.1598 - val_acc: 0.6763 - val_score: 0.7226\n",
      "Epoch 101/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4827 - acc: 0.9100 - score: 0.9239\n",
      "Epoch 00101: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 0.4841 - acc: 0.9095 - score: 0.9212 - val_loss: 1.1383 - val_acc: 0.6838 - val_score: 0.7300\n",
      "Epoch 102/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.9128 - score: 0.9259\n",
      "Epoch 00102: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.4724 - acc: 0.9126 - score: 0.9255 - val_loss: 0.7593 - val_acc: 0.7915 - val_score: 0.8241\n",
      "Epoch 103/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4797 - acc: 0.9095 - score: 0.9238\n",
      "Epoch 00103: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 892us/sample - loss: 0.4804 - acc: 0.9093 - score: 0.9228 - val_loss: 0.7994 - val_acc: 0.7661 - val_score: 0.8016\n",
      "Epoch 104/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5001 - acc: 0.9040 - score: 0.9186\n",
      "Epoch 00104: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.5013 - acc: 0.9037 - score: 0.9169 - val_loss: 0.8752 - val_acc: 0.7414 - val_score: 0.7807\n",
      "Epoch 105/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4906 - acc: 0.9054 - score: 0.9192\n",
      "Epoch 00105: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 892us/sample - loss: 0.4918 - acc: 0.9049 - score: 0.9164 - val_loss: 1.3598 - val_acc: 0.6324 - val_score: 0.6822\n",
      "Epoch 106/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4755 - acc: 0.9155 - score: 0.9282\n",
      "Epoch 00106: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 0.4768 - acc: 0.9150 - score: 0.9258 - val_loss: 0.8043 - val_acc: 0.7682 - val_score: 0.8028\n",
      "Epoch 107/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.4470 - acc: 0.9207 - score: 0.9327\n",
      "Epoch 00107: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.4483 - acc: 0.9198 - score: 0.9299 - val_loss: 1.1809 - val_acc: 0.6722 - val_score: 0.7180\n",
      "Epoch 108/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.9099 - score: 0.9238\n",
      "Epoch 00108: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 883us/sample - loss: 0.4714 - acc: 0.9097 - score: 0.9227 - val_loss: 0.8606 - val_acc: 0.7558 - val_score: 0.7953\n",
      "Epoch 109/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4651 - acc: 0.9119 - score: 0.9248\n",
      "Epoch 00109: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 903us/sample - loss: 0.4655 - acc: 0.9119 - score: 0.9247 - val_loss: 0.7649 - val_acc: 0.7798 - val_score: 0.8097\n",
      "Epoch 110/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4472 - acc: 0.9176 - score: 0.9303\n",
      "Epoch 00110: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.4486 - acc: 0.9170 - score: 0.9274 - val_loss: 1.8940 - val_acc: 0.5549 - val_score: 0.6136\n",
      "Epoch 111/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.9075 - score: 0.9216\n",
      "Epoch 00111: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.4929 - acc: 0.9073 - score: 0.9205 - val_loss: 0.8642 - val_acc: 0.7545 - val_score: 0.7892\n",
      "Epoch 112/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4830 - acc: 0.9088 - score: 0.9218\n",
      "Epoch 00112: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 883us/sample - loss: 0.4846 - acc: 0.9083 - score: 0.9192 - val_loss: 0.7402 - val_acc: 0.7936 - val_score: 0.8249\n",
      "Epoch 113/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.4583 - acc: 0.9153 - score: 0.9282\n",
      "Epoch 00113: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.4601 - acc: 0.9146 - score: 0.9244 - val_loss: 0.7776 - val_acc: 0.7840 - val_score: 0.8150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.9195 - score: 0.9320\n",
      "Epoch 00114: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.4454 - acc: 0.9193 - score: 0.9309 - val_loss: 1.5320 - val_acc: 0.6159 - val_score: 0.6682\n",
      "Epoch 115/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4566 - acc: 0.9174 - score: 0.9297\n",
      "Epoch 00115: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.4573 - acc: 0.9172 - score: 0.9288 - val_loss: 1.5763 - val_acc: 0.5789 - val_score: 0.6284\n",
      "Epoch 116/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.9243 - score: 0.9364\n",
      "Epoch 00116: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.4458 - acc: 0.9237 - score: 0.9330 - val_loss: 1.2572 - val_acc: 0.6516 - val_score: 0.6974\n",
      "Epoch 117/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4650 - acc: 0.9167 - score: 0.9296\n",
      "Epoch 00117: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.4658 - acc: 0.9165 - score: 0.9283 - val_loss: 0.7386 - val_acc: 0.7942 - val_score: 0.8245\n",
      "Epoch 118/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.4750 - acc: 0.9090 - score: 0.9230\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "\n",
      "Epoch 00118: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.4757 - acc: 0.9088 - score: 0.9216 - val_loss: 1.0420 - val_acc: 0.6989 - val_score: 0.7435\n",
      "118 0.40000000727595725\n",
      "Epoch 119/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3825 - acc: 0.9435 - score: 0.9526\n",
      "Epoch 00119: val_score did not improve from 0.83099\n",
      "5834/5834 [==============================] - 5s 890us/sample - loss: 0.3829 - acc: 0.9433 - score: 0.9509 - val_loss: 0.8568 - val_acc: 0.7599 - val_score: 0.7975\n",
      "Epoch 120/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.9562 - score: 0.9633\n",
      "Epoch 00120: val_score improved from 0.83099 to 0.85986, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 917us/sample - loss: 0.3560 - acc: 0.9561 - score: 0.9626 - val_loss: 0.6286 - val_acc: 0.8333 - val_score: 0.8599\n",
      "Epoch 121/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3368 - acc: 0.9631 - score: 0.9687\n",
      "Epoch 00121: val_score did not improve from 0.85986\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 0.3373 - acc: 0.9630 - score: 0.9681 - val_loss: 0.6172 - val_acc: 0.8237 - val_score: 0.8500\n",
      "Epoch 122/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.9633 - score: 0.9695\n",
      "Epoch 00122: val_score did not improve from 0.85986\n",
      "5834/5834 [==============================] - 5s 893us/sample - loss: 0.3421 - acc: 0.9630 - score: 0.9676 - val_loss: 0.6636 - val_acc: 0.8148 - val_score: 0.8430\n",
      "Epoch 123/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.9627 - score: 0.9681\n",
      "Epoch 00123: val_score did not improve from 0.85986\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.3410 - acc: 0.9626 - score: 0.9674 - val_loss: 0.8745 - val_acc: 0.7524 - val_score: 0.7894\n",
      "Epoch 124/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.9626 - score: 0.9688\n",
      "Epoch 00124: val_score did not improve from 0.85986\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.3362 - acc: 0.9625 - score: 0.9682 - val_loss: 1.0325 - val_acc: 0.7181 - val_score: 0.7604\n",
      "Epoch 125/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.9571 - score: 0.9642\n",
      "Epoch 00125: val_score did not improve from 0.85986\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.3376 - acc: 0.9566 - score: 0.9619 - val_loss: 0.8348 - val_acc: 0.7599 - val_score: 0.7959\n",
      "Epoch 126/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.9665 - score: 0.9717\n",
      "Epoch 00126: val_score improved from 0.85986 to 0.86362, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 918us/sample - loss: 0.3247 - acc: 0.9661 - score: 0.9705 - val_loss: 0.5857 - val_acc: 0.8395 - val_score: 0.8636\n",
      "Epoch 127/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.9653 - score: 0.9705\n",
      "Epoch 00127: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.3292 - acc: 0.9650 - score: 0.9689 - val_loss: 0.9960 - val_acc: 0.7181 - val_score: 0.7578\n",
      "Epoch 128/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.9624 - score: 0.9681- ETA: 0s - loss: 0.3320 - acc: 0.9621 - score: \n",
      "Epoch 00128: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 0.3306 - acc: 0.9623 - score: 0.9677 - val_loss: 0.7162 - val_acc: 0.7949 - val_score: 0.8282\n",
      "Epoch 129/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.9693 - score: 0.9740\n",
      "Epoch 00129: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 893us/sample - loss: 0.3165 - acc: 0.9693 - score: 0.9725 - val_loss: 0.7591 - val_acc: 0.7840 - val_score: 0.8150\n",
      "Epoch 130/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.9643 - score: 0.9698\n",
      "Epoch 00130: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.3202 - acc: 0.9640 - score: 0.9680 - val_loss: 0.6507 - val_acc: 0.8210 - val_score: 0.8497\n",
      "Epoch 131/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9715 - score: 0.9757\n",
      "Epoch 00131: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.3115 - acc: 0.9715 - score: 0.9759 - val_loss: 0.8217 - val_acc: 0.7743 - val_score: 0.8047\n",
      "Epoch 132/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.9633 - score: 0.9690\n",
      "Epoch 00132: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.3294 - acc: 0.9630 - score: 0.9673 - val_loss: 0.6854 - val_acc: 0.8169 - val_score: 0.8449\n",
      "Epoch 133/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.9663 - score: 0.9717\n",
      "Epoch 00133: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.3266 - acc: 0.9659 - score: 0.9688 - val_loss: 0.8068 - val_acc: 0.7826 - val_score: 0.8144\n",
      "Epoch 134/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.9662 - score: 0.9713\n",
      "Epoch 00134: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 932us/sample - loss: 0.3193 - acc: 0.9659 - score: 0.9695 - val_loss: 0.6164 - val_acc: 0.8368 - val_score: 0.8626\n",
      "Epoch 135/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.9689 - score: 0.9738\n",
      "Epoch 00135: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 925us/sample - loss: 0.3164 - acc: 0.9690 - score: 0.9740 - val_loss: 0.6269 - val_acc: 0.8224 - val_score: 0.8485\n",
      "Epoch 136/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.9724 - score: 0.9765\n",
      "Epoch 00136: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 935us/sample - loss: 0.3094 - acc: 0.9721 - score: 0.9748 - val_loss: 0.6147 - val_acc: 0.8258 - val_score: 0.8521\n",
      "Epoch 137/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.9642 - score: 0.9697\n",
      "Epoch 00137: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 936us/sample - loss: 0.3244 - acc: 0.9637 - score: 0.9687 - val_loss: 0.6280 - val_acc: 0.8272 - val_score: 0.8533\n",
      "Epoch 138/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9722 - score: 0.9763\n",
      "Epoch 00138: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 939us/sample - loss: 0.3019 - acc: 0.9719 - score: 0.9745 - val_loss: 0.7468 - val_acc: 0.8025 - val_score: 0.8290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.9629 - score: 0.9690\n",
      "Epoch 00139: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 6s 944us/sample - loss: 0.3254 - acc: 0.9628 - score: 0.9684 - val_loss: 1.1158 - val_acc: 0.7044 - val_score: 0.7460\n",
      "Epoch 140/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.9646 - score: 0.9699\n",
      "Epoch 00140: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 930us/sample - loss: 0.3237 - acc: 0.9643 - score: 0.9675 - val_loss: 0.7694 - val_acc: 0.7771 - val_score: 0.8102\n",
      "Epoch 141/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9657 - score: 0.9713\n",
      "Epoch 00141: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 937us/sample - loss: 0.3155 - acc: 0.9654 - score: 0.9698 - val_loss: 0.9221 - val_acc: 0.7497 - val_score: 0.7847\n",
      "Epoch 142/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.9663 - score: 0.9718\n",
      "Epoch 00142: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 939us/sample - loss: 0.3110 - acc: 0.9662 - score: 0.9710 - val_loss: 0.6196 - val_acc: 0.8333 - val_score: 0.8581\n",
      "Epoch 143/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9706 - score: 0.9757- ETA: 0s - loss: 0.3032 - acc: 0.9699 - s\n",
      "Epoch 00143: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 6s 946us/sample - loss: 0.3041 - acc: 0.9707 - score: 0.9760 - val_loss: 0.6316 - val_acc: 0.8237 - val_score: 0.8501\n",
      "Epoch 144/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.9724 - score: 0.9769\n",
      "Epoch 00144: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 933us/sample - loss: 0.3005 - acc: 0.9724 - score: 0.9772 - val_loss: 0.7141 - val_acc: 0.8066 - val_score: 0.8370\n",
      "Epoch 145/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9686 - score: 0.9739\n",
      "Epoch 00145: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 937us/sample - loss: 0.3075 - acc: 0.9683 - score: 0.9723 - val_loss: 1.2271 - val_acc: 0.6838 - val_score: 0.7254\n",
      "Epoch 146/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.9648 - score: 0.9702\n",
      "Epoch 00146: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "\n",
      "Epoch 00146: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 906us/sample - loss: 0.3140 - acc: 0.9642 - score: 0.9671 - val_loss: 1.1329 - val_acc: 0.6872 - val_score: 0.7306\n",
      "146 0.40000001818989284\n",
      "Epoch 147/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9766 - score: 0.9799\n",
      "Epoch 00147: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.2786 - acc: 0.9765 - score: 0.9794 - val_loss: 0.6491 - val_acc: 0.8155 - val_score: 0.8394\n",
      "Epoch 148/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9816 - score: 0.9845\n",
      "Epoch 00148: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.2707 - acc: 0.9811 - score: 0.9816 - val_loss: 0.5858 - val_acc: 0.8340 - val_score: 0.8596\n",
      "Epoch 149/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9813 - score: 0.9842\n",
      "Epoch 00149: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 0.2695 - acc: 0.9810 - score: 0.9829 - val_loss: 0.6775 - val_acc: 0.8148 - val_score: 0.8408\n",
      "Epoch 150/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9832 - score: 0.9864\n",
      "Epoch 00150: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 904us/sample - loss: 0.2584 - acc: 0.9829 - score: 0.9845 - val_loss: 0.6017 - val_acc: 0.8326 - val_score: 0.8570\n",
      "Epoch 151/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9815 - score: 0.9841\n",
      "Epoch 00151: val_score did not improve from 0.86362\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 0.2670 - acc: 0.9810 - score: 0.9814 - val_loss: 0.6881 - val_acc: 0.8045 - val_score: 0.8341\n",
      "Epoch 152/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9849 - score: 0.9877\n",
      "Epoch 00152: val_score improved from 0.86362 to 0.86802, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 927us/sample - loss: 0.2614 - acc: 0.9846 - score: 0.9860 - val_loss: 0.5506 - val_acc: 0.8457 - val_score: 0.8680\n",
      "Epoch 153/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9816 - score: 0.9849\n",
      "Epoch 00153: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 0.2652 - acc: 0.9811 - score: 0.9825 - val_loss: 0.6179 - val_acc: 0.8210 - val_score: 0.8471\n",
      "Epoch 154/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9825 - score: 0.9855\n",
      "Epoch 00154: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 893us/sample - loss: 0.2651 - acc: 0.9820 - score: 0.9828 - val_loss: 0.5819 - val_acc: 0.8368 - val_score: 0.8608\n",
      "Epoch 155/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9852 - score: 0.9874\n",
      "Epoch 00155: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 889us/sample - loss: 0.2650 - acc: 0.9853 - score: 0.9876 - val_loss: 0.5827 - val_acc: 0.8347 - val_score: 0.8588\n",
      "Epoch 156/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9815 - score: 0.9839\n",
      "Epoch 00156: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.2559 - acc: 0.9813 - score: 0.9832 - val_loss: 0.6060 - val_acc: 0.8395 - val_score: 0.8625\n",
      "Epoch 157/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2683 - acc: 0.9818 - score: 0.9847\n",
      "Epoch 00157: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 903us/sample - loss: 0.2687 - acc: 0.9818 - score: 0.9849 - val_loss: 0.5879 - val_acc: 0.8333 - val_score: 0.8574\n",
      "Epoch 158/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2584 - acc: 0.9818 - score: 0.9845\n",
      "Epoch 00158: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 0.2594 - acc: 0.9815 - score: 0.9827 - val_loss: 0.6797 - val_acc: 0.8025 - val_score: 0.8319\n",
      "Epoch 159/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9825 - score: 0.9854\n",
      "Epoch 00159: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.2576 - acc: 0.9820 - score: 0.9825 - val_loss: 0.5833 - val_acc: 0.8436 - val_score: 0.8673\n",
      "Epoch 160/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9818 - score: 0.9848\n",
      "Epoch 00160: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 907us/sample - loss: 0.2560 - acc: 0.9813 - score: 0.9824 - val_loss: 0.5759 - val_acc: 0.8340 - val_score: 0.8582\n",
      "Epoch 161/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9821 - score: 0.9847\n",
      "Epoch 00161: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.2602 - acc: 0.9815 - score: 0.9810 - val_loss: 0.5902 - val_acc: 0.8388 - val_score: 0.8616\n",
      "Epoch 162/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.9837 - score: 0.9860\n",
      "Epoch 00162: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2568 - acc: 0.9834 - score: 0.9841 - val_loss: 0.5706 - val_acc: 0.8368 - val_score: 0.8608\n",
      "Epoch 163/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9864 - score: 0.9882\n",
      "Epoch 00163: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.2495 - acc: 0.9861 - score: 0.9865 - val_loss: 0.6181 - val_acc: 0.8217 - val_score: 0.8476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9813 - score: 0.9842\n",
      "Epoch 00164: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.2519 - acc: 0.9810 - score: 0.9825 - val_loss: 0.5879 - val_acc: 0.8347 - val_score: 0.8591\n",
      "Epoch 165/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9892 - score: 0.9907\n",
      "Epoch 00165: val_score did not improve from 0.86802\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.2453 - acc: 0.9890 - score: 0.9899 - val_loss: 0.6224 - val_acc: 0.8278 - val_score: 0.8528\n",
      "Epoch 166/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9880 - score: 0.9900\n",
      "Epoch 00166: val_score improved from 0.86802 to 0.87083, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 922us/sample - loss: 0.2435 - acc: 0.9878 - score: 0.9892 - val_loss: 0.5676 - val_acc: 0.8477 - val_score: 0.8708\n",
      "Epoch 167/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.9837 - score: 0.9862\n",
      "Epoch 00167: val_score did not improve from 0.87083\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 0.2533 - acc: 0.9835 - score: 0.9854 - val_loss: 0.5613 - val_acc: 0.8416 - val_score: 0.8656\n",
      "Epoch 168/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9859 - score: 0.9880\n",
      "Epoch 00168: val_score did not improve from 0.87083\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2457 - acc: 0.9858 - score: 0.9870 - val_loss: 0.6045 - val_acc: 0.8278 - val_score: 0.8526\n",
      "Epoch 169/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.9842 - score: 0.9865\n",
      "Epoch 00169: val_score did not improve from 0.87083\n",
      "5834/5834 [==============================] - 5s 907us/sample - loss: 0.2500 - acc: 0.9841 - score: 0.9856 - val_loss: 0.5840 - val_acc: 0.8368 - val_score: 0.8605\n",
      "Epoch 170/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9880 - score: 0.9899\n",
      "Epoch 00170: val_score did not improve from 0.87083\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 0.2429 - acc: 0.9880 - score: 0.9900 - val_loss: 0.5721 - val_acc: 0.8443 - val_score: 0.8677\n",
      "Epoch 171/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.9844 - score: 0.9866\n",
      "Epoch 00171: val_score did not improve from 0.87083\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.2510 - acc: 0.9844 - score: 0.9858 - val_loss: 0.6379 - val_acc: 0.8251 - val_score: 0.8499\n",
      "Epoch 172/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9812 - score: 0.9844\n",
      "Epoch 00172: val_score did not improve from 0.87083\n",
      "5834/5834 [==============================] - 5s 892us/sample - loss: 0.2525 - acc: 0.9811 - score: 0.9839 - val_loss: 0.5833 - val_acc: 0.8388 - val_score: 0.8647\n",
      "Epoch 173/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9847 - score: 0.9868\n",
      "Epoch 00173: val_score did not improve from 0.87083\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2502 - acc: 0.9846 - score: 0.9862 - val_loss: 0.6620 - val_acc: 0.8128 - val_score: 0.8392\n",
      "Epoch 174/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9844 - score: 0.9870\n",
      "Epoch 00174: val_score did not improve from 0.87083\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2491 - acc: 0.9841 - score: 0.9853 - val_loss: 0.8565 - val_acc: 0.7695 - val_score: 0.8010\n",
      "Epoch 175/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9854 - score: 0.9877\n",
      "Epoch 00175: val_score did not improve from 0.87083\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.2415 - acc: 0.9846 - score: 0.9828 - val_loss: 0.5522 - val_acc: 0.8443 - val_score: 0.8677\n",
      "Epoch 176/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9823 - score: 0.9849\n",
      "Epoch 00176: val_score improved from 0.87083 to 0.87403, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 923us/sample - loss: 0.2579 - acc: 0.9822 - score: 0.9843 - val_loss: 0.5517 - val_acc: 0.8539 - val_score: 0.8740\n",
      "Epoch 177/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9861 - score: 0.9884\n",
      "Epoch 00177: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 889us/sample - loss: 0.2446 - acc: 0.9859 - score: 0.9875 - val_loss: 0.6099 - val_acc: 0.8265 - val_score: 0.8510\n",
      "Epoch 178/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9876 - score: 0.9892\n",
      "Epoch 00178: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.2460 - acc: 0.9875 - score: 0.9884 - val_loss: 0.5701 - val_acc: 0.8402 - val_score: 0.8639\n",
      "Epoch 179/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9833 - score: 0.9861\n",
      "Epoch 00179: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 904us/sample - loss: 0.2543 - acc: 0.9832 - score: 0.9854 - val_loss: 0.5988 - val_acc: 0.8381 - val_score: 0.8620\n",
      "Epoch 180/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9840 - score: 0.9868\n",
      "Epoch 00180: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.2456 - acc: 0.9839 - score: 0.9860 - val_loss: 0.5539 - val_acc: 0.8491 - val_score: 0.8707\n",
      "Epoch 181/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9866 - score: 0.9884\n",
      "Epoch 00181: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 893us/sample - loss: 0.2471 - acc: 0.9865 - score: 0.9876 - val_loss: 0.5636 - val_acc: 0.8477 - val_score: 0.8699\n",
      "Epoch 182/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9842 - score: 0.9867\n",
      "Epoch 00182: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.2413 - acc: 0.9839 - score: 0.9850 - val_loss: 0.6496 - val_acc: 0.8189 - val_score: 0.8449\n",
      "Epoch 183/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9851 - score: 0.9877\n",
      "Epoch 00183: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 907us/sample - loss: 0.2450 - acc: 0.9846 - score: 0.9843 - val_loss: 0.5860 - val_acc: 0.8450 - val_score: 0.8683\n",
      "Epoch 184/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9859 - score: 0.9881\n",
      "Epoch 00184: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 904us/sample - loss: 0.2502 - acc: 0.9859 - score: 0.9882 - val_loss: 0.5774 - val_acc: 0.8450 - val_score: 0.8674\n",
      "Epoch 185/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9847 - score: 0.9870\n",
      "Epoch 00185: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 912us/sample - loss: 0.2474 - acc: 0.9847 - score: 0.9871 - val_loss: 0.6056 - val_acc: 0.8368 - val_score: 0.8612\n",
      "Epoch 186/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9856 - score: 0.9876\n",
      "Epoch 00186: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 889us/sample - loss: 0.2452 - acc: 0.9858 - score: 0.9879 - val_loss: 0.5797 - val_acc: 0.8374 - val_score: 0.8636\n",
      "Epoch 187/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9859 - score: 0.9884\n",
      "Epoch 00187: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.2411 - acc: 0.9853 - score: 0.9846 - val_loss: 0.5910 - val_acc: 0.8354 - val_score: 0.8590\n",
      "Epoch 188/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9884 - score: 0.9905\n",
      "Epoch 00188: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 907us/sample - loss: 0.2381 - acc: 0.9880 - score: 0.9888 - val_loss: 0.5859 - val_acc: 0.8340 - val_score: 0.8588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9885 - score: 0.9903\n",
      "Epoch 00189: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.2349 - acc: 0.9883 - score: 0.9894 - val_loss: 0.5847 - val_acc: 0.8409 - val_score: 0.8656\n",
      "Epoch 190/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9875 - score: 0.9894\n",
      "Epoch 00190: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 893us/sample - loss: 0.2412 - acc: 0.9871 - score: 0.9884 - val_loss: 0.5955 - val_acc: 0.8409 - val_score: 0.8643\n",
      "Epoch 191/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9880 - score: 0.9899\n",
      "Epoch 00191: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 909us/sample - loss: 0.2315 - acc: 0.9877 - score: 0.9878 - val_loss: 0.6001 - val_acc: 0.8306 - val_score: 0.8554\n",
      "Epoch 192/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9877 - score: 0.9895\n",
      "Epoch 00192: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2432 - acc: 0.9878 - score: 0.9897 - val_loss: 0.5827 - val_acc: 0.8388 - val_score: 0.8637\n",
      "Epoch 193/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9883 - score: 0.9898\n",
      "Epoch 00193: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2339 - acc: 0.9877 - score: 0.9860 - val_loss: 0.5804 - val_acc: 0.8402 - val_score: 0.8642\n",
      "Epoch 194/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9840 - score: 0.9865- ETA: 0s - loss: 0.2415 - acc: 0.9838 - score: 0.98\n",
      "Epoch 00194: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 905us/sample - loss: 0.2408 - acc: 0.9835 - score: 0.9839 - val_loss: 0.5599 - val_acc: 0.8436 - val_score: 0.8658\n",
      "Epoch 195/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9851 - score: 0.9871\n",
      "Epoch 00195: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 892us/sample - loss: 0.2383 - acc: 0.9844 - score: 0.9836 - val_loss: 0.5822 - val_acc: 0.8368 - val_score: 0.8607\n",
      "Epoch 196/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9870 - score: 0.9890\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.\n",
      "\n",
      "Epoch 00196: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.2374 - acc: 0.9868 - score: 0.9881 - val_loss: 0.6834 - val_acc: 0.8093 - val_score: 0.8379\n",
      "196 0.39999998863131747\n",
      "Epoch 197/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9875 - score: 0.9896\n",
      "Epoch 00197: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.2296 - acc: 0.9873 - score: 0.9888 - val_loss: 0.5473 - val_acc: 0.8498 - val_score: 0.8740\n",
      "Epoch 198/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9900 - score: 0.9917\n",
      "Epoch 00198: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2223 - acc: 0.9899 - score: 0.9909 - val_loss: 0.5511 - val_acc: 0.8512 - val_score: 0.8735\n",
      "Epoch 199/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9904 - score: 0.9919\n",
      "Epoch 00199: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.2221 - acc: 0.9901 - score: 0.9901 - val_loss: 0.5509 - val_acc: 0.8484 - val_score: 0.8708\n",
      "Epoch 200/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9882 - score: 0.9898\n",
      "Epoch 00200: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 893us/sample - loss: 0.2264 - acc: 0.9873 - score: 0.9857 - val_loss: 0.5549 - val_acc: 0.8477 - val_score: 0.8722\n",
      "Epoch 201/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9913 - score: 0.9924- ETA: 2s - loss: 0.216\n",
      "Epoch 00201: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.2196 - acc: 0.9911 - score: 0.9907 - val_loss: 0.5545 - val_acc: 0.8484 - val_score: 0.8720\n",
      "Epoch 202/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9896 - score: 0.9910\n",
      "Epoch 00202: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.2273 - acc: 0.9892 - score: 0.9892 - val_loss: 0.5563 - val_acc: 0.8457 - val_score: 0.8687\n",
      "Epoch 203/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9902 - score: 0.9917\n",
      "Epoch 00203: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.2201 - acc: 0.9902 - score: 0.9918 - val_loss: 0.5580 - val_acc: 0.8471 - val_score: 0.8692\n",
      "Epoch 204/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9899 - score: 0.9915\n",
      "Epoch 00204: val_score did not improve from 0.87403\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.2212 - acc: 0.9895 - score: 0.9894 - val_loss: 0.5447 - val_acc: 0.8525 - val_score: 0.8737\n",
      "Epoch 205/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9916 - score: 0.9930\n",
      "Epoch 00205: val_score improved from 0.87403 to 0.87514, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 926us/sample - loss: 0.2140 - acc: 0.9914 - score: 0.9919 - val_loss: 0.5401 - val_acc: 0.8532 - val_score: 0.8751\n",
      "Epoch 206/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9899 - score: 0.9916\n",
      "Epoch 00206: val_score improved from 0.87514 to 0.87688, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 906us/sample - loss: 0.2241 - acc: 0.9895 - score: 0.9895 - val_loss: 0.5448 - val_acc: 0.8553 - val_score: 0.8769\n",
      "Epoch 207/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9904 - score: 0.9919\n",
      "Epoch 00207: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 888us/sample - loss: 0.2196 - acc: 0.9899 - score: 0.9888 - val_loss: 0.5686 - val_acc: 0.8477 - val_score: 0.8701\n",
      "Epoch 208/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9944 - score: 0.9952\n",
      "Epoch 00208: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.2176 - acc: 0.9942 - score: 0.9951 - val_loss: 0.5509 - val_acc: 0.8464 - val_score: 0.8697\n",
      "Epoch 209/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9916 - score: 0.9928\n",
      "Epoch 00209: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 0.2220 - acc: 0.9916 - score: 0.9929 - val_loss: 0.5462 - val_acc: 0.8464 - val_score: 0.8693\n",
      "Epoch 210/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9931 - score: 0.9945\n",
      "Epoch 00210: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 904us/sample - loss: 0.2153 - acc: 0.9931 - score: 0.9945 - val_loss: 0.5315 - val_acc: 0.8464 - val_score: 0.8704\n",
      "Epoch 211/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9940 - score: 0.9950\n",
      "Epoch 00211: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.2153 - acc: 0.9940 - score: 0.9951 - val_loss: 0.5401 - val_acc: 0.8477 - val_score: 0.8702\n",
      "Epoch 212/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9924 - score: 0.9937\n",
      "Epoch 00212: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.2112 - acc: 0.9925 - score: 0.9937 - val_loss: 0.5615 - val_acc: 0.8484 - val_score: 0.8714\n",
      "Epoch 213/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9906 - score: 0.9920\n",
      "Epoch 00213: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.2233 - acc: 0.9904 - score: 0.9914 - val_loss: 0.5510 - val_acc: 0.8450 - val_score: 0.8677\n",
      "Epoch 214/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9933 - score: 0.9944\n",
      "Epoch 00214: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.2140 - acc: 0.9930 - score: 0.9923 - val_loss: 0.5414 - val_acc: 0.8457 - val_score: 0.8693\n",
      "Epoch 215/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9910 - score: 0.9927\n",
      "Epoch 00215: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.2173 - acc: 0.9909 - score: 0.9919 - val_loss: 0.5463 - val_acc: 0.8422 - val_score: 0.8677\n",
      "Epoch 216/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9907 - score: 0.9922\n",
      "Epoch 00216: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.2164 - acc: 0.9904 - score: 0.9905 - val_loss: 0.5486 - val_acc: 0.8505 - val_score: 0.8727\n",
      "Epoch 217/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9902 - score: 0.9914\n",
      "Epoch 00217: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 903us/sample - loss: 0.2238 - acc: 0.9899 - score: 0.9897 - val_loss: 0.5725 - val_acc: 0.8422 - val_score: 0.8654\n",
      "Epoch 218/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9921 - score: 0.9936\n",
      "Epoch 00218: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 904us/sample - loss: 0.2197 - acc: 0.9918 - score: 0.9922 - val_loss: 0.5490 - val_acc: 0.8429 - val_score: 0.8668\n",
      "Epoch 219/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9928 - score: 0.9942\n",
      "Epoch 00219: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.2226 - acc: 0.9926 - score: 0.9933 - val_loss: 0.5522 - val_acc: 0.8464 - val_score: 0.8702\n",
      "Epoch 220/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9910 - score: 0.9923\n",
      "Epoch 00220: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2157 - acc: 0.9909 - score: 0.9923 - val_loss: 0.5461 - val_acc: 0.8429 - val_score: 0.8666\n",
      "Epoch 221/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9924 - score: 0.9937\n",
      "Epoch 00221: val_score did not improve from 0.87688\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.2089 - acc: 0.9921 - score: 0.9919 - val_loss: 0.5482 - val_acc: 0.8443 - val_score: 0.8674\n",
      "Epoch 222/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9933 - score: 0.9944\n",
      "Epoch 00222: val_score improved from 0.87688 to 0.87753, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 925us/sample - loss: 0.2125 - acc: 0.9933 - score: 0.9945 - val_loss: 0.5359 - val_acc: 0.8553 - val_score: 0.8775\n",
      "Epoch 223/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9904 - score: 0.9917\n",
      "Epoch 00223: val_score did not improve from 0.87753\n",
      "5834/5834 [==============================] - 5s 886us/sample - loss: 0.2145 - acc: 0.9904 - score: 0.9918 - val_loss: 0.5435 - val_acc: 0.8553 - val_score: 0.8761\n",
      "Epoch 224/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9909 - score: 0.9924\n",
      "Epoch 00224: val_score did not improve from 0.87753\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.2201 - acc: 0.9904 - score: 0.9895 - val_loss: 0.5477 - val_acc: 0.8477 - val_score: 0.8707\n",
      "Epoch 225/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9933 - score: 0.9944\n",
      "Epoch 00225: val_score did not improve from 0.87753\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 0.2127 - acc: 0.9933 - score: 0.9945 - val_loss: 0.5313 - val_acc: 0.8512 - val_score: 0.8746\n",
      "Epoch 226/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9911 - score: 0.9921\n",
      "Epoch 00226: val_score did not improve from 0.87753\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.2185 - acc: 0.9909 - score: 0.9912 - val_loss: 0.5611 - val_acc: 0.8484 - val_score: 0.8711\n",
      "Epoch 227/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9892 - score: 0.9906\n",
      "Epoch 00227: val_score did not improve from 0.87753\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.2212 - acc: 0.9890 - score: 0.9898 - val_loss: 0.5512 - val_acc: 0.8498 - val_score: 0.8721\n",
      "Epoch 228/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9918 - score: 0.9927\n",
      "Epoch 00228: val_score did not improve from 0.87753\n",
      "5834/5834 [==============================] - 5s 908us/sample - loss: 0.2242 - acc: 0.9914 - score: 0.9908 - val_loss: 0.5998 - val_acc: 0.8306 - val_score: 0.8558\n",
      "Epoch 229/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9895 - score: 0.9911\n",
      "Epoch 00229: val_score improved from 0.87753 to 0.88047, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 5s 923us/sample - loss: 0.2167 - acc: 0.9894 - score: 0.9902 - val_loss: 0.5384 - val_acc: 0.8580 - val_score: 0.8805\n",
      "Epoch 230/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9921 - score: 0.9931\n",
      "Epoch 00230: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 0.2106 - acc: 0.9919 - score: 0.9921 - val_loss: 0.5426 - val_acc: 0.8491 - val_score: 0.8717\n",
      "Epoch 231/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9929 - score: 0.9942\n",
      "Epoch 00231: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 904us/sample - loss: 0.2123 - acc: 0.9930 - score: 0.9944 - val_loss: 0.5544 - val_acc: 0.8471 - val_score: 0.8699\n",
      "Epoch 232/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9897 - score: 0.9910\n",
      "Epoch 00232: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2205 - acc: 0.9895 - score: 0.9904 - val_loss: 0.5612 - val_acc: 0.8443 - val_score: 0.8687\n",
      "Epoch 233/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9919 - score: 0.9930\n",
      "Epoch 00233: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 0.2143 - acc: 0.9919 - score: 0.9930 - val_loss: 0.5437 - val_acc: 0.8519 - val_score: 0.8746\n",
      "Epoch 234/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9936 - score: 0.9945\n",
      "Epoch 00234: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.2132 - acc: 0.9935 - score: 0.9936 - val_loss: 0.5434 - val_acc: 0.8539 - val_score: 0.8756\n",
      "Epoch 235/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9909 - score: 0.9923\n",
      "Epoch 00235: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.2135 - acc: 0.9906 - score: 0.9907 - val_loss: 0.5614 - val_acc: 0.8464 - val_score: 0.8687\n",
      "Epoch 236/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9911 - score: 0.9927\n",
      "Epoch 00236: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2155 - acc: 0.9909 - score: 0.9910 - val_loss: 0.5366 - val_acc: 0.8560 - val_score: 0.8773\n",
      "Epoch 237/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9940 - score: 0.9949\n",
      "Epoch 00237: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.2079 - acc: 0.9937 - score: 0.9933 - val_loss: 0.5601 - val_acc: 0.8477 - val_score: 0.8698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9921 - score: 0.9934\n",
      "Epoch 00238: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 905us/sample - loss: 0.2185 - acc: 0.9918 - score: 0.9915 - val_loss: 0.5403 - val_acc: 0.8484 - val_score: 0.8725\n",
      "Epoch 239/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9930 - score: 0.9940\n",
      "Epoch 00239: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 0.2163 - acc: 0.9926 - score: 0.9923 - val_loss: 0.5514 - val_acc: 0.8457 - val_score: 0.8683\n",
      "Epoch 240/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9931 - score: 0.9942\n",
      "Epoch 00240: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 905us/sample - loss: 0.2127 - acc: 0.9926 - score: 0.9911 - val_loss: 0.5521 - val_acc: 0.8567 - val_score: 0.8796\n",
      "Epoch 241/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9940 - score: 0.9950\n",
      "Epoch 00241: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 891us/sample - loss: 0.2072 - acc: 0.9935 - score: 0.9929 - val_loss: 0.5499 - val_acc: 0.8539 - val_score: 0.8753\n",
      "Epoch 242/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9924 - score: 0.9935\n",
      "Epoch 00242: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2177 - acc: 0.9923 - score: 0.9927 - val_loss: 0.5901 - val_acc: 0.8409 - val_score: 0.8636\n",
      "Epoch 243/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9919 - score: 0.9931\n",
      "Epoch 00243: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 0.2151 - acc: 0.9919 - score: 0.9932 - val_loss: 0.5427 - val_acc: 0.8505 - val_score: 0.8728\n",
      "Epoch 244/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9911 - score: 0.9922\n",
      "Epoch 00244: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 894us/sample - loss: 0.2110 - acc: 0.9907 - score: 0.9909 - val_loss: 0.6058 - val_acc: 0.8340 - val_score: 0.8590\n",
      "Epoch 245/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9918 - score: 0.9931\n",
      "Epoch 00245: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 898us/sample - loss: 0.2083 - acc: 0.9918 - score: 0.9932 - val_loss: 0.5403 - val_acc: 0.8567 - val_score: 0.8793\n",
      "Epoch 246/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9921 - score: 0.9933\n",
      "Epoch 00246: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 900us/sample - loss: 0.2128 - acc: 0.9919 - score: 0.9923 - val_loss: 0.5609 - val_acc: 0.8450 - val_score: 0.8700\n",
      "Epoch 247/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9912 - score: 0.9926\n",
      "Epoch 00247: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 903us/sample - loss: 0.2133 - acc: 0.9911 - score: 0.9918 - val_loss: 0.5337 - val_acc: 0.8546 - val_score: 0.8767\n",
      "Epoch 248/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9928 - score: 0.9938\n",
      "Epoch 00248: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.2074 - acc: 0.9928 - score: 0.9939 - val_loss: 0.5398 - val_acc: 0.8505 - val_score: 0.8723\n",
      "Epoch 249/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9926 - score: 0.9940\n",
      "Epoch 00249: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.\n",
      "\n",
      "Epoch 00249: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 902us/sample - loss: 0.2058 - acc: 0.9923 - score: 0.9922 - val_loss: 0.5336 - val_acc: 0.8546 - val_score: 0.8764\n",
      "249 0.4\n",
      "Epoch 250/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9942 - score: 0.9950\n",
      "Epoch 00250: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 899us/sample - loss: 0.2121 - acc: 0.9937 - score: 0.9921 - val_loss: 0.5516 - val_acc: 0.8567 - val_score: 0.8791\n",
      "Epoch 251/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9947 - score: 0.9954\n",
      "Epoch 00251: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 903us/sample - loss: 0.2040 - acc: 0.9943 - score: 0.9934 - val_loss: 0.5396 - val_acc: 0.8567 - val_score: 0.8787\n",
      "Epoch 252/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9938 - score: 0.9948\n",
      "Epoch 00252: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 895us/sample - loss: 0.2090 - acc: 0.9935 - score: 0.9927 - val_loss: 0.5381 - val_acc: 0.8539 - val_score: 0.8750\n",
      "Epoch 253/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9945 - score: 0.9955\n",
      "Epoch 00253: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 896us/sample - loss: 0.2026 - acc: 0.9945 - score: 0.9955 - val_loss: 0.5353 - val_acc: 0.8560 - val_score: 0.8771\n",
      "Epoch 254/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9950 - score: 0.9958\n",
      "Epoch 00254: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 901us/sample - loss: 0.2026 - acc: 0.9947 - score: 0.9939 - val_loss: 0.5328 - val_acc: 0.8553 - val_score: 0.8775\n",
      "Epoch 255/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9933 - score: 0.9944\n",
      "Epoch 00255: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.1992 - acc: 0.9930 - score: 0.9926 - val_loss: 0.5440 - val_acc: 0.8553 - val_score: 0.8777\n",
      "Epoch 256/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9961 - score: 0.9968\n",
      "Epoch 00256: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 897us/sample - loss: 0.2020 - acc: 0.9957 - score: 0.9948 - val_loss: 0.5409 - val_acc: 0.8525 - val_score: 0.8748\n",
      "Epoch 257/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9950 - score: 0.9958\n",
      "Epoch 00257: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 910us/sample - loss: 0.2005 - acc: 0.9949 - score: 0.9951 - val_loss: 0.5369 - val_acc: 0.8546 - val_score: 0.8751\n",
      "Epoch 258/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9931 - score: 0.9941\n",
      "Epoch 00258: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 933us/sample - loss: 0.2087 - acc: 0.9926 - score: 0.9910 - val_loss: 0.5397 - val_acc: 0.8525 - val_score: 0.8750\n",
      "Epoch 259/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9938 - score: 0.9946\n",
      "Epoch 00259: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 934us/sample - loss: 0.2021 - acc: 0.9935 - score: 0.9927 - val_loss: 0.5328 - val_acc: 0.8573 - val_score: 0.8787\n",
      "Epoch 260/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9940 - score: 0.9949\n",
      "Epoch 00260: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 939us/sample - loss: 0.2014 - acc: 0.9938 - score: 0.9939 - val_loss: 0.5410 - val_acc: 0.8573 - val_score: 0.8798\n",
      "Epoch 261/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9934 - score: 0.9946\n",
      "Epoch 00261: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 6s 947us/sample - loss: 0.2002 - acc: 0.9930 - score: 0.9934 - val_loss: 0.5366 - val_acc: 0.8594 - val_score: 0.8798\n",
      "Epoch 262/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9948 - score: 0.9957\n",
      "Epoch 00262: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 6s 949us/sample - loss: 0.2038 - acc: 0.9947 - score: 0.9947 - val_loss: 0.5373 - val_acc: 0.8525 - val_score: 0.8741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9916 - score: 0.9928\n",
      "Epoch 00263: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 936us/sample - loss: 0.2074 - acc: 0.9914 - score: 0.9918 - val_loss: 0.5332 - val_acc: 0.8573 - val_score: 0.8781\n",
      "Epoch 264/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9936 - score: 0.9945\n",
      "Epoch 00264: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 6s 947us/sample - loss: 0.2067 - acc: 0.9933 - score: 0.9929 - val_loss: 0.5378 - val_acc: 0.8546 - val_score: 0.8767\n",
      "Epoch 265/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9948 - score: 0.9955\n",
      "Epoch 00265: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 924us/sample - loss: 0.2032 - acc: 0.9945 - score: 0.9936 - val_loss: 0.5348 - val_acc: 0.8567 - val_score: 0.8771\n",
      "Epoch 266/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9926 - score: 0.9936\n",
      "Epoch 00266: val_score did not improve from 0.88047\n",
      "5834/5834 [==============================] - 5s 939us/sample - loss: 0.2110 - acc: 0.9919 - score: 0.9898 - val_loss: 0.5433 - val_acc: 0.8573 - val_score: 0.8778\n",
      "Epoch 267/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9935 - score: 0.9944\n",
      "Epoch 00267: val_score improved from 0.88047 to 0.88155, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 6s 975us/sample - loss: 0.2065 - acc: 0.9930 - score: 0.9913 - val_loss: 0.5282 - val_acc: 0.8608 - val_score: 0.8815\n",
      "Epoch 268/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9952 - score: 0.9958\n",
      "Epoch 00268: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 5s 934us/sample - loss: 0.2004 - acc: 0.9952 - score: 0.9958 - val_loss: 0.5258 - val_acc: 0.8553 - val_score: 0.8763\n",
      "Epoch 269/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9937 - score: 0.9949\n",
      "Epoch 00269: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 945us/sample - loss: 0.2034 - acc: 0.9933 - score: 0.9930 - val_loss: 0.5445 - val_acc: 0.8505 - val_score: 0.8737\n",
      "Epoch 270/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9945 - score: 0.9952\n",
      "Epoch 00270: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 949us/sample - loss: 0.2005 - acc: 0.9943 - score: 0.9944 - val_loss: 0.5371 - val_acc: 0.8532 - val_score: 0.8749\n",
      "Epoch 271/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9940 - score: 0.9949\n",
      "Epoch 00271: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 949us/sample - loss: 0.1998 - acc: 0.9935 - score: 0.9922 - val_loss: 0.5296 - val_acc: 0.8546 - val_score: 0.8773\n",
      "Epoch 272/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9943 - score: 0.9953\n",
      "Epoch 00272: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 964us/sample - loss: 0.2007 - acc: 0.9942 - score: 0.9943 - val_loss: 0.5388 - val_acc: 0.8512 - val_score: 0.8737\n",
      "Epoch 273/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9947 - score: 0.9956- ETA: 2s - l\n",
      "Epoch 00273: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 5s 941us/sample - loss: 0.2065 - acc: 0.9943 - score: 0.9938 - val_loss: 0.5330 - val_acc: 0.8525 - val_score: 0.8748\n",
      "Epoch 274/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9940 - score: 0.9950- ETA: 1s - loss: 0.2031 -\n",
      "Epoch 00274: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 946us/sample - loss: 0.2020 - acc: 0.9940 - score: 0.9950 - val_loss: 0.5382 - val_acc: 0.8560 - val_score: 0.8779\n",
      "Epoch 275/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9940 - score: 0.9950\n",
      "Epoch 00275: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 949us/sample - loss: 0.2053 - acc: 0.9940 - score: 0.9950 - val_loss: 0.5376 - val_acc: 0.8546 - val_score: 0.8756\n",
      "Epoch 276/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9943 - score: 0.9951\n",
      "Epoch 00276: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 949us/sample - loss: 0.1997 - acc: 0.9940 - score: 0.9930 - val_loss: 0.5282 - val_acc: 0.8546 - val_score: 0.8760\n",
      "Epoch 277/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9955 - score: 0.9964\n",
      "Epoch 00277: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 5s 942us/sample - loss: 0.2017 - acc: 0.9952 - score: 0.9948 - val_loss: 0.5324 - val_acc: 0.8587 - val_score: 0.8798\n",
      "Epoch 278/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9936 - score: 0.9946\n",
      "Epoch 00278: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 5s 942us/sample - loss: 0.2031 - acc: 0.9937 - score: 0.9946 - val_loss: 0.5393 - val_acc: 0.8567 - val_score: 0.8780\n",
      "Epoch 279/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9938 - score: 0.9947\n",
      "Epoch 00279: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 5s 936us/sample - loss: 0.2001 - acc: 0.9938 - score: 0.9948 - val_loss: 0.5328 - val_acc: 0.8594 - val_score: 0.8802\n",
      "Epoch 280/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9943 - score: 0.9951\n",
      "Epoch 00280: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 945us/sample - loss: 0.2013 - acc: 0.9943 - score: 0.9951 - val_loss: 0.5370 - val_acc: 0.8560 - val_score: 0.8764\n",
      "Epoch 281/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9935 - score: 0.9946\n",
      "Epoch 00281: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 5s 942us/sample - loss: 0.2037 - acc: 0.9933 - score: 0.9937 - val_loss: 0.5302 - val_acc: 0.8546 - val_score: 0.8767\n",
      "Epoch 282/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9947 - score: 0.9956\n",
      "Epoch 00282: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 943us/sample - loss: 0.1969 - acc: 0.9945 - score: 0.9947 - val_loss: 0.5348 - val_acc: 0.8512 - val_score: 0.8728\n",
      "Epoch 283/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9940 - score: 0.9950\n",
      "Epoch 00283: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 5s 941us/sample - loss: 0.2030 - acc: 0.9937 - score: 0.9929 - val_loss: 0.5401 - val_acc: 0.8539 - val_score: 0.8754\n",
      "Epoch 284/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9955 - score: 0.9963\n",
      "Epoch 00284: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 5s 932us/sample - loss: 0.2014 - acc: 0.9955 - score: 0.9964 - val_loss: 0.5306 - val_acc: 0.8567 - val_score: 0.8778\n",
      "Epoch 285/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9929 - score: 0.9940\n",
      "Epoch 00285: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 5s 932us/sample - loss: 0.2016 - acc: 0.9925 - score: 0.9914 - val_loss: 0.5366 - val_acc: 0.8525 - val_score: 0.8742\n",
      "Epoch 286/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9935 - score: 0.9943\n",
      "Epoch 00286: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 947us/sample - loss: 0.2026 - acc: 0.9933 - score: 0.9933 - val_loss: 0.5314 - val_acc: 0.8539 - val_score: 0.8760\n",
      "Epoch 287/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9921 - score: 0.9932\n",
      "Epoch 00287: ReduceLROnPlateau reducing learning rate to 4.09600033890456e-06.\n",
      "\n",
      "Epoch 00287: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 951us/sample - loss: 0.2078 - acc: 0.9918 - score: 0.9914 - val_loss: 0.5342 - val_acc: 0.8567 - val_score: 0.8772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287 0.4\n",
      "Epoch 288/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9931 - score: 0.9942\n",
      "Epoch 00288: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 961us/sample - loss: 0.2051 - acc: 0.9928 - score: 0.9924 - val_loss: 0.5286 - val_acc: 0.8567 - val_score: 0.8783\n",
      "Epoch 289/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9942 - score: 0.9950\n",
      "Epoch 00289: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 944us/sample - loss: 0.2024 - acc: 0.9940 - score: 0.9944 - val_loss: 0.5293 - val_acc: 0.8594 - val_score: 0.8806\n",
      "Epoch 290/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9948 - score: 0.9956\n",
      "Epoch 00290: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 943us/sample - loss: 0.2027 - acc: 0.9947 - score: 0.9950 - val_loss: 0.5293 - val_acc: 0.8553 - val_score: 0.8773\n",
      "Epoch 291/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9933 - score: 0.9946\n",
      "Epoch 00291: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 5s 935us/sample - loss: 0.1972 - acc: 0.9933 - score: 0.9946 - val_loss: 0.5273 - val_acc: 0.8573 - val_score: 0.8777\n",
      "Epoch 292/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9948 - score: 0.9957\n",
      "Epoch 00292: val_score did not improve from 0.88155\n",
      "5834/5834 [==============================] - 6s 944us/sample - loss: 0.1934 - acc: 0.9947 - score: 0.9949 - val_loss: 0.5281 - val_acc: 0.8594 - val_score: 0.8799\n",
      "Epoch 293/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9942 - score: 0.9952\n",
      "Epoch 00293: val_score improved from 0.88155 to 0.88208, saving model to aspp_baseline2.h5\n",
      "5834/5834 [==============================] - 6s 958us/sample - loss: 0.1989 - acc: 0.9942 - score: 0.9953 - val_loss: 0.5292 - val_acc: 0.8615 - val_score: 0.8821\n",
      "Epoch 294/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9935 - score: 0.9945\n",
      "Epoch 00294: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 936us/sample - loss: 0.2034 - acc: 0.9933 - score: 0.9935 - val_loss: 0.5342 - val_acc: 0.8594 - val_score: 0.8807\n",
      "Epoch 295/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9955 - score: 0.9961\n",
      "Epoch 00295: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 6s 943us/sample - loss: 0.2026 - acc: 0.9955 - score: 0.9962 - val_loss: 0.5321 - val_acc: 0.8580 - val_score: 0.8787\n",
      "Epoch 296/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9943 - score: 0.9952\n",
      "Epoch 00296: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 940us/sample - loss: 0.2019 - acc: 0.9940 - score: 0.9931 - val_loss: 0.5313 - val_acc: 0.8587 - val_score: 0.8795\n",
      "Epoch 297/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9946 - score: 0.9953\n",
      "Epoch 00297: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 6s 945us/sample - loss: 0.1983 - acc: 0.9943 - score: 0.9938 - val_loss: 0.5338 - val_acc: 0.8539 - val_score: 0.8758\n",
      "Epoch 298/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9957 - score: 0.9963\n",
      "Epoch 00298: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 930us/sample - loss: 0.1945 - acc: 0.9957 - score: 0.9964 - val_loss: 0.5362 - val_acc: 0.8539 - val_score: 0.8759\n",
      "Epoch 299/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9942 - score: 0.9952\n",
      "Epoch 00299: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 933us/sample - loss: 0.2037 - acc: 0.9938 - score: 0.9932 - val_loss: 0.5333 - val_acc: 0.8560 - val_score: 0.8775\n",
      "Epoch 300/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9936 - score: 0.9945\n",
      "Epoch 00300: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 6s 945us/sample - loss: 0.2049 - acc: 0.9937 - score: 0.9945 - val_loss: 0.5328 - val_acc: 0.8573 - val_score: 0.8783\n",
      "Epoch 301/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9938 - score: 0.9947\n",
      "Epoch 00301: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 936us/sample - loss: 0.2001 - acc: 0.9933 - score: 0.9916 - val_loss: 0.5288 - val_acc: 0.8580 - val_score: 0.8787\n",
      "Epoch 302/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9950 - score: 0.9958\n",
      "Epoch 00302: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 933us/sample - loss: 0.1970 - acc: 0.9950 - score: 0.9958 - val_loss: 0.5297 - val_acc: 0.8567 - val_score: 0.8779\n",
      "Epoch 303/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9952 - score: 0.9960\n",
      "Epoch 00303: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 6s 953us/sample - loss: 0.1995 - acc: 0.9950 - score: 0.9951 - val_loss: 0.5279 - val_acc: 0.8567 - val_score: 0.8778\n",
      "Epoch 304/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9918 - score: 0.9930\n",
      "Epoch 00304: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 936us/sample - loss: 0.2042 - acc: 0.9914 - score: 0.9915 - val_loss: 0.5279 - val_acc: 0.8560 - val_score: 0.8775\n",
      "Epoch 305/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9938 - score: 0.9948\n",
      "Epoch 00305: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 940us/sample - loss: 0.1960 - acc: 0.9938 - score: 0.9948 - val_loss: 0.5337 - val_acc: 0.8560 - val_score: 0.8770\n",
      "Epoch 306/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9954 - score: 0.9960\n",
      "Epoch 00306: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 936us/sample - loss: 0.1982 - acc: 0.9954 - score: 0.9960 - val_loss: 0.5325 - val_acc: 0.8573 - val_score: 0.8779\n",
      "Epoch 307/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9961 - score: 0.9966\n",
      "Epoch 00307: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 940us/sample - loss: 0.1994 - acc: 0.9957 - score: 0.9944 - val_loss: 0.5294 - val_acc: 0.8567 - val_score: 0.8785\n",
      "Epoch 308/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9945 - score: 0.9953\n",
      "Epoch 00308: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 928us/sample - loss: 0.1965 - acc: 0.9945 - score: 0.9954 - val_loss: 0.5321 - val_acc: 0.8546 - val_score: 0.8764\n",
      "Epoch 309/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9959 - score: 0.9966\n",
      "Epoch 00309: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 6s 947us/sample - loss: 0.1912 - acc: 0.9959 - score: 0.9966 - val_loss: 0.5320 - val_acc: 0.8532 - val_score: 0.8744\n",
      "Epoch 310/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9940 - score: 0.9950\n",
      "Epoch 00310: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 6s 951us/sample - loss: 0.2040 - acc: 0.9938 - score: 0.9941 - val_loss: 0.5322 - val_acc: 0.8573 - val_score: 0.8778\n",
      "Epoch 311/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9955 - score: 0.9962\n",
      "Epoch 00311: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 6s 952us/sample - loss: 0.2010 - acc: 0.9952 - score: 0.9941 - val_loss: 0.5338 - val_acc: 0.8525 - val_score: 0.8745\n",
      "Epoch 312/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9940 - score: 0.9949\n",
      "Epoch 00312: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 937us/sample - loss: 0.1972 - acc: 0.9940 - score: 0.9950 - val_loss: 0.5324 - val_acc: 0.8560 - val_score: 0.8769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9948 - score: 0.9955\n",
      "Epoch 00313: ReduceLROnPlateau reducing learning rate to 1.6384001355618238e-06.\n",
      "\n",
      "Epoch 00313: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 942us/sample - loss: 0.1931 - acc: 0.9949 - score: 0.9956 - val_loss: 0.5352 - val_acc: 0.8532 - val_score: 0.8754\n",
      "313 0.4\n",
      "Epoch 314/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9950 - score: 0.9959\n",
      "Epoch 00314: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 6s 951us/sample - loss: 0.2006 - acc: 0.9945 - score: 0.9926 - val_loss: 0.5326 - val_acc: 0.8532 - val_score: 0.8744\n",
      "Epoch 315/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9952 - score: 0.9959\n",
      "Epoch 00315: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 939us/sample - loss: 0.1956 - acc: 0.9943 - score: 0.9910 - val_loss: 0.5307 - val_acc: 0.8553 - val_score: 0.8767\n",
      "Epoch 316/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9936 - score: 0.9947\n",
      "Epoch 00316: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 919us/sample - loss: 0.1998 - acc: 0.9937 - score: 0.9947 - val_loss: 0.5296 - val_acc: 0.8546 - val_score: 0.8769\n",
      "Epoch 317/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9948 - score: 0.9956\n",
      "Epoch 00317: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 933us/sample - loss: 0.2026 - acc: 0.9943 - score: 0.9929 - val_loss: 0.5301 - val_acc: 0.8560 - val_score: 0.8774\n",
      "Epoch 318/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9957 - score: 0.9966- ETA: 2s - loss: 0.190\n",
      "Epoch 00318: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 931us/sample - loss: 0.1952 - acc: 0.9955 - score: 0.9957 - val_loss: 0.5310 - val_acc: 0.8573 - val_score: 0.8786\n",
      "Epoch 319/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9952 - score: 0.9960\n",
      "Epoch 00319: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 917us/sample - loss: 0.1944 - acc: 0.9950 - score: 0.9949 - val_loss: 0.5312 - val_acc: 0.8553 - val_score: 0.8769\n",
      "Epoch 320/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9936 - score: 0.9944\n",
      "Epoch 00320: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 915us/sample - loss: 0.1986 - acc: 0.9935 - score: 0.9938 - val_loss: 0.5315 - val_acc: 0.8553 - val_score: 0.8768\n",
      "Epoch 321/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9942 - score: 0.9949\n",
      "Epoch 00321: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 910us/sample - loss: 0.1978 - acc: 0.9938 - score: 0.9930 - val_loss: 0.5293 - val_acc: 0.8580 - val_score: 0.8789\n",
      "Epoch 322/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9962 - score: 0.9968\n",
      "Epoch 00322: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 918us/sample - loss: 0.1949 - acc: 0.9959 - score: 0.9948 - val_loss: 0.5298 - val_acc: 0.8573 - val_score: 0.8781\n",
      "Epoch 323/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9938 - score: 0.9946\n",
      "Epoch 00323: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 908us/sample - loss: 0.2015 - acc: 0.9937 - score: 0.9937 - val_loss: 0.5282 - val_acc: 0.8573 - val_score: 0.8784\n",
      "Epoch 324/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9948 - score: 0.9955- ETA: 1s - loss: 0.18\n",
      "Epoch 00324: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 914us/sample - loss: 0.1998 - acc: 0.9945 - score: 0.9935 - val_loss: 0.5288 - val_acc: 0.8580 - val_score: 0.8786\n",
      "Epoch 325/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9937 - score: 0.9946\n",
      "Epoch 00325: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 906us/sample - loss: 0.2036 - acc: 0.9938 - score: 0.9948 - val_loss: 0.5295 - val_acc: 0.8560 - val_score: 0.8779\n",
      "Epoch 326/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9946 - score: 0.9954\n",
      "Epoch 00326: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 913us/sample - loss: 0.1953 - acc: 0.9943 - score: 0.9936 - val_loss: 0.5312 - val_acc: 0.8573 - val_score: 0.8792\n",
      "Epoch 327/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9947 - score: 0.9955\n",
      "Epoch 00327: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 918us/sample - loss: 0.1997 - acc: 0.9945 - score: 0.9946 - val_loss: 0.5312 - val_acc: 0.8573 - val_score: 0.8785\n",
      "Epoch 328/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9948 - score: 0.9955\n",
      "Epoch 00328: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 914us/sample - loss: 0.1910 - acc: 0.9947 - score: 0.9945 - val_loss: 0.5312 - val_acc: 0.8567 - val_score: 0.8783\n",
      "Epoch 329/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9943 - score: 0.9952\n",
      "Epoch 00329: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 915us/sample - loss: 0.1975 - acc: 0.9940 - score: 0.9932 - val_loss: 0.5309 - val_acc: 0.8573 - val_score: 0.8790\n",
      "Epoch 330/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9935 - score: 0.9945\n",
      "Epoch 00330: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 913us/sample - loss: 0.2017 - acc: 0.9931 - score: 0.9925 - val_loss: 0.5322 - val_acc: 0.8573 - val_score: 0.8795\n",
      "Epoch 331/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9950 - score: 0.9960\n",
      "Epoch 00331: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 919us/sample - loss: 0.2044 - acc: 0.9949 - score: 0.9953 - val_loss: 0.5305 - val_acc: 0.8560 - val_score: 0.8778\n",
      "Epoch 332/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9954 - score: 0.9960\n",
      "Epoch 00332: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 925us/sample - loss: 0.1943 - acc: 0.9952 - score: 0.9950 - val_loss: 0.5329 - val_acc: 0.8539 - val_score: 0.8755\n",
      "Epoch 333/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9946 - score: 0.9954\n",
      "Epoch 00333: ReduceLROnPlateau reducing learning rate to 6.553600542247295e-07.\n",
      "\n",
      "Epoch 00333: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 914us/sample - loss: 0.1946 - acc: 0.9943 - score: 0.9935 - val_loss: 0.5327 - val_acc: 0.8546 - val_score: 0.8768\n",
      "333 0.4\n",
      "Epoch 334/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9943 - score: 0.9951\n",
      "Epoch 00334: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 927us/sample - loss: 0.1941 - acc: 0.9938 - score: 0.9922 - val_loss: 0.5322 - val_acc: 0.8553 - val_score: 0.8768\n",
      "Epoch 335/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9948 - score: 0.9956\n",
      "Epoch 00335: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 917us/sample - loss: 0.1979 - acc: 0.9949 - score: 0.9956 - val_loss: 0.5341 - val_acc: 0.8553 - val_score: 0.8766\n",
      "Epoch 336/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9935 - score: 0.9944\n",
      "Epoch 00336: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 914us/sample - loss: 0.1976 - acc: 0.9935 - score: 0.9944 - val_loss: 0.5332 - val_acc: 0.8560 - val_score: 0.8776\n",
      "Epoch 337/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9952 - score: 0.9959\n",
      "Epoch 00337: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 912us/sample - loss: 0.1992 - acc: 0.9950 - score: 0.9952 - val_loss: 0.5315 - val_acc: 0.8553 - val_score: 0.8766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 338/800\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9960 - score: 0.9966\n",
      "Epoch 00338: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 911us/sample - loss: 0.1954 - acc: 0.9955 - score: 0.9945 - val_loss: 0.5302 - val_acc: 0.8580 - val_score: 0.8781\n",
      "Epoch 339/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9940 - score: 0.9950\n",
      "Epoch 00339: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 918us/sample - loss: 0.1987 - acc: 0.9938 - score: 0.9940 - val_loss: 0.5297 - val_acc: 0.8580 - val_score: 0.8791\n",
      "Epoch 340/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9952 - score: 0.9960\n",
      "Epoch 00340: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 908us/sample - loss: 0.1921 - acc: 0.9950 - score: 0.9951 - val_loss: 0.5315 - val_acc: 0.8567 - val_score: 0.8781\n",
      "Epoch 341/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9945 - score: 0.9953\n",
      "Epoch 00341: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 907us/sample - loss: 0.1985 - acc: 0.9942 - score: 0.9934 - val_loss: 0.5318 - val_acc: 0.8560 - val_score: 0.8770\n",
      "Epoch 342/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9955 - score: 0.9962\n",
      "Epoch 00342: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 910us/sample - loss: 0.1913 - acc: 0.9955 - score: 0.9962 - val_loss: 0.5325 - val_acc: 0.8567 - val_score: 0.8768\n",
      "Epoch 343/800\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9947 - score: 0.9955\n",
      "Epoch 00343: val_score did not improve from 0.88208\n",
      "5834/5834 [==============================] - 5s 932us/sample - loss: 0.2007 - acc: 0.9942 - score: 0.9928 - val_loss: 0.5319 - val_acc: 0.8567 - val_score: 0.8776\n",
      "Epoch 00343: early stopping\n",
      "Train on 5837 samples, validate on 1455 samples\n",
      "0 1.0000000474974513\n",
      "Epoch 1/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 2.2169 - acc: 0.2869 - score: 0.3755\n",
      "Epoch 00001: val_score improved from -inf to 0.19559, saving model to aspp_baseline3.h5\n",
      "5837/5837 [==============================] - 9s 2ms/sample - loss: 2.2175 - acc: 0.2864 - score: 0.3739 - val_loss: 4.4766 - val_acc: 0.0935 - val_score: 0.1956\n",
      "Epoch 2/800\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.9318 - acc: 0.3901 - score: 0.4706\n",
      "Epoch 00002: val_score improved from 0.19559 to 0.29734, saving model to aspp_baseline3.h5\n",
      "5837/5837 [==============================] - 5s 901us/sample - loss: 1.9308 - acc: 0.3896 - score: 0.4691 - val_loss: 2.5810 - val_acc: 0.1973 - val_score: 0.2973\n",
      "Epoch 3/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.8341 - acc: 0.4217 - score: 0.4993\n",
      "Epoch 00003: val_score improved from 0.29734 to 0.39981, saving model to aspp_baseline3.h5\n",
      "5837/5837 [==============================] - 5s 926us/sample - loss: 1.8342 - acc: 0.4220 - score: 0.5001 - val_loss: 2.1459 - val_acc: 0.2976 - val_score: 0.3998\n",
      "Epoch 4/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.7521 - acc: 0.4475 - score: 0.5235\n",
      "Epoch 00004: val_score improved from 0.39981 to 0.50637, saving model to aspp_baseline3.h5\n",
      "5837/5837 [==============================] - 5s 926us/sample - loss: 1.7521 - acc: 0.4477 - score: 0.5243 - val_loss: 1.8043 - val_acc: 0.4302 - val_score: 0.5064\n",
      "Epoch 5/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.7119 - acc: 0.4655 - score: 0.5393\n",
      "Epoch 00005: val_score did not improve from 0.50637\n",
      "5837/5837 [==============================] - 5s 904us/sample - loss: 1.7119 - acc: 0.4651 - score: 0.5379 - val_loss: 2.1786 - val_acc: 0.3010 - val_score: 0.3902\n",
      "Epoch 6/800\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.6587 - acc: 0.4806 - score: 0.5517\n",
      "Epoch 00006: val_score did not improve from 0.50637\n",
      "5837/5837 [==============================] - 5s 908us/sample - loss: 1.6561 - acc: 0.4821 - score: 0.5531 - val_loss: 1.9538 - val_acc: 0.3677 - val_score: 0.4638\n",
      "Epoch 7/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.6163 - acc: 0.4978 - score: 0.5690\n",
      "Epoch 00007: val_score did not improve from 0.50637\n",
      "5837/5837 [==============================] - 5s 916us/sample - loss: 1.6164 - acc: 0.4975 - score: 0.5680 - val_loss: 2.2362 - val_acc: 0.2680 - val_score: 0.3615\n",
      "Epoch 8/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.5885 - acc: 0.5055 - score: 0.5763\n",
      "Epoch 00008: val_score did not improve from 0.50637\n",
      "5837/5837 [==============================] - 5s 925us/sample - loss: 1.5892 - acc: 0.5049 - score: 0.5734 - val_loss: 1.8068 - val_acc: 0.4089 - val_score: 0.4816\n",
      "Epoch 9/800\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.5663 - acc: 0.5111 - score: 0.5799\n",
      "Epoch 00009: val_score improved from 0.50637 to 0.55602, saving model to aspp_baseline3.h5\n",
      "5837/5837 [==============================] - 6s 946us/sample - loss: 1.5644 - acc: 0.5124 - score: 0.5816 - val_loss: 1.6245 - val_acc: 0.4818 - val_score: 0.5560\n",
      "Epoch 10/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.5346 - acc: 0.5247 - score: 0.5929\n",
      "Epoch 00010: val_score did not improve from 0.55602\n",
      "5837/5837 [==============================] - 5s 936us/sample - loss: 1.5350 - acc: 0.5246 - score: 0.5923 - val_loss: 1.7994 - val_acc: 0.4344 - val_score: 0.5106\n",
      "Epoch 11/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.5016 - acc: 0.5278 - score: 0.5939\n",
      "Epoch 00011: val_score did not improve from 0.55602\n",
      "5837/5837 [==============================] - 5s 928us/sample - loss: 1.5029 - acc: 0.5273 - score: 0.5918 - val_loss: 1.9304 - val_acc: 0.3828 - val_score: 0.4681\n",
      "Epoch 12/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.4784 - acc: 0.5495 - score: 0.6137\n",
      "Epoch 00012: val_score improved from 0.55602 to 0.59107, saving model to aspp_baseline3.h5\n",
      "5837/5837 [==============================] - 6s 975us/sample - loss: 1.4786 - acc: 0.5496 - score: 0.6139 - val_loss: 1.5797 - val_acc: 0.5216 - val_score: 0.5911\n",
      "Epoch 13/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.4515 - acc: 0.5572 - score: 0.6190\n",
      "Epoch 00013: val_score did not improve from 0.59107\n",
      "5837/5837 [==============================] - 5s 914us/sample - loss: 1.4522 - acc: 0.5566 - score: 0.6167 - val_loss: 1.6624 - val_acc: 0.4887 - val_score: 0.5557\n",
      "Epoch 14/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.4193 - acc: 0.5677 - score: 0.628 - ETA: 0s - loss: 1.4185 - acc: 0.5682 - score: 0.6287\n",
      "Epoch 00014: val_score did not improve from 0.59107\n",
      "5837/5837 [==============================] - 6s 950us/sample - loss: 1.4190 - acc: 0.5683 - score: 0.6289 - val_loss: 1.7964 - val_acc: 0.4605 - val_score: 0.5373\n",
      "Epoch 15/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.4016 - acc: 0.5735 - score: 0.6345\n",
      "Epoch 00015: val_score did not improve from 0.59107\n",
      "5837/5837 [==============================] - 6s 954us/sample - loss: 1.4024 - acc: 0.5732 - score: 0.6339 - val_loss: 1.6224 - val_acc: 0.4838 - val_score: 0.5565\n",
      "Epoch 16/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.3714 - acc: 0.5802 - score: 0.6398\n",
      "Epoch 00016: val_score did not improve from 0.59107\n",
      "5837/5837 [==============================] - 6s 983us/sample - loss: 1.3712 - acc: 0.5804 - score: 0.6411 - val_loss: 2.3218 - val_acc: 0.3759 - val_score: 0.4601\n",
      "Epoch 17/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.3422 - acc: 0.5867 - score: 0.6456\n",
      "Epoch 00017: val_score did not improve from 0.59107\n",
      "5837/5837 [==============================] - 6s 951us/sample - loss: 1.3426 - acc: 0.5866 - score: 0.6450 - val_loss: 1.7171 - val_acc: 0.4625 - val_score: 0.5319\n",
      "Epoch 18/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.3332 - acc: 0.5913 - score: 0.6501\n",
      "Epoch 00018: val_score did not improve from 0.59107\n",
      "5837/5837 [==============================] - 6s 968us/sample - loss: 1.3338 - acc: 0.5912 - score: 0.6491 - val_loss: 1.6470 - val_acc: 0.4962 - val_score: 0.5644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.3186 - acc: 0.6087 - score: 0.6639\n",
      "Epoch 00019: val_score did not improve from 0.59107\n",
      "5837/5837 [==============================] - 6s 950us/sample - loss: 1.3202 - acc: 0.6080 - score: 0.6609 - val_loss: 2.3311 - val_acc: 0.3897 - val_score: 0.4626\n",
      "Epoch 20/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.3120 - acc: 0.6058 - score: 0.6610\n",
      "Epoch 00020: val_score did not improve from 0.59107\n",
      "5837/5837 [==============================] - 6s 948us/sample - loss: 1.3124 - acc: 0.6054 - score: 0.6598 - val_loss: 1.7182 - val_acc: 0.4351 - val_score: 0.5139\n",
      "Epoch 21/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.2781 - acc: 0.6121 - score: 0.6673\n",
      "Epoch 00021: val_score did not improve from 0.59107\n",
      "5837/5837 [==============================] - 6s 965us/sample - loss: 1.2784 - acc: 0.6121 - score: 0.6674 - val_loss: 1.8703 - val_acc: 0.4357 - val_score: 0.5166\n",
      "Epoch 22/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.2436 - acc: 0.6283 - score: 0.6805\n",
      "Epoch 00022: val_score improved from 0.59107 to 0.59717, saving model to aspp_baseline3.h5\n",
      "5837/5837 [==============================] - 5s 941us/sample - loss: 1.2448 - acc: 0.6275 - score: 0.6771 - val_loss: 1.5013 - val_acc: 0.5361 - val_score: 0.5972\n",
      "Epoch 23/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.2240 - acc: 0.6358 - score: 0.6874\n",
      "Epoch 00023: val_score did not improve from 0.59717\n",
      "5837/5837 [==============================] - 6s 953us/sample - loss: 1.2246 - acc: 0.6358 - score: 0.6869 - val_loss: 1.8059 - val_acc: 0.4591 - val_score: 0.5382\n",
      "Epoch 24/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.2027 - acc: 0.6513 - score: 0.7009\n",
      "Epoch 00024: val_score did not improve from 0.59717\n",
      "5837/5837 [==============================] - 5s 941us/sample - loss: 1.2041 - acc: 0.6507 - score: 0.6989 - val_loss: 1.5084 - val_acc: 0.5223 - val_score: 0.5936\n",
      "Epoch 25/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1855 - acc: 0.6520 - score: 0.7017- ETA: 0s - loss: 1.1808 - acc: 0.6539 - score: 0\n",
      "Epoch 00025: val_score did not improve from 0.59717\n",
      "5837/5837 [==============================] - 6s 962us/sample - loss: 1.1860 - acc: 0.6517 - score: 0.7001 - val_loss: 1.6813 - val_acc: 0.4729 - val_score: 0.5473\n",
      "Epoch 26/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1740 - acc: 0.6597 - score: 0.7078\n",
      "Epoch 00026: val_score did not improve from 0.59717\n",
      "5837/5837 [==============================] - 6s 948us/sample - loss: 1.1748 - acc: 0.6592 - score: 0.7061 - val_loss: 1.7758 - val_acc: 0.4742 - val_score: 0.5476\n",
      "Epoch 27/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1636 - acc: 0.6624 - score: 0.7099\n",
      "Epoch 00027: val_score improved from 0.59717 to 0.63149, saving model to aspp_baseline3.h5\n",
      "5837/5837 [==============================] - 6s 954us/sample - loss: 1.1648 - acc: 0.6622 - score: 0.7087 - val_loss: 1.3153 - val_acc: 0.5739 - val_score: 0.6315\n",
      "Epoch 28/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1422 - acc: 0.6657 - score: 0.7144\n",
      "Epoch 00028: val_score did not improve from 0.63149\n",
      "5837/5837 [==============================] - 5s 911us/sample - loss: 1.1429 - acc: 0.6659 - score: 0.7153 - val_loss: 1.6816 - val_acc: 0.5010 - val_score: 0.5764\n",
      "Epoch 29/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1267 - acc: 0.6729 - score: 0.7197\n",
      "Epoch 00029: val_score did not improve from 0.63149\n",
      "5837/5837 [==============================] - 5s 915us/sample - loss: 1.1270 - acc: 0.6728 - score: 0.7191 - val_loss: 1.8160 - val_acc: 0.4838 - val_score: 0.5586\n",
      "Epoch 30/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1100 - acc: 0.6861 - score: 0.7304\n",
      "Epoch 00030: val_score did not improve from 0.63149\n",
      "5837/5837 [==============================] - 5s 910us/sample - loss: 1.1112 - acc: 0.6855 - score: 0.7278 - val_loss: 1.9780 - val_acc: 0.4591 - val_score: 0.5263\n",
      "Epoch 31/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1056 - acc: 0.6787 - score: 0.7256\n",
      "Epoch 00031: val_score did not improve from 0.63149\n",
      "5837/5837 [==============================] - 5s 913us/sample - loss: 1.1058 - acc: 0.6786 - score: 0.7248 - val_loss: 1.7377 - val_acc: 0.4873 - val_score: 0.5561\n",
      "Epoch 32/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0882 - acc: 0.6913 - score: 0.7361\n",
      "Epoch 00032: val_score did not improve from 0.63149\n",
      "5837/5837 [==============================] - 5s 906us/sample - loss: 1.0884 - acc: 0.6911 - score: 0.7353 - val_loss: 1.6970 - val_acc: 0.5423 - val_score: 0.6028\n",
      "Epoch 33/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0785 - acc: 0.6961 - score: 0.7401\n",
      "Epoch 00033: val_score did not improve from 0.63149\n",
      "5837/5837 [==============================] - 5s 913us/sample - loss: 1.0793 - acc: 0.6959 - score: 0.7392 - val_loss: 2.2291 - val_acc: 0.3911 - val_score: 0.4719\n",
      "Epoch 34/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0535 - acc: 0.6971 - score: 0.7420\n",
      "Epoch 00034: val_score did not improve from 0.63149\n",
      "5837/5837 [==============================] - 5s 924us/sample - loss: 1.0541 - acc: 0.6969 - score: 0.7413 - val_loss: 2.4214 - val_acc: 0.3684 - val_score: 0.4552\n",
      "Epoch 35/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0628 - acc: 0.7002 - score: 0.7425\n",
      "Epoch 00035: val_score did not improve from 0.63149\n",
      "5837/5837 [==============================] - 5s 940us/sample - loss: 1.0631 - acc: 0.7004 - score: 0.7434 - val_loss: 1.6462 - val_acc: 0.4887 - val_score: 0.5614\n",
      "Epoch 36/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0389 - acc: 0.7121 - score: 0.7548\n",
      "Epoch 00036: val_score did not improve from 0.63149\n",
      "5837/5837 [==============================] - 6s 949us/sample - loss: 1.0399 - acc: 0.7118 - score: 0.7542 - val_loss: 1.7445 - val_acc: 0.4763 - val_score: 0.5471\n",
      "Epoch 37/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0227 - acc: 0.7177 - score: 0.7582\n",
      "Epoch 00037: val_score did not improve from 0.63149\n",
      "5837/5837 [==============================] - 5s 938us/sample - loss: 1.0229 - acc: 0.7175 - score: 0.7576 - val_loss: 2.1843 - val_acc: 0.3856 - val_score: 0.4688\n",
      "Epoch 38/800\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0359 - acc: 0.7071 - score: 0.7501\n",
      "Epoch 00038: val_score improved from 0.63149 to 0.69301, saving model to aspp_baseline3.h5\n",
      "5837/5837 [==============================] - 6s 968us/sample - loss: 1.0362 - acc: 0.7069 - score: 0.7493 - val_loss: 1.1620 - val_acc: 0.6433 - val_score: 0.6930\n",
      "Epoch 39/800\n",
      " 576/5837 [=>............................] - ETA: 4s - loss: 0.9790 - acc: 0.7101 - score: 0.7517"
     ]
    }
   ],
   "source": [
    "#模型加载与训练\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess=tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "kfold = StratifiedKFold(5, shuffle=True,random_state=12255877)\n",
    "proba_t = np.zeros((7500, 19))\n",
    "train_score=[]\n",
    "test_score=[]\n",
    "for fold,(xx,yy) in enumerate(kfold.split(x,y)):\n",
    "    tf.reset_default_graph()\n",
    "    inputs=Input(shape=[64,8,1])\n",
    "    outputs=CNN(inputs,num_classes=19)\n",
    "    model=Model(inputs=inputs,outputs=outputs)\n",
    "    _y=to_categorical(y,19)\n",
    "    plateau=ReduceLROnPlateau(monitor=\"val_score\",\n",
    "                                verbose=1,\n",
    "                                mode='max',\n",
    "                                factor=0.4,\n",
    "                                patience=20)\n",
    "    early_stopping=EarlyStopping(monitor='val_acc',\n",
    "                                   verbose=1,\n",
    "                                   mode='max',\n",
    "                                   patience=50)\n",
    "    checkpoint=ModelCheckpoint(f'aspp_baseline{fold}.h5',\n",
    "                                 monitor='val_score',\n",
    "                                 verbose=1,\n",
    "                                 mode='max',\n",
    "                                 save_best_only=True)\n",
    "    weight_decay=WeightDecayScheduler(init_lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=AdamW(lr=0.001,weight_decay=4e-4),metrics=[\"acc\",score])\n",
    "    trained_model=model.fit(\n",
    "            x[xx],\n",
    "            _y[xx],\n",
    "            batch_size=batch_size,\n",
    "            class_weight=(1-class_weight)**2,\n",
    "            shuffle=True,\n",
    "            validation_data=(x[yy],_y[yy]),\n",
    "            epochs=800,\n",
    "            callbacks=[plateau,early_stopping,checkpoint,weight_decay])\n",
    "    model.load_weights(f'aspp_baseline{fold}.h5')\n",
    "    proba_t+=model.predict(x_val, verbose=0, batch_size=1024)/5.\n",
    "    train_score.append(np.array(trained_model.history[\"score\"]).max())\n",
    "    test_score.append(np.array(trained_model.history[\"val_score\"]).max())\n",
    "label=proba_t.argmax(axis=1)\n",
    "print(\"on_train_set:\",np.array(train_score))\n",
    "print(\"average:\",np.array(train_score).mean())\n",
    "print(\"on_test_set:\",np.array(test_score))\n",
    "print(\"average:\",np.array(test_score).mean())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_train_set: [0.9955843  0.99118483 0.9907479  0.99437135 0.99428225]\n",
      "average: 0.99323416\n",
      "on_test_set: [0.87848437 0.8689438  0.8834407  0.8719805  0.871452  ]\n",
      "average: 0.8748603\n"
     ]
    }
   ],
   "source": [
    "#训练结果\n",
    "print(\"on_train_set:\",np.array(train_score))\n",
    "print(\"average:\",np.array(train_score).mean())\n",
    "print(\"on_test_set:\",np.array(test_score))\n",
    "print(\"average:\",np.array(test_score).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存预测的类别信息\n",
    "import pandas as pd\n",
    "frame = pd.DataFrame(label)\n",
    "frame.rename(columns={0:'behavior_id'},inplace = True)\n",
    "frame.reset_index(inplace = True)\n",
    "frame.rename(columns={'index':'fragment_id'},inplace = True)\n",
    "frame.to_csv('submit_reverse_8748_9932.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存预测的类别置信度信息\n",
    "import pandas as pd\n",
    "frame = pd.DataFrame(proba_t)\n",
    "frame.rename(columns={},inplace = True)\n",
    "frame.reset_index(inplace = True)\n",
    "frame.rename(columns={'index':'fragment_id'},inplace = True)\n",
    "frame.to_csv('submit_reverse_prob_87.48_99.32.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Layer in module tensorflow.python.keras.engine.base_layer:\n",
      "\n",
      "class Layer(tensorflow.python.module.module.Module)\n",
      " |  Layer(trainable=True, name=None, dtype=None, dynamic=False, **kwargs)\n",
      " |  \n",
      " |  Base layer class.\n",
      " |  \n",
      " |  This is the class from which all layers inherit.\n",
      " |  \n",
      " |  A layer is a class implementing common neural networks operations, such\n",
      " |  as convolution, batch norm, etc. These operations require managing weights,\n",
      " |  losses, updates, and inter-layer connectivity.\n",
      " |  \n",
      " |  Users will just instantiate a layer and then treat it as a callable.\n",
      " |  \n",
      " |  We recommend that descendants of `Layer` implement the following methods:\n",
      " |  \n",
      " |  * `__init__()`: Save configuration in member variables\n",
      " |  * `build()`: Called once from `__call__`, when we know the shapes of inputs\n",
      " |    and `dtype`. Should have the calls to `add_weight()`, and then\n",
      " |    call the super's `build()` (which sets `self.built = True`, which is\n",
      " |    nice in case the user wants to call `build()` manually before the\n",
      " |    first `__call__`).\n",
      " |  * `call()`: Called in `__call__` after making sure `build()` has been called\n",
      " |    once. Should actually perform the logic of applying the layer to the\n",
      " |    input tensors (which should be passed in as the first argument).\n",
      " |  \n",
      " |  Arguments:\n",
      " |    trainable: Boolean, whether the layer's variables should be trainable.\n",
      " |    name: String name of the layer.\n",
      " |    dtype: The dtype of the layer's computations and weights (default of\n",
      " |      `None` means use `tf.keras.backend.floatx` in TensorFlow 2, or the type\n",
      " |      of the first input in TensorFlow 1).\n",
      " |    dynamic: Set this to `True` if your layer should only be run eagerly, and\n",
      " |      should not be used to generate a static computation graph.\n",
      " |      This would be the case for a Tree-RNN or a recursive network,\n",
      " |      for example, or generally for any layer that manipulates tensors\n",
      " |      using Python control flow. If `False`, we assume that the layer can\n",
      " |      safely be used to generate a static computation graph.\n",
      " |  \n",
      " |  Read-only properties:\n",
      " |    name: The name of the layer (string).\n",
      " |    dtype: The dtype of the layer's computations and weights. If mixed\n",
      " |      precision is used with a `tf.keras.mixed_precision.experimental.Policy`,\n",
      " |      this is instead just the dtype of the layer's weights, as the computations\n",
      " |      are done in a different dtype.\n",
      " |    updates: List of update ops of this layer.\n",
      " |    losses: List of losses added by this layer.\n",
      " |    trainable_weights: List of variables to be included in backprop.\n",
      " |    non_trainable_weights: List of variables that should not be\n",
      " |      included in backprop.\n",
      " |    weights: The concatenation of the lists trainable_weights and\n",
      " |      non_trainable_weights (in this order).\n",
      " |  \n",
      " |  Mutable properties:\n",
      " |    trainable: Whether the layer should be trained (boolean).\n",
      " |    input_spec: Optional (list of) `InputSpec` object(s) specifying the\n",
      " |      constraints on inputs that can be accepted by the layer.\n",
      " |  \n",
      " |  ### Dtypes and casting\n",
      " |  Each layer has a dtype, which is typically the dtype of the layer's\n",
      " |  computations and variables. A layer's dtype can be queried via the\n",
      " |  `Layer.dtype` property. The dtype is specified with the `dtype` constructor\n",
      " |  argument. In TensorFlow 2, the dtype defaults to `tf.keras.backend.floatx()`\n",
      " |  if no dtype is passed. `floatx()` itself defaults to \"float32\". Additionally,\n",
      " |  layers will cast their inputs to the layer's dtype in TensorFlow 2. For\n",
      " |  example:\n",
      " |  \n",
      " |  ```\n",
      " |  x = tf.ones((4, 4, 4, 4), dtype='float64')\n",
      " |  layer = tf.keras.layers.Conv2D(filters=4, kernel_size=2)\n",
      " |  print(layer.dtype)  # float32\n",
      " |  \n",
      " |  # `layer` casts it's inputs to layer.dtype, which is float32, and does\n",
      " |  # computations in float32.\n",
      " |  y = layer(x)\n",
      " |  ```\n",
      " |  \n",
      " |  Currently, only tensors in the first argument to the layer's `call` method are\n",
      " |  casted. For example:\n",
      " |  \n",
      " |  ```\n",
      " |  class MyLayer(tf.keras.layers.Layer):\n",
      " |    # Bug! `b` will not be casted.\n",
      " |    def call(self, a, b):\n",
      " |      return a + 1., b + 1.\n",
      " |  \n",
      " |  a = tf.constant(1., dtype=\"float32\")\n",
      " |  b = tf.constant(1., dtype=\"float32\")\n",
      " |  \n",
      " |  layer = MyLayer(dtype=\"float64\")\n",
      " |  x, y = layer(a, b)\n",
      " |  print(x.dtype)  # float64\n",
      " |  print(y.dtype)  # float32. Not casted since `b` was not passed to first input\n",
      " |  ```\n",
      " |  \n",
      " |  It is recommended to accept tensors only in the first argument. This way,\n",
      " |  all tensors are casted to the layer's dtype. `MyLayer` should therefore be\n",
      " |  written as:\n",
      " |  \n",
      " |  ```\n",
      " |  class MyLayer(tf.keras.layers.Layer):\n",
      " |    # Now, all tensor inputs will be casted.\n",
      " |    def call(self, inputs):\n",
      " |      a, b = inputs\n",
      " |      return a + 1., b + 1.\n",
      " |  \n",
      " |  a = tf.constant(1., dtype=\"float32\")\n",
      " |  b = tf.constant(1., dtype=\"float32\")\n",
      " |  \n",
      " |  layer = MyLayer(dtype=\"float64\")\n",
      " |  x, y = layer((a, b))\n",
      " |  print(x.dtype)  # float64\n",
      " |  print(y.dtype)  # float64.\n",
      " |  ```\n",
      " |  \n",
      " |  In a future minor release, tensors in other arguments may be casted as well.\n",
      " |  \n",
      " |  Currently, other arguments are not automatically casted for\n",
      " |  technical reasons, but this may change in a future minor release.\n",
      " |  \n",
      " |  A layer subclass can prevent its inputs from being autocasted by passing\n",
      " |  `autocast=False` to the layer constructor. For example:\n",
      " |  \n",
      " |  ```\n",
      " |  class MyLayer(tf.keras.layers.Layer):\n",
      " |  \n",
      " |    def __init__(self, **kwargs):\n",
      " |      kwargs['autocast']=False\n",
      " |      super(MyLayer, self).__init__(**kwargs)\n",
      " |  \n",
      " |    def call(self, inp):\n",
      " |      return inp\n",
      " |  \n",
      " |  x = tf.ones((4, 4, 4, 4), dtype='float64')\n",
      " |  layer = MyLayer()\n",
      " |  print(layer.dtype)  # float32.\n",
      " |  y = layer(x)  # MyLayer will not cast inputs to it's dtype of float32\n",
      " |  print(y.dtype)  # float64\n",
      " |  ```\n",
      " |  \n",
      " |  #### Running models in float64 in TensorFlow 2\n",
      " |  \n",
      " |  If you want to run a Model in float64, you can set floatx to be float64 by\n",
      " |  calling `tf.keras.backend.set_floatx('float64')`. This will cause all layers\n",
      " |  to default to float64 instead of float32:\n",
      " |  \n",
      " |  ```\n",
      " |  tf.keras.backend.set_floatx('float64')\n",
      " |  layer1 = tf.keras.layers.Dense(4)\n",
      " |  layer2 = tf.keras.layers.Dense(4)\n",
      " |  \n",
      " |  x = tf.ones((4, 4))\n",
      " |  y = layer2(layer1(x))  # Both layers run in float64\n",
      " |  ```\n",
      " |  \n",
      " |  Alternatively, you can pass `dtype='float64'` to each individual layer. Note\n",
      " |  that if you have any layers which contain other layers as members, you must\n",
      " |  ensure each sublayer gets `dtype='float64'` passed to it's constructor as\n",
      " |  well:\n",
      " |  \n",
      " |  ```\n",
      " |  layer1 = tf.keras.layers.Dense(4, dtype='float64')\n",
      " |  layer2 = tf.keras.layers.Dense(4, dtype='float64')\n",
      " |  \n",
      " |  x = tf.ones((4, 4))\n",
      " |  y = layer2(layer1(x))  # Both layers run in float64\n",
      " |  \n",
      " |  class NestedLayer(tf.keras.layers.Layer):\n",
      " |    def __init__(self, **kwargs):\n",
      " |      super(NestedLayer, self).__init__(**kwargs)\n",
      " |      self.dense = tf.keras.layers.Dense(4, dtype=kwargs.get('dtype'))\n",
      " |  \n",
      " |    def call(self, inp):\n",
      " |      return self.dense(inp)\n",
      " |  \n",
      " |  layer3 = NestedLayer(dtype='float64')\n",
      " |  z = layer3(x)  # layer3's dense layer runs in float64, since NestedLayer\n",
      " |                 # correcty passed it's dtype to it's dense layer\n",
      " |  \n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, inputs, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __init__(self, trainable=True, name=None, dtype=None, dynamic=False, **kwargs)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Actvity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter` and\n",
      " |          `collections`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs, **kwargs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  dynamic\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this module as passed or determined in the ctor.\n",
      " |      \n",
      " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
      " |      parent module names.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of variables owned by this module and it's submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      ```\n",
      " |      class MyModule(tf.Module):\n",
      " |        @tf.Module.with_name_scope\n",
      " |        def __call__(self, x):\n",
      " |          if not hasattr(self, 'w'):\n",
      " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
      " |          return tf.matmul(x, self.w)\n",
      " |      ```\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      ```\n",
      " |      mod = MyModule()\n",
      " |      mod(tf.ones([8, 32]))\n",
      " |      # ==> <tf.Tensor: ...>\n",
      " |      mod.w\n",
      " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      ```\n",
      " |      a = tf.Module()\n",
      " |      b = tf.Module()\n",
      " |      c = tf.Module()\n",
      " |      a.b = b\n",
      " |      b.c = c\n",
      " |      assert list(a.submodules) == [b, c]\n",
      " |      assert list(b.submodules) == [c]\n",
      " |      assert list(c.submodules) == []\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.layers.Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_self_setattr_tracking': True,\n",
       " '_nested_outputs': <tf.Tensor 'softmax/Softmax:0' shape=(?, 19) dtype=float32>,\n",
       " '_nested_inputs': <tf.Tensor 'input_1:0' shape=(?, 64, 9, 1) dtype=float32>,\n",
       " 'inputs': [<tf.Tensor 'input_1:0' shape=(?, 64, 9, 1) dtype=float32>],\n",
       " 'outputs': [<tf.Tensor 'softmax/Softmax:0' shape=(?, 19) dtype=float32>],\n",
       " '_thread_local': <_thread._local at 0x22703123f48>,\n",
       " '_name': 'model',\n",
       " '_activity_regularizer': None,\n",
       " '_trainable': True,\n",
       " '_dynamic': False,\n",
       " '_is_compiled': True,\n",
       " '_layers': [<tensorflow.python.keras.engine.input_layer.InputLayer at 0x226df8faa08>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226f73dec48>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f29c1c88>,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226f6a77a48>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e9cbb808>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x227013a6088>,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226ef8d8308>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e408af88>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226ef941c48>,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226e4069188>,\n",
       "  <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x226e9cc8d48>,\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x22700620108>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e9cee508>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226e9d2cb48>,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x22670aa0188>,\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x226e9d29688>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22672111208>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226efe07e08>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226efdfe388>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22700109988>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22700103048>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x227048b0c08>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22703a0b188>,\n",
       "  <tensorflow.python.keras.layers.pooling.AveragePooling2D at 0x226efdfe0c8>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226efdd9608>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22700110c48>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22703bbd708>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x2270488b088>,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x22703a17148>,\n",
       "  <tensorflow.python.keras.layers.convolutional.UpSampling2D at 0x226efdf96c8>,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x227000f9248>,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x227025a3fc8>,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x22703b84188>,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x227048b7cc8>,\n",
       "  <tensorflow.python.keras.layers.merge.Add at 0x226efdec9c8>,\n",
       "  <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x227048a6188>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x227017f6b08>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x227017f6f88>,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d0808>,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d88c8>,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048dec08>,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d02c8>,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048e3c88>,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048e3848>,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x22703768448>,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x2267085e508>,\n",
       "  <tensorflow.python.keras.layers.merge.Add at 0x227017ee208>,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22703768248>,\n",
       "  <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x22703758b48>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x22703747608>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x22703746d88>,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x226f661c5c8>,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x226f660cf08>,\n",
       "  <tensorflow.python.keras.layers.merge.Multiply at 0x22703756888>,\n",
       "  <__main__.SoftThreshold at 0x226f661ce88>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f6607cc8>,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226f65f5588>,\n",
       "  <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x22672113888>,\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x226f9a03508>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x226f9a1c788>,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f9a09608>,\n",
       "  <tensorflow.python.keras.layers.advanced_activations.Softmax at 0x226f99f3408>],\n",
       " '_compute_output_and_mask_jointly': True,\n",
       " 'supports_masking': False,\n",
       " 'optimizer': <extend.AdamW at 0x226fad75c88>,\n",
       " '_trainable_weights': [],\n",
       " '_non_trainable_weights': [],\n",
       " '_updates': [],\n",
       " '_losses': [],\n",
       " '_callable_losses': [],\n",
       " '_metrics': [],\n",
       " '_scope': None,\n",
       " '_reuse': None,\n",
       " '_graph': <tensorflow.python.framework.ops.Graph at 0x226df8c5808>,\n",
       " '_dtype_policy': <Policy \"infer\", loss_scale=None>,\n",
       " '_dtype_defaulted_to_floatx': False,\n",
       " '_outbound_nodes': [],\n",
       " '_inbound_nodes': [<tensorflow.python.keras.engine.node.Node at 0x22703116f08>],\n",
       " '_trackable_saver': <tensorflow.python.training.tracking.util.TrackableSaver at 0x226feac3d08>,\n",
       " 'built': True,\n",
       " '_is_graph_network': True,\n",
       " '_expects_training_arg': True,\n",
       " '_expects_mask_arg': True,\n",
       " '_autocast': False,\n",
       " '_input_layers': [<tensorflow.python.keras.engine.input_layer.InputLayer at 0x226df8faa08>],\n",
       " '_output_layers': [<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x226f99f3408>],\n",
       " '_input_coordinates': [(<tensorflow.python.keras.engine.input_layer.InputLayer at 0x226df8faa08>,\n",
       "   0,\n",
       "   0)],\n",
       " '_output_coordinates': [(<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x226f99f3408>,\n",
       "   0,\n",
       "   0)],\n",
       " '_output_mask_cache': {},\n",
       " '_output_tensor_cache': {},\n",
       " '_output_shape_cache': {},\n",
       " '_network_nodes': {'activation_1_ib-0',\n",
       "  'activation_2_ib-0',\n",
       "  'activation_3_ib-0',\n",
       "  'activation_4_ib-0',\n",
       "  'activation_5_ib-0',\n",
       "  'activation_6_ib-0',\n",
       "  'activation_7_ib-0',\n",
       "  'activation_8_ib-0',\n",
       "  'activation_9_ib-0',\n",
       "  'activation_ib-0',\n",
       "  'add_1_ib-0',\n",
       "  'add_ib-0',\n",
       "  'average_pooling2d_ib-0',\n",
       "  'batch_normalization_10_ib-0',\n",
       "  'batch_normalization_1_ib-0',\n",
       "  'batch_normalization_2_ib-0',\n",
       "  'batch_normalization_3_ib-0',\n",
       "  'batch_normalization_4_ib-0',\n",
       "  'batch_normalization_5_ib-0',\n",
       "  'batch_normalization_6_ib-0',\n",
       "  'batch_normalization_7_ib-0',\n",
       "  'batch_normalization_8_ib-0',\n",
       "  'batch_normalization_9_ib-0',\n",
       "  'batch_normalization_ib-0',\n",
       "  'conv2d_10_ib-0',\n",
       "  'conv2d_1_ib-0',\n",
       "  'conv2d_2_ib-0',\n",
       "  'conv2d_3_ib-0',\n",
       "  'conv2d_4_ib-0',\n",
       "  'conv2d_5_ib-0',\n",
       "  'conv2d_6_ib-0',\n",
       "  'conv2d_7_ib-0',\n",
       "  'conv2d_8_ib-0',\n",
       "  'conv2d_9_ib-0',\n",
       "  'conv2d_ib-0',\n",
       "  'dense_1_ib-0',\n",
       "  'dense_2_ib-0',\n",
       "  'dense_3_ib-0',\n",
       "  'dense_4_ib-0',\n",
       "  'dense_ib-0',\n",
       "  'dropout_1_ib-0',\n",
       "  'dropout_2_ib-0',\n",
       "  'dropout_ib-0',\n",
       "  'global_average_pooling2d_1_ib-0',\n",
       "  'global_average_pooling2d_2_ib-0',\n",
       "  'global_average_pooling2d_ib-0',\n",
       "  'input_1_ib-0',\n",
       "  'max_pooling2d_ib-0',\n",
       "  'multiply_ib-0',\n",
       "  'soft_threshold_ib-0',\n",
       "  'softmax_ib-0',\n",
       "  'tf_op_layer_ExpandDims_1_ib-0',\n",
       "  'tf_op_layer_ExpandDims_2_ib-0',\n",
       "  'tf_op_layer_ExpandDims_3_ib-0',\n",
       "  'tf_op_layer_ExpandDims_ib-0',\n",
       "  'tf_op_layer_mul_1_ib-0',\n",
       "  'tf_op_layer_mul_2_ib-0',\n",
       "  'tf_op_layer_mul_3_ib-0',\n",
       "  'tf_op_layer_mul_4_ib-0',\n",
       "  'tf_op_layer_mul_5_ib-0',\n",
       "  'tf_op_layer_mul_ib-0',\n",
       "  'up_sampling2d_ib-0'},\n",
       " '_nodes_by_depth': defaultdict(list,\n",
       "             {0: [<tensorflow.python.keras.engine.node.Node at 0x226feac1288>],\n",
       "              1: [<tensorflow.python.keras.engine.node.Node at 0x227031162c8>],\n",
       "              2: [<tensorflow.python.keras.engine.node.Node at 0x22703124f48>],\n",
       "              3: [<tensorflow.python.keras.engine.node.Node at 0x226f9a0e208>],\n",
       "              4: [<tensorflow.python.keras.engine.node.Node at 0x226f9a1cec8>],\n",
       "              5: [<tensorflow.python.keras.engine.node.Node at 0x226f9a1ca88>],\n",
       "              6: [<tensorflow.python.keras.engine.node.Node at 0x226f9a19988>],\n",
       "              7: [<tensorflow.python.keras.engine.node.Node at 0x226f65ff848>],\n",
       "              14: [<tensorflow.python.keras.engine.node.Node at 0x22703757588>],\n",
       "              8: [<tensorflow.python.keras.engine.node.Node at 0x226f6607c48>],\n",
       "              9: [<tensorflow.python.keras.engine.node.Node at 0x226f6607648>],\n",
       "              10: [<tensorflow.python.keras.engine.node.Node at 0x226f660c308>],\n",
       "              11: [<tensorflow.python.keras.engine.node.Node at 0x226f65fe348>],\n",
       "              12: [<tensorflow.python.keras.engine.node.Node at 0x22703763288>],\n",
       "              13: [<tensorflow.python.keras.engine.node.Node at 0x227037462c8>],\n",
       "              15: [<tensorflow.python.keras.engine.node.Node at 0x227048d7388>],\n",
       "              16: [<tensorflow.python.keras.engine.node.Node at 0x226f295e788>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x227048e33c8>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x227037681c8>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x2267085e188>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x227048ea3c8>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x227048ea488>],\n",
       "              23: [<tensorflow.python.keras.engine.node.Node at 0x22701811fc8>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x227048b0888>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x22703bad3c8>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x22700105d48>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x226efde8408>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x226efdd8f08>],\n",
       "              17: [<tensorflow.python.keras.engine.node.Node at 0x227048fd248>],\n",
       "              18: [<tensorflow.python.keras.engine.node.Node at 0x227048d8c88>],\n",
       "              19: [<tensorflow.python.keras.engine.node.Node at 0x227048d7288>],\n",
       "              20: [<tensorflow.python.keras.engine.node.Node at 0x227017eae88>],\n",
       "              21: [<tensorflow.python.keras.engine.node.Node at 0x227017f5d88>],\n",
       "              22: [<tensorflow.python.keras.engine.node.Node at 0x227017ff448>],\n",
       "              24: [<tensorflow.python.keras.engine.node.Node at 0x22701815d08>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x22703baadc8>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x22703bb4bc8>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x227001098c8>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x226efdecf88>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x226efe07908>],\n",
       "              25: [<tensorflow.python.keras.engine.node.Node at 0x227048a1a88>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x22703b99888>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x227025b9888>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x227000fcb08>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x226efe01b48>,\n",
       "               <tensorflow.python.keras.engine.node.Node at 0x22703a07608>],\n",
       "              26: [<tensorflow.python.keras.engine.node.Node at 0x22672101608>],\n",
       "              27: [<tensorflow.python.keras.engine.node.Node at 0x22672111108>],\n",
       "              28: [<tensorflow.python.keras.engine.node.Node at 0x226720f3308>],\n",
       "              29: [<tensorflow.python.keras.engine.node.Node at 0x22670ad1ac8>],\n",
       "              30: [<tensorflow.python.keras.engine.node.Node at 0x226e9cee8c8>],\n",
       "              31: [<tensorflow.python.keras.engine.node.Node at 0x226e9cee0c8>],\n",
       "              32: [<tensorflow.python.keras.engine.node.Node at 0x22700620888>],\n",
       "              33: [<tensorflow.python.keras.engine.node.Node at 0x226e9cc8c88>],\n",
       "              34: [<tensorflow.python.keras.engine.node.Node at 0x226ef93db88>],\n",
       "              35: [<tensorflow.python.keras.engine.node.Node at 0x226e408ab88>],\n",
       "              36: [<tensorflow.python.keras.engine.node.Node at 0x226e3b068c8>],\n",
       "              37: [<tensorflow.python.keras.engine.node.Node at 0x226f058a088>],\n",
       "              38: [<tensorflow.python.keras.engine.node.Node at 0x22673d4bec8>],\n",
       "              39: [<tensorflow.python.keras.engine.node.Node at 0x226ef93bb08>],\n",
       "              40: [<tensorflow.python.keras.engine.node.Node at 0x22704902bc8>],\n",
       "              41: [<tensorflow.python.keras.engine.node.Node at 0x227014a0608>]}),\n",
       " '_layer_call_argspecs': {<tensorflow.python.keras.engine.input_layer.InputLayer at 0x226df8faa08>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226f73dec48>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f29c1c88>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226f6a77a48>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e9cbb808>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x227013a6088>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226ef8d8308>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e408af88>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226ef941c48>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226e4069188>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x226e9cc8d48>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x22700620108>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e9cee508>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226e9d2cb48>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x22670aa0188>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x226e9d29688>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22672111208>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226efe07e08>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226efdfe388>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22700109988>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22700103048>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x227048b0c08>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22703a0b188>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.pooling.AveragePooling2D at 0x226efdfe0c8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226efdd9608>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22700110c48>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22703bbd708>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x2270488b088>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x22703a17148>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.UpSampling2D at 0x226efdf96c8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x227000f9248>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x227025a3fc8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x22703b84188>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x227048b7cc8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.merge.Add at 0x226efdec9c8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x227048a6188>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x227017f6b08>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x227017f6f88>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d0808>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d88c8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048dec08>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d02c8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048e3c88>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048e3848>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x22703768448>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x2267085e508>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.merge.Add at 0x227017ee208>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22703768248>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x22703758b48>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x22703747608>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x22703746d88>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x226f661c5c8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x226f660cf08>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.merge.Multiply at 0x22703756888>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <__main__.SoftThreshold at 0x226f661ce88>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f6607cc8>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226f65f5588>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x22672113888>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x226f9a03508>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x226f9a1c788>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f9a09608>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.advanced_activations.Softmax at 0x226f99f3408>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})},\n",
       " '_self_unconditional_checkpoint_dependencies': [TrackableReference(name='layer-0', ref=<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x00000226DF8FAA08>),\n",
       "  TrackableReference(name='layer_with_weights-0', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226F73DEC48>),\n",
       "  TrackableReference(name='layer-1', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226F73DEC48>),\n",
       "  TrackableReference(name='layer_with_weights-1', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226F29C1C88>),\n",
       "  TrackableReference(name='layer-2', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226F29C1C88>),\n",
       "  TrackableReference(name='layer-3', ref=<tensorflow.python.keras.layers.core.Activation object at 0x00000226F6A77A48>),\n",
       "  TrackableReference(name='layer_with_weights-2', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226E9CBB808>),\n",
       "  TrackableReference(name='layer-4', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226E9CBB808>),\n",
       "  TrackableReference(name='layer_with_weights-3', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000227013A6088>),\n",
       "  TrackableReference(name='layer-5', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000227013A6088>),\n",
       "  TrackableReference(name='layer-6', ref=<tensorflow.python.keras.layers.core.Activation object at 0x00000226EF8D8308>),\n",
       "  TrackableReference(name='layer_with_weights-4', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226E408AF88>),\n",
       "  TrackableReference(name='layer-7', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226E408AF88>),\n",
       "  TrackableReference(name='layer_with_weights-5', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226EF941C48>),\n",
       "  TrackableReference(name='layer-8', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226EF941C48>),\n",
       "  TrackableReference(name='layer-9', ref=<tensorflow.python.keras.layers.core.Activation object at 0x00000226E4069188>),\n",
       "  TrackableReference(name='layer-10', ref=<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x00000226E9CC8D48>),\n",
       "  TrackableReference(name='layer-11', ref=<tensorflow.python.keras.layers.core.Dropout object at 0x0000022700620108>),\n",
       "  TrackableReference(name='layer_with_weights-6', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226E9CEE508>),\n",
       "  TrackableReference(name='layer-12', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226E9CEE508>),\n",
       "  TrackableReference(name='layer_with_weights-7', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226E9D2CB48>),\n",
       "  TrackableReference(name='layer-13', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226E9D2CB48>),\n",
       "  TrackableReference(name='layer-14', ref=<tensorflow.python.keras.layers.core.Activation object at 0x0000022670AA0188>),\n",
       "  TrackableReference(name='layer-15', ref=<tensorflow.python.keras.layers.core.Dropout object at 0x00000226E9D29688>),\n",
       "  TrackableReference(name='layer_with_weights-8', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x0000022672111208>),\n",
       "  TrackableReference(name='layer-16', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x0000022672111208>),\n",
       "  TrackableReference(name='layer_with_weights-9', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226EFE07E08>),\n",
       "  TrackableReference(name='layer-17', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226EFE07E08>),\n",
       "  TrackableReference(name='layer_with_weights-10', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226EFDFE388>),\n",
       "  TrackableReference(name='layer-18', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000226EFDFE388>),\n",
       "  TrackableReference(name='layer_with_weights-11', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x0000022700109988>),\n",
       "  TrackableReference(name='layer-19', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x0000022700109988>),\n",
       "  TrackableReference(name='layer_with_weights-12', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x0000022700103048>),\n",
       "  TrackableReference(name='layer-20', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x0000022700103048>),\n",
       "  TrackableReference(name='layer_with_weights-13', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000227048B0C08>),\n",
       "  TrackableReference(name='layer-21', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000227048B0C08>),\n",
       "  TrackableReference(name='layer_with_weights-14', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x0000022703A0B188>),\n",
       "  TrackableReference(name='layer-22', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x0000022703A0B188>),\n",
       "  TrackableReference(name='layer-23', ref=<tensorflow.python.keras.layers.pooling.AveragePooling2D object at 0x00000226EFDFE0C8>),\n",
       "  TrackableReference(name='layer_with_weights-15', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226EFDD9608>),\n",
       "  TrackableReference(name='layer-24', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226EFDD9608>),\n",
       "  TrackableReference(name='layer_with_weights-16', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x0000022700110C48>),\n",
       "  TrackableReference(name='layer-25', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x0000022700110C48>),\n",
       "  TrackableReference(name='layer_with_weights-17', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x0000022703BBD708>),\n",
       "  TrackableReference(name='layer-26', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x0000022703BBD708>),\n",
       "  TrackableReference(name='layer_with_weights-18', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000002270488B088>),\n",
       "  TrackableReference(name='layer-27', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000002270488B088>),\n",
       "  TrackableReference(name='layer-28', ref=<tensorflow.python.keras.layers.core.Activation object at 0x0000022703A17148>),\n",
       "  TrackableReference(name='layer-29', ref=<tensorflow.python.keras.layers.convolutional.UpSampling2D object at 0x00000226EFDF96C8>),\n",
       "  TrackableReference(name='layer-30', ref=<tensorflow.python.keras.layers.core.Activation object at 0x00000227000F9248>),\n",
       "  TrackableReference(name='layer-31', ref=<tensorflow.python.keras.layers.core.Activation object at 0x00000227025A3FC8>),\n",
       "  TrackableReference(name='layer-32', ref=<tensorflow.python.keras.layers.core.Activation object at 0x0000022703B84188>),\n",
       "  TrackableReference(name='layer-33', ref=<tensorflow.python.keras.layers.core.Activation object at 0x00000227048B7CC8>),\n",
       "  TrackableReference(name='layer-34', ref=<tensorflow.python.keras.layers.merge.Add object at 0x00000226EFDEC9C8>),\n",
       "  TrackableReference(name='layer-35', ref=<tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D object at 0x00000227048A6188>),\n",
       "  TrackableReference(name='layer_with_weights-19', ref=<tensorflow.python.keras.layers.core.Dense object at 0x00000227017F6B08>),\n",
       "  TrackableReference(name='layer-36', ref=<tensorflow.python.keras.layers.core.Dense object at 0x00000227017F6B08>),\n",
       "  TrackableReference(name='layer_with_weights-20', ref=<tensorflow.python.keras.layers.core.Dense object at 0x00000227017F6F88>),\n",
       "  TrackableReference(name='layer-37', ref=<tensorflow.python.keras.layers.core.Dense object at 0x00000227017F6F88>),\n",
       "  TrackableReference(name='layer-38', ref=<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x00000227048D0808>),\n",
       "  TrackableReference(name='layer-39', ref=<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x00000227048D88C8>),\n",
       "  TrackableReference(name='layer-40', ref=<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x00000227048DEC08>),\n",
       "  TrackableReference(name='layer-41', ref=<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x00000227048D02C8>),\n",
       "  TrackableReference(name='layer-42', ref=<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x00000227048E3C88>),\n",
       "  TrackableReference(name='layer-43', ref=<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x00000227048E3848>),\n",
       "  TrackableReference(name='layer-44', ref=<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x0000022703768448>),\n",
       "  TrackableReference(name='layer-45', ref=<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x000002267085E508>),\n",
       "  TrackableReference(name='layer-46', ref=<tensorflow.python.keras.layers.merge.Add object at 0x00000227017EE208>),\n",
       "  TrackableReference(name='layer_with_weights-21', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x0000022703768248>),\n",
       "  TrackableReference(name='layer-47', ref=<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x0000022703768248>),\n",
       "  TrackableReference(name='layer-48', ref=<tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D object at 0x0000022703758B48>),\n",
       "  TrackableReference(name='layer_with_weights-22', ref=<tensorflow.python.keras.layers.core.Dense object at 0x0000022703747608>),\n",
       "  TrackableReference(name='layer-49', ref=<tensorflow.python.keras.layers.core.Dense object at 0x0000022703747608>),\n",
       "  TrackableReference(name='layer_with_weights-23', ref=<tensorflow.python.keras.layers.core.Dense object at 0x0000022703746D88>),\n",
       "  TrackableReference(name='layer-50', ref=<tensorflow.python.keras.layers.core.Dense object at 0x0000022703746D88>),\n",
       "  TrackableReference(name='layer-51', ref=<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x00000226F661C5C8>),\n",
       "  TrackableReference(name='layer-52', ref=<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x00000226F660CF08>),\n",
       "  TrackableReference(name='layer-53', ref=<tensorflow.python.keras.layers.merge.Multiply object at 0x0000022703756888>),\n",
       "  TrackableReference(name='layer-54', ref=<__main__.SoftThreshold object at 0x00000226F661CE88>),\n",
       "  TrackableReference(name='layer_with_weights-24', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226F6607CC8>),\n",
       "  TrackableReference(name='layer-55', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226F6607CC8>),\n",
       "  TrackableReference(name='layer-56', ref=<tensorflow.python.keras.layers.core.Activation object at 0x00000226F65F5588>),\n",
       "  TrackableReference(name='layer-57', ref=<tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D object at 0x0000022672113888>),\n",
       "  TrackableReference(name='layer-58', ref=<tensorflow.python.keras.layers.core.Dropout object at 0x00000226F9A03508>),\n",
       "  TrackableReference(name='layer_with_weights-25', ref=<tensorflow.python.keras.layers.core.Dense object at 0x00000226F9A1C788>),\n",
       "  TrackableReference(name='layer-59', ref=<tensorflow.python.keras.layers.core.Dense object at 0x00000226F9A1C788>),\n",
       "  TrackableReference(name='layer_with_weights-26', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226F9A09608>),\n",
       "  TrackableReference(name='layer-60', ref=<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x00000226F9A09608>),\n",
       "  TrackableReference(name='layer-61', ref=<tensorflow.python.keras.layers.advanced_activations.Softmax object at 0x00000226F99F3408>),\n",
       "  TrackableReference(name='optimizer', ref=<extend.AdamW object at 0x00000226FAD75C88>)],\n",
       " '_self_unconditional_dependency_names': {'layer-0': <tensorflow.python.keras.engine.input_layer.InputLayer at 0x226df8faa08>,\n",
       "  'layer_with_weights-0': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226f73dec48>,\n",
       "  'layer-1': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226f73dec48>,\n",
       "  'layer_with_weights-1': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f29c1c88>,\n",
       "  'layer-2': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f29c1c88>,\n",
       "  'layer-3': <tensorflow.python.keras.layers.core.Activation at 0x226f6a77a48>,\n",
       "  'layer_with_weights-2': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e9cbb808>,\n",
       "  'layer-4': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e9cbb808>,\n",
       "  'layer_with_weights-3': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x227013a6088>,\n",
       "  'layer-5': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x227013a6088>,\n",
       "  'layer-6': <tensorflow.python.keras.layers.core.Activation at 0x226ef8d8308>,\n",
       "  'layer_with_weights-4': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e408af88>,\n",
       "  'layer-7': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e408af88>,\n",
       "  'layer_with_weights-5': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226ef941c48>,\n",
       "  'layer-8': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226ef941c48>,\n",
       "  'layer-9': <tensorflow.python.keras.layers.core.Activation at 0x226e4069188>,\n",
       "  'layer-10': <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x226e9cc8d48>,\n",
       "  'layer-11': <tensorflow.python.keras.layers.core.Dropout at 0x22700620108>,\n",
       "  'layer_with_weights-6': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e9cee508>,\n",
       "  'layer-12': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e9cee508>,\n",
       "  'layer_with_weights-7': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226e9d2cb48>,\n",
       "  'layer-13': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226e9d2cb48>,\n",
       "  'layer-14': <tensorflow.python.keras.layers.core.Activation at 0x22670aa0188>,\n",
       "  'layer-15': <tensorflow.python.keras.layers.core.Dropout at 0x226e9d29688>,\n",
       "  'layer_with_weights-8': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22672111208>,\n",
       "  'layer-16': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22672111208>,\n",
       "  'layer_with_weights-9': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226efe07e08>,\n",
       "  'layer-17': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226efe07e08>,\n",
       "  'layer_with_weights-10': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226efdfe388>,\n",
       "  'layer-18': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226efdfe388>,\n",
       "  'layer_with_weights-11': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22700109988>,\n",
       "  'layer-19': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22700109988>,\n",
       "  'layer_with_weights-12': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22700103048>,\n",
       "  'layer-20': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22700103048>,\n",
       "  'layer_with_weights-13': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x227048b0c08>,\n",
       "  'layer-21': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x227048b0c08>,\n",
       "  'layer_with_weights-14': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22703a0b188>,\n",
       "  'layer-22': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22703a0b188>,\n",
       "  'layer-23': <tensorflow.python.keras.layers.pooling.AveragePooling2D at 0x226efdfe0c8>,\n",
       "  'layer_with_weights-15': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226efdd9608>,\n",
       "  'layer-24': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226efdd9608>,\n",
       "  'layer_with_weights-16': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22700110c48>,\n",
       "  'layer-25': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22700110c48>,\n",
       "  'layer_with_weights-17': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22703bbd708>,\n",
       "  'layer-26': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22703bbd708>,\n",
       "  'layer_with_weights-18': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x2270488b088>,\n",
       "  'layer-27': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x2270488b088>,\n",
       "  'layer-28': <tensorflow.python.keras.layers.core.Activation at 0x22703a17148>,\n",
       "  'layer-29': <tensorflow.python.keras.layers.convolutional.UpSampling2D at 0x226efdf96c8>,\n",
       "  'layer-30': <tensorflow.python.keras.layers.core.Activation at 0x227000f9248>,\n",
       "  'layer-31': <tensorflow.python.keras.layers.core.Activation at 0x227025a3fc8>,\n",
       "  'layer-32': <tensorflow.python.keras.layers.core.Activation at 0x22703b84188>,\n",
       "  'layer-33': <tensorflow.python.keras.layers.core.Activation at 0x227048b7cc8>,\n",
       "  'layer-34': <tensorflow.python.keras.layers.merge.Add at 0x226efdec9c8>,\n",
       "  'layer-35': <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x227048a6188>,\n",
       "  'layer_with_weights-19': <tensorflow.python.keras.layers.core.Dense at 0x227017f6b08>,\n",
       "  'layer-36': <tensorflow.python.keras.layers.core.Dense at 0x227017f6b08>,\n",
       "  'layer_with_weights-20': <tensorflow.python.keras.layers.core.Dense at 0x227017f6f88>,\n",
       "  'layer-37': <tensorflow.python.keras.layers.core.Dense at 0x227017f6f88>,\n",
       "  'layer-38': <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d0808>,\n",
       "  'layer-39': <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d88c8>,\n",
       "  'layer-40': <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048dec08>,\n",
       "  'layer-41': <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d02c8>,\n",
       "  'layer-42': <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048e3c88>,\n",
       "  'layer-43': <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048e3848>,\n",
       "  'layer-44': <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x22703768448>,\n",
       "  'layer-45': <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x2267085e508>,\n",
       "  'layer-46': <tensorflow.python.keras.layers.merge.Add at 0x227017ee208>,\n",
       "  'layer_with_weights-21': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22703768248>,\n",
       "  'layer-47': <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22703768248>,\n",
       "  'layer-48': <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x22703758b48>,\n",
       "  'layer_with_weights-22': <tensorflow.python.keras.layers.core.Dense at 0x22703747608>,\n",
       "  'layer-49': <tensorflow.python.keras.layers.core.Dense at 0x22703747608>,\n",
       "  'layer_with_weights-23': <tensorflow.python.keras.layers.core.Dense at 0x22703746d88>,\n",
       "  'layer-50': <tensorflow.python.keras.layers.core.Dense at 0x22703746d88>,\n",
       "  'layer-51': <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x226f661c5c8>,\n",
       "  'layer-52': <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x226f660cf08>,\n",
       "  'layer-53': <tensorflow.python.keras.layers.merge.Multiply at 0x22703756888>,\n",
       "  'layer-54': <__main__.SoftThreshold at 0x226f661ce88>,\n",
       "  'layer_with_weights-24': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f6607cc8>,\n",
       "  'layer-55': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f6607cc8>,\n",
       "  'layer-56': <tensorflow.python.keras.layers.core.Activation at 0x226f65f5588>,\n",
       "  'layer-57': <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x22672113888>,\n",
       "  'layer-58': <tensorflow.python.keras.layers.core.Dropout at 0x226f9a03508>,\n",
       "  'layer_with_weights-25': <tensorflow.python.keras.layers.core.Dense at 0x226f9a1c788>,\n",
       "  'layer-59': <tensorflow.python.keras.layers.core.Dense at 0x226f9a1c788>,\n",
       "  'layer_with_weights-26': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f9a09608>,\n",
       "  'layer-60': <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f9a09608>,\n",
       "  'layer-61': <tensorflow.python.keras.layers.advanced_activations.Softmax at 0x226f99f3408>,\n",
       "  'optimizer': <extend.AdamW at 0x226fad75c88>},\n",
       " '_self_unconditional_deferred_dependencies': {},\n",
       " '_self_update_uid': -1,\n",
       " '_self_name_based_restores': set(),\n",
       " 'output_names': ['softmax'],\n",
       " 'input_names': ['input_1'],\n",
       " '_feed_input_names': ['input_1'],\n",
       " '_feed_inputs': [<tf.Tensor 'input_1:0' shape=(?, 64, 9, 1) dtype=float32>],\n",
       " '_feed_input_shapes': [(None, 64, 9, 1)],\n",
       " '_distribution_strategy': None,\n",
       " '_compile_time_distribution_strategy': <tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy at 0x2245d7087c8>,\n",
       " '_compile_distribution': False,\n",
       " '_run_eagerly': None,\n",
       " '_experimental_run_tf_function': False,\n",
       " 'loss': 'categorical_crossentropy',\n",
       " 'loss_weights': None,\n",
       " 'sample_weight_mode': None,\n",
       " '_compile_metrics': ['acc', <function __main__.score(y_true, y_pred)>],\n",
       " '_compile_weighted_metrics': None,\n",
       " '_training_endpoints': [<tensorflow.python.keras.engine.training._TrainingEndpoint at 0x226feaeb608>],\n",
       " '_compiled_trainable_state': {<tensorflow.python.keras.engine.training.Model at 0x2270311f848>: True,\n",
       "  <tensorflow.python.keras.engine.input_layer.InputLayer at 0x226df8faa08>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226f73dec48>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f29c1c88>: True,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226f6a77a48>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e9cbb808>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x227013a6088>: True,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226ef8d8308>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e408af88>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226ef941c48>: True,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226e4069188>: True,\n",
       "  <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x226e9cc8d48>: True,\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x22700620108>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226e9cee508>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226e9d2cb48>: True,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x22670aa0188>: True,\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x226e9d29688>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22672111208>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226efe07e08>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x226efdfe388>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22700109988>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22700103048>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x227048b0c08>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22703a0b188>: True,\n",
       "  <tensorflow.python.keras.layers.pooling.AveragePooling2D at 0x226efdfe0c8>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226efdd9608>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22700110c48>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x22703bbd708>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x2270488b088>: True,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x22703a17148>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.UpSampling2D at 0x226efdf96c8>: True,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x227000f9248>: True,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x227025a3fc8>: True,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x22703b84188>: True,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x227048b7cc8>: True,\n",
       "  <tensorflow.python.keras.layers.merge.Add at 0x226efdec9c8>: True,\n",
       "  <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x227048a6188>: True,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x227017f6b08>: True,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x227017f6f88>: True,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d0808>: True,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d88c8>: True,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048dec08>: True,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048d02c8>: True,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048e3c88>: True,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x227048e3848>: True,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x22703768448>: True,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x2267085e508>: True,\n",
       "  <tensorflow.python.keras.layers.merge.Add at 0x227017ee208>: True,\n",
       "  <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22703768248>: True,\n",
       "  <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x22703758b48>: True,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x22703747608>: True,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x22703746d88>: True,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x226f661c5c8>: True,\n",
       "  <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x226f660cf08>: True,\n",
       "  <tensorflow.python.keras.layers.merge.Multiply at 0x22703756888>: True,\n",
       "  <__main__.SoftThreshold at 0x226f661ce88>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f6607cc8>: True,\n",
       "  <tensorflow.python.keras.layers.core.Activation at 0x226f65f5588>: True,\n",
       "  <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x22672113888>: True,\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x226f9a03508>: True,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x226f9a1c788>: True,\n",
       "  <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x226f9a09608>: True,\n",
       "  <tensorflow.python.keras.layers.advanced_activations.Softmax at 0x226f99f3408>: True},\n",
       " '_distributed_model_cache': {},\n",
       " '_distributed_function_cache': {},\n",
       " '_compile_metric_functions': [<tensorflow.python.keras.metrics.MeanMetricWrapper at 0x226feaebf08>,\n",
       "  <tensorflow.python.keras.metrics.MeanMetricWrapper at 0x226feaeb208>],\n",
       " 'loss_functions': [<tensorflow.python.keras.losses.LossFunctionWrapper at 0x226feacbc08>],\n",
       " '_per_output_metrics': [OrderedDict([('acc',\n",
       "                <tensorflow.python.keras.metrics.MeanMetricWrapper at 0x226feaebf08>),\n",
       "               ('score',\n",
       "                <tensorflow.python.keras.metrics.MeanMetricWrapper at 0x226feaeb208>)])],\n",
       " '_per_output_weighted_metrics': [OrderedDict()],\n",
       " 'total_loss': <tf.Tensor 'loss/add:0' shape=() dtype=float32>,\n",
       " '_function_kwargs': {},\n",
       " 'train_function': <tensorflow.python.keras.backend.GraphExecutionFunction at 0x22702844c08>,\n",
       " 'test_function': <tensorflow.python.keras.backend.GraphExecutionFunction at 0x22701d55e48>,\n",
       " 'predict_function': None,\n",
       " '_collected_trainable_weights': [<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 32) dtype=float32>,\n",
       "  <tf.Variable 'conv2d/bias:0' shape=(32,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization/gamma:0' shape=(32,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization/beta:0' shape=(32,) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 32, 64) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_1/bias:0' shape=(64,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_1/gamma:0' shape=(64,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_1/beta:0' shape=(64,) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 64, 128) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_2/bias:0' shape=(128,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_2/gamma:0' shape=(128,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_2/beta:0' shape=(128,) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 128, 256) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_3/bias:0' shape=(256,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_3/gamma:0' shape=(256,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_3/beta:0' shape=(256,) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_4/kernel:0' shape=(1, 1, 256, 384) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_4/bias:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_5/kernel:0' shape=(3, 3, 256, 384) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_5/bias:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_6/kernel:0' shape=(3, 3, 256, 384) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_6/bias:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_7/kernel:0' shape=(3, 3, 256, 384) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_7/bias:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_8/kernel:0' shape=(3, 3, 256, 384) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_8/bias:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_9/kernel:0' shape=(3, 3, 256, 384) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_9/bias:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_4/gamma:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_4/beta:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_5/gamma:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_5/beta:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_6/gamma:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_6/beta:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_7/gamma:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_7/beta:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_8/gamma:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_8/beta:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'dense/kernel:0' shape=(384, 24) dtype=float32>,\n",
       "  <tf.Variable 'dense/bias:0' shape=(24,) dtype=float32>,\n",
       "  <tf.Variable 'dense_1/kernel:0' shape=(24, 384) dtype=float32>,\n",
       "  <tf.Variable 'dense_1/bias:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_10/kernel:0' shape=(3, 3, 384, 384) dtype=float32>,\n",
       "  <tf.Variable 'conv2d_10/bias:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'dense_2/kernel:0' shape=(384, 24) dtype=float32>,\n",
       "  <tf.Variable 'dense_2/bias:0' shape=(24,) dtype=float32>,\n",
       "  <tf.Variable 'dense_3/kernel:0' shape=(24, 384) dtype=float32>,\n",
       "  <tf.Variable 'dense_3/bias:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_9/gamma:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_9/beta:0' shape=(384,) dtype=float32>,\n",
       "  <tf.Variable 'dense_4/kernel:0' shape=(384, 19) dtype=float32>,\n",
       "  <tf.Variable 'dense_4/bias:0' shape=(19,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_10/gamma:0' shape=(19,) dtype=float32>,\n",
       "  <tf.Variable 'batch_normalization_10/beta:0' shape=(19,) dtype=float32>],\n",
       " 'history': <tensorflow.python.keras.callbacks.History at 0x227008fb308>,\n",
       " 'stop_training': False}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
