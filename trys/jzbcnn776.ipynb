{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "train = pd.read_csv(data_path+'/sensor_train.csv')\n",
    "test = pd.read_csv(data_path+'/sensor_test.csv')\n",
    "sub = pd.read_csv(data_path + '/提交结果示例.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['acc'] = (train.acc_x ** 2 + train.acc_y ** 2 + train.acc_z ** 2) ** .5\n",
    "train['accg'] = (train.acc_xg ** 2 + train.acc_yg ** 2 + train.acc_zg ** 2) ** .5\n",
    "test['acc'] = (test.acc_x ** 2 + test.acc_y ** 2 + test.acc_z ** 2) ** .5\n",
    "test['accg'] = (test.acc_xg ** 2 + test.acc_yg ** 2 + test.acc_zg ** 2) ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 39.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# accs = ['acc_x', 'acc_y', 'acc_z']\n",
    "# accgs = ['acc_xg', 'acc_yg', 'acc_zg']\n",
    "# two_accs = [['acc_x', 'acc_y'], ['acc_y', 'acc_z'], ['acc_x', 'acc_z']] # ['acc_y', 'acc_x'], ['acc_z', 'acc_y']\n",
    "# two_accgs =  [['acc_xg', 'acc_yg'], ['acc_yg', 'acc_zg'], ['acc_xg', 'acc_zg']] # , ['acc_yg', 'acc_xg'], ['acc_zg', 'acc_yg']\n",
    "# acc_and_accg = accs + accgs\n",
    "\n",
    "for data in tqdm([train , test]):\n",
    "    data['acc1'] = (data['acc_x'] ** 2 + data['acc_y'] ** 2) ** 0.5\n",
    "    data['accg1'] = (data['acc_xg'] ** 2 + data['acc_yg'] ** 2) ** 0.5\n",
    "    \n",
    "    data['acc2'] = (data['acc_x'] ** 2 + data['acc_z'] ** 2) ** 0.5\n",
    "    data['accg2'] = (data['acc_xg'] ** 2 + data['acc_zg'] ** 2) ** 0.5\n",
    "    \n",
    "#     data['acc3'] = (data['acc_y'] ** 2 + data['acc_z'] ** 2) ** 0.5\n",
    "#     data['accg3'] = (data['acc_yg'] ** 2 + data['acc_zg'] ** 2) ** 0.5  # y - z系列 under 4%%\n",
    "\n",
    "\n",
    "for data in [train, test]:\n",
    "    data['acc_sub'] = ((data['acc_xg'] - data['acc_x']) ** 2 + (data['acc_yg'] - data['acc_y']) ** 2 + (data['acc_zg'] - data['acc_z'])**2) ** 0.5\n",
    "    data['acc_sub1'] = ((data['acc_xg'] - data['acc_x']) ** 2 + (data['acc_yg'] - data['acc_y']) ** 2) ** 0.5\n",
    "    data['acc_sub2'] = ((data['acc_xg'] - data['acc_x']) ** 2 + (data['acc_zg'] - data['acc_z'])**2) ** 0.5\n",
    "#     data['acc_sub3'] = ((data['acc_yg'] - data['acc_y']) ** 2 + (data['acc_zg'] - data['acc_z'])**2) ** 0.5\n",
    "\n",
    "\n",
    "for data in [train, test]:\n",
    "    data['accxg_diff_accx'] = data['acc_xg'] - data['acc_x']\n",
    "    data['accyg_diff_accy'] = data['acc_yg'] - data['acc_y']\n",
    "    data['acczg_diff_accz'] = data['acc_zg'] - data['acc_z']\n",
    "    \n",
    "# for data in [train, test]:\n",
    "#     data['accx_map_accy'] = data['acc_x'] * np.cos(data['acc_y'].apply(lambda x: np.abs(x)))\n",
    "#     data['accx_map_accz'] = data['acc_x'] * np.cos(data['acc_z'].apply(lambda x: np.abs(x)))\n",
    "    \n",
    "#     data['accxg_map_accyg'] = data['acc_xg'] * np.cos(data['acc_yg'].apply(lambda x: np.abs(x)))\n",
    "#     data['accxg_map_acczg'] = data['acc_xg'] * np.cos(data['acc_zg'].apply(lambda x: np.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 时间倒序\n",
    "# train1 = pd.DataFrame()\n",
    "# for id in tqdm(train['fragment_id'].unique()):\n",
    "#     tmp = train[train['fragment_id'] == id]\n",
    "#     tmp.sort_values(by = 'time_point', ascending = False, inplace = True)\n",
    "#     train1 = pd.concat([train1, tmp], axis = 0)\n",
    "\n",
    "# test1 = pd.DataFrame()\n",
    "# for id in tqdm(test['fragment_id'].unique()):\n",
    "#     tmp = test[test['fragment_id'] == id]\n",
    "#     tmp.sort_values(by = 'time_point', ascending = False, inplace = True)\n",
    "#     test1 = pd.concat([test1, tmp], axis = 0)\n",
    "\n",
    "# train = train1\n",
    "# test = test1\n",
    "\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425359, 25) (430182, 24) Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'accg', 'acc1', 'accg1',\n",
      "       'acc2', 'accg2', 'acc_sub', 'acc_sub1', 'acc_sub2', 'accxg_diff_accx',\n",
      "       'accyg_diff_accy', 'acczg_diff_accz', 'accx_map_accy', 'accx_map_accz',\n",
      "       'accxg_map_accyg', 'accxg_map_acczg'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape, train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7292/7292 [00:16<00:00, 431.25it/s]\n",
      "100%|██████████| 7500/7500 [00:17<00:00, 428.33it/s]\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((7292, 60, train.shape[1] - 3, 1))\n",
    "t = np.zeros((7500, 60, train.shape[1] - 3, 1))\n",
    "for i in tqdm(range(7292)):\n",
    "    tmp = train[train.fragment_id == i][:60]\n",
    "    x[i, :, :, 0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n",
    "                                      axis=1), 60, np.array(tmp.time_point))[0]\n",
    "for i in tqdm(range(7500)):\n",
    "    tmp = test[test.fragment_id == i][:60]\n",
    "    t[i, :, :, 0] = resample(tmp.drop(['fragment_id', 'time_point'],\n",
    "                                      axis=1), 60, np.array(tmp.time_point))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernal_size = 3\n",
    "kfold = StratifiedKFold(5, shuffle=True)\n",
    "def Net():\n",
    "    input = Input(shape=(60, train.shape[1] - 3, 1))\n",
    "    X = Conv2D(filters=64,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               padding='same')(input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Conv2D(filters=128,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X = MaxPooling2D()(X)\n",
    "    X = AveragePooling2D()(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Conv2D(filters=256,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Conv2D(filters=512,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = BatchNormalization()(Dropout(0.2)(Dense(128, activation='relu')(Flatten()(X))))\n",
    "    X = Dense(19, activation='softmax')(X)\n",
    "    return Model([input], X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================train 1th fold============================================================\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 60, 22, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 60, 22, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 60, 22, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 60, 22, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 60, 22, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 30, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 15, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 5, 256)        295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 5, 256)        1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 5, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 5, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 15, 5, 512)        2048      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 1,622,291\n",
      "Trainable params: 1,620,115\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "Train on 5833 samples, validate on 1459 samples\n",
      "Epoch 1/500\n",
      "5833/5833 - 7s - loss: 2.1696 - acc: 0.3127 - val_loss: 2.9821 - val_acc: 0.0768\n",
      "Epoch 2/500\n",
      "5833/5833 - 2s - loss: 1.8117 - acc: 0.3880 - val_loss: 2.6309 - val_acc: 0.2036\n",
      "Epoch 3/500\n",
      "5833/5833 - 2s - loss: 1.6566 - acc: 0.4277 - val_loss: 3.4085 - val_acc: 0.1645\n",
      "Epoch 4/500\n",
      "5833/5833 - 2s - loss: 1.5766 - acc: 0.4492 - val_loss: 2.4522 - val_acc: 0.2659\n",
      "Epoch 5/500\n",
      "5833/5833 - 2s - loss: 1.4868 - acc: 0.4747 - val_loss: 3.5136 - val_acc: 0.2111\n",
      "Epoch 6/500\n",
      "5833/5833 - 2s - loss: 1.4272 - acc: 0.5020 - val_loss: 5.6754 - val_acc: 0.1960\n",
      "Epoch 7/500\n",
      "5833/5833 - 2s - loss: 1.3728 - acc: 0.5093 - val_loss: 1.9070 - val_acc: 0.4270\n",
      "Epoch 8/500\n",
      "5833/5833 - 2s - loss: 1.3555 - acc: 0.5157 - val_loss: 2.0210 - val_acc: 0.4051\n",
      "Epoch 9/500\n",
      "5833/5833 - 2s - loss: 1.2893 - acc: 0.5405 - val_loss: 2.2200 - val_acc: 0.3249\n",
      "Epoch 10/500\n",
      "5833/5833 - 2s - loss: 1.2733 - acc: 0.5489 - val_loss: 1.7866 - val_acc: 0.4441\n",
      "Epoch 11/500\n",
      "5833/5833 - 2s - loss: 1.2308 - acc: 0.5618 - val_loss: 1.5412 - val_acc: 0.4531\n",
      "Epoch 12/500\n",
      "5833/5833 - 2s - loss: 1.2004 - acc: 0.5755 - val_loss: 1.3982 - val_acc: 0.5024\n",
      "Epoch 13/500\n",
      "5833/5833 - 2s - loss: 1.1842 - acc: 0.5700 - val_loss: 1.3646 - val_acc: 0.5141\n",
      "Epoch 14/500\n",
      "5833/5833 - 2s - loss: 1.1706 - acc: 0.5875 - val_loss: 1.6477 - val_acc: 0.4969\n",
      "Epoch 15/500\n",
      "5833/5833 - 2s - loss: 1.1348 - acc: 0.5947 - val_loss: 1.5757 - val_acc: 0.4736\n",
      "Epoch 16/500\n",
      "5833/5833 - 2s - loss: 1.0989 - acc: 0.6040 - val_loss: 1.2630 - val_acc: 0.5497\n",
      "Epoch 17/500\n",
      "5833/5833 - 2s - loss: 1.0825 - acc: 0.6163 - val_loss: 1.4354 - val_acc: 0.5003\n",
      "Epoch 18/500\n",
      "5833/5833 - 2s - loss: 1.0574 - acc: 0.6146 - val_loss: 1.6090 - val_acc: 0.4990\n",
      "Epoch 19/500\n",
      "5833/5833 - 2s - loss: 1.0343 - acc: 0.6307 - val_loss: 1.6726 - val_acc: 0.4380\n",
      "Epoch 20/500\n",
      "5833/5833 - 2s - loss: 1.0210 - acc: 0.6390 - val_loss: 1.6731 - val_acc: 0.4524\n",
      "Epoch 21/500\n",
      "5833/5833 - 2s - loss: 0.9952 - acc: 0.6395 - val_loss: 1.1547 - val_acc: 0.5936\n",
      "Epoch 22/500\n",
      "5833/5833 - 2s - loss: 0.9702 - acc: 0.6467 - val_loss: 1.7400 - val_acc: 0.4757\n",
      "Epoch 23/500\n",
      "5833/5833 - 2s - loss: 0.9498 - acc: 0.6602 - val_loss: 1.6941 - val_acc: 0.4572\n",
      "Epoch 24/500\n",
      "5833/5833 - 2s - loss: 0.9285 - acc: 0.6669 - val_loss: 1.5359 - val_acc: 0.4983\n",
      "Epoch 25/500\n",
      "5833/5833 - 2s - loss: 0.9159 - acc: 0.6642 - val_loss: 1.5572 - val_acc: 0.5257\n",
      "Epoch 26/500\n",
      "5833/5833 - 2s - loss: 0.9058 - acc: 0.6672 - val_loss: 1.4698 - val_acc: 0.5141\n",
      "Epoch 27/500\n",
      "5833/5833 - 2s - loss: 0.8849 - acc: 0.6791 - val_loss: 1.2501 - val_acc: 0.5826\n",
      "Epoch 28/500\n",
      "5833/5833 - 2s - loss: 0.8706 - acc: 0.6837 - val_loss: 1.2504 - val_acc: 0.6141\n",
      "Epoch 29/500\n",
      "5833/5833 - 2s - loss: 0.8477 - acc: 0.6897 - val_loss: 1.2653 - val_acc: 0.5867\n",
      "Epoch 30/500\n",
      "5833/5833 - 2s - loss: 0.8326 - acc: 0.7003 - val_loss: 2.1107 - val_acc: 0.4942\n",
      "Epoch 31/500\n",
      "5833/5833 - 2s - loss: 0.8163 - acc: 0.7086 - val_loss: 1.8836 - val_acc: 0.4839\n",
      "Epoch 32/500\n",
      "5833/5833 - 2s - loss: 0.8054 - acc: 0.7096 - val_loss: 1.1221 - val_acc: 0.6217\n",
      "Epoch 33/500\n",
      "5833/5833 - 2s - loss: 0.7958 - acc: 0.7092 - val_loss: 1.3435 - val_acc: 0.5709\n",
      "Epoch 34/500\n",
      "5833/5833 - 2s - loss: 0.7768 - acc: 0.7154 - val_loss: 1.5180 - val_acc: 0.5435\n",
      "Epoch 35/500\n",
      "5833/5833 - 2s - loss: 0.7669 - acc: 0.7276 - val_loss: 1.1328 - val_acc: 0.6210\n",
      "Epoch 36/500\n",
      "5833/5833 - 2s - loss: 0.7676 - acc: 0.7226 - val_loss: 1.4325 - val_acc: 0.5696\n",
      "Epoch 37/500\n",
      "5833/5833 - 2s - loss: 0.7366 - acc: 0.7346 - val_loss: 1.1055 - val_acc: 0.6265\n",
      "Epoch 38/500\n",
      "5833/5833 - 2s - loss: 0.7202 - acc: 0.7375 - val_loss: 2.0435 - val_acc: 0.4805\n",
      "Epoch 39/500\n",
      "5833/5833 - 2s - loss: 0.7022 - acc: 0.7475 - val_loss: 1.0342 - val_acc: 0.6395\n",
      "Epoch 40/500\n",
      "5833/5833 - 2s - loss: 0.7050 - acc: 0.7483 - val_loss: 1.3506 - val_acc: 0.5888\n",
      "Epoch 41/500\n",
      "5833/5833 - 2s - loss: 0.6962 - acc: 0.7485 - val_loss: 1.2001 - val_acc: 0.6347\n",
      "Epoch 42/500\n",
      "5833/5833 - 2s - loss: 0.6654 - acc: 0.7579 - val_loss: 1.1321 - val_acc: 0.6402\n",
      "Epoch 43/500\n",
      "5833/5833 - 2s - loss: 0.6518 - acc: 0.7631 - val_loss: 1.1888 - val_acc: 0.6230\n",
      "Epoch 44/500\n",
      "5833/5833 - 2s - loss: 0.6636 - acc: 0.7614 - val_loss: 1.7141 - val_acc: 0.5449\n",
      "Epoch 45/500\n",
      "5833/5833 - 2s - loss: 0.6441 - acc: 0.7638 - val_loss: 1.2395 - val_acc: 0.6059\n",
      "Epoch 46/500\n",
      "5833/5833 - 2s - loss: 0.6297 - acc: 0.7734 - val_loss: 1.3323 - val_acc: 0.5997\n",
      "Epoch 47/500\n",
      "5833/5833 - 2s - loss: 0.6118 - acc: 0.7816 - val_loss: 1.4603 - val_acc: 0.5689\n",
      "Epoch 48/500\n",
      "5833/5833 - 2s - loss: 0.6053 - acc: 0.7818 - val_loss: 1.2378 - val_acc: 0.6292\n",
      "Epoch 49/500\n",
      "5833/5833 - 2s - loss: 0.6080 - acc: 0.7828 - val_loss: 1.5705 - val_acc: 0.5867\n",
      "Epoch 50/500\n",
      "5833/5833 - 2s - loss: 0.6048 - acc: 0.7814 - val_loss: 1.2456 - val_acc: 0.6237\n",
      "Epoch 51/500\n",
      "5833/5833 - 2s - loss: 0.5826 - acc: 0.7895 - val_loss: 1.7697 - val_acc: 0.5250\n",
      "Epoch 52/500\n",
      "5833/5833 - 2s - loss: 0.5780 - acc: 0.7934 - val_loss: 1.4210 - val_acc: 0.5997\n",
      "Epoch 53/500\n",
      "5833/5833 - 2s - loss: 0.5640 - acc: 0.7963 - val_loss: 1.1177 - val_acc: 0.6861\n",
      "Epoch 54/500\n",
      "5833/5833 - 2s - loss: 0.5598 - acc: 0.7967 - val_loss: 1.0731 - val_acc: 0.6498\n",
      "Epoch 55/500\n",
      "5833/5833 - 2s - loss: 0.5524 - acc: 0.8047 - val_loss: 1.3247 - val_acc: 0.6045\n",
      "Epoch 56/500\n",
      "5833/5833 - 2s - loss: 0.5411 - acc: 0.8037 - val_loss: 1.1968 - val_acc: 0.6450\n",
      "Epoch 57/500\n",
      "5833/5833 - 2s - loss: 0.5303 - acc: 0.7998 - val_loss: 1.2839 - val_acc: 0.6182\n",
      "Epoch 58/500\n",
      "5833/5833 - 2s - loss: 0.5174 - acc: 0.8135 - val_loss: 1.4729 - val_acc: 0.5846\n",
      "Epoch 59/500\n",
      "5833/5833 - 2s - loss: 0.5206 - acc: 0.8152 - val_loss: 1.1615 - val_acc: 0.6587\n",
      "Epoch 60/500\n",
      "5833/5833 - 2s - loss: 0.5057 - acc: 0.8193 - val_loss: 1.4912 - val_acc: 0.5888\n",
      "Epoch 61/500\n",
      "5833/5833 - 2s - loss: 0.4914 - acc: 0.8226 - val_loss: 1.3811 - val_acc: 0.6292\n",
      "Epoch 62/500\n",
      "5833/5833 - 2s - loss: 0.4711 - acc: 0.8298 - val_loss: 0.9747 - val_acc: 0.7005\n",
      "Epoch 63/500\n",
      "5833/5833 - 2s - loss: 0.4679 - acc: 0.8334 - val_loss: 1.3531 - val_acc: 0.6059\n",
      "Epoch 64/500\n",
      "5833/5833 - 2s - loss: 0.4805 - acc: 0.8234 - val_loss: 1.3957 - val_acc: 0.6463\n",
      "Epoch 65/500\n",
      "5833/5833 - 2s - loss: 0.4787 - acc: 0.8260 - val_loss: 1.4051 - val_acc: 0.5867\n",
      "Epoch 66/500\n",
      "5833/5833 - 2s - loss: 0.4526 - acc: 0.8370 - val_loss: 1.0999 - val_acc: 0.6717\n",
      "Epoch 67/500\n",
      "5833/5833 - 2s - loss: 0.4561 - acc: 0.8335 - val_loss: 1.0537 - val_acc: 0.6895\n",
      "Epoch 68/500\n",
      "5833/5833 - 2s - loss: 0.4579 - acc: 0.8327 - val_loss: 1.3453 - val_acc: 0.6539\n",
      "Epoch 69/500\n",
      "5833/5833 - 2s - loss: 0.4474 - acc: 0.8316 - val_loss: 1.3931 - val_acc: 0.6196\n",
      "Epoch 70/500\n",
      "5833/5833 - 2s - loss: 0.4369 - acc: 0.8414 - val_loss: 1.3669 - val_acc: 0.6436\n",
      "Epoch 71/500\n",
      "5833/5833 - 2s - loss: 0.4398 - acc: 0.8433 - val_loss: 1.1275 - val_acc: 0.6779\n",
      "Epoch 72/500\n",
      "5833/5833 - 2s - loss: 0.4268 - acc: 0.8412 - val_loss: 1.5037 - val_acc: 0.5942\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "5833/5833 - 2s - loss: 0.4241 - acc: 0.8445 - val_loss: 1.2686 - val_acc: 0.6600\n",
      "Epoch 74/500\n",
      "5833/5833 - 2s - loss: 0.3516 - acc: 0.8774 - val_loss: 1.2178 - val_acc: 0.6792\n",
      "Epoch 75/500\n",
      "5833/5833 - 2s - loss: 0.3281 - acc: 0.8800 - val_loss: 0.9145 - val_acc: 0.7361\n",
      "Epoch 76/500\n",
      "5833/5833 - 2s - loss: 0.3411 - acc: 0.8750 - val_loss: 1.1713 - val_acc: 0.6642\n",
      "Epoch 77/500\n",
      "5833/5833 - 2s - loss: 0.3239 - acc: 0.8812 - val_loss: 0.9529 - val_acc: 0.7457\n",
      "Epoch 78/500\n",
      "5833/5833 - 2s - loss: 0.3125 - acc: 0.8893 - val_loss: 1.4716 - val_acc: 0.6607\n",
      "Epoch 79/500\n",
      "5833/5833 - 2s - loss: 0.3119 - acc: 0.8855 - val_loss: 1.1063 - val_acc: 0.6881\n",
      "Epoch 80/500\n",
      "5833/5833 - 2s - loss: 0.3071 - acc: 0.8865 - val_loss: 0.9484 - val_acc: 0.7416\n",
      "Epoch 81/500\n",
      "5833/5833 - 2s - loss: 0.3019 - acc: 0.8922 - val_loss: 1.0078 - val_acc: 0.7190\n",
      "Epoch 82/500\n",
      "5833/5833 - 2s - loss: 0.3004 - acc: 0.8905 - val_loss: 0.9116 - val_acc: 0.7560\n",
      "Epoch 83/500\n",
      "5833/5833 - 2s - loss: 0.2703 - acc: 0.9035 - val_loss: 1.0213 - val_acc: 0.7258\n",
      "Epoch 84/500\n",
      "5833/5833 - 2s - loss: 0.2713 - acc: 0.9037 - val_loss: 0.9877 - val_acc: 0.7265\n",
      "Epoch 85/500\n",
      "5833/5833 - 2s - loss: 0.2876 - acc: 0.8941 - val_loss: 1.1493 - val_acc: 0.7101\n",
      "Epoch 86/500\n",
      "5833/5833 - 2s - loss: 0.2744 - acc: 0.9033 - val_loss: 0.9500 - val_acc: 0.7546\n",
      "Epoch 87/500\n",
      "5833/5833 - 2s - loss: 0.2650 - acc: 0.9057 - val_loss: 1.1172 - val_acc: 0.6998\n",
      "Epoch 88/500\n",
      "5833/5833 - 2s - loss: 0.2649 - acc: 0.9002 - val_loss: 1.0615 - val_acc: 0.7197\n",
      "Epoch 89/500\n",
      "5833/5833 - 2s - loss: 0.2625 - acc: 0.9042 - val_loss: 0.9468 - val_acc: 0.7533\n",
      "Epoch 90/500\n",
      "5833/5833 - 2s - loss: 0.2627 - acc: 0.9016 - val_loss: 1.2095 - val_acc: 0.7025\n",
      "Epoch 91/500\n",
      "5833/5833 - 2s - loss: 0.2568 - acc: 0.9112 - val_loss: 1.0504 - val_acc: 0.7395\n",
      "Epoch 92/500\n",
      "5833/5833 - 2s - loss: 0.2535 - acc: 0.9145 - val_loss: 1.1634 - val_acc: 0.7135\n",
      "Epoch 93/500\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "5833/5833 - 2s - loss: 0.2611 - acc: 0.9066 - val_loss: 1.0402 - val_acc: 0.7286\n",
      "Epoch 94/500\n",
      "5833/5833 - 2s - loss: 0.2254 - acc: 0.9196 - val_loss: 1.0523 - val_acc: 0.7245\n",
      "Epoch 95/500\n",
      "5833/5833 - 2s - loss: 0.2094 - acc: 0.9290 - val_loss: 0.9057 - val_acc: 0.7752\n",
      "Epoch 96/500\n",
      "5833/5833 - 2s - loss: 0.2162 - acc: 0.9263 - val_loss: 1.0760 - val_acc: 0.7313\n",
      "Epoch 97/500\n",
      "5833/5833 - 2s - loss: 0.1963 - acc: 0.9326 - val_loss: 1.0342 - val_acc: 0.7382\n",
      "Epoch 98/500\n",
      "5833/5833 - 2s - loss: 0.1994 - acc: 0.9311 - val_loss: 0.9431 - val_acc: 0.7622\n",
      "Epoch 99/500\n",
      "5833/5833 - 2s - loss: 0.1935 - acc: 0.9342 - val_loss: 1.0903 - val_acc: 0.7382\n",
      "Epoch 100/500\n",
      "5833/5833 - 2s - loss: 0.1914 - acc: 0.9361 - val_loss: 0.9118 - val_acc: 0.7642\n",
      "Epoch 101/500\n",
      "5833/5833 - 2s - loss: 0.1814 - acc: 0.9369 - val_loss: 1.0096 - val_acc: 0.7491\n",
      "Epoch 102/500\n",
      "5833/5833 - 2s - loss: 0.1972 - acc: 0.9287 - val_loss: 1.0757 - val_acc: 0.7402\n",
      "Epoch 103/500\n",
      "5833/5833 - 2s - loss: 0.1885 - acc: 0.9350 - val_loss: 1.0503 - val_acc: 0.7437\n",
      "Epoch 104/500\n",
      "5833/5833 - 2s - loss: 0.1793 - acc: 0.9385 - val_loss: 1.0095 - val_acc: 0.7539\n",
      "Epoch 105/500\n",
      "5833/5833 - 2s - loss: 0.1829 - acc: 0.9331 - val_loss: 1.1943 - val_acc: 0.7176\n",
      "Epoch 106/500\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "5833/5833 - 2s - loss: 0.1791 - acc: 0.9371 - val_loss: 1.0628 - val_acc: 0.7498\n",
      "Epoch 107/500\n",
      "5833/5833 - 2s - loss: 0.1508 - acc: 0.9496 - val_loss: 0.9161 - val_acc: 0.7800\n",
      "Epoch 108/500\n",
      "5833/5833 - 2s - loss: 0.1631 - acc: 0.9427 - val_loss: 0.9261 - val_acc: 0.7629\n",
      "Epoch 109/500\n",
      "5833/5833 - 2s - loss: 0.1509 - acc: 0.9455 - val_loss: 1.0964 - val_acc: 0.7368\n",
      "Epoch 110/500\n",
      "5833/5833 - 2s - loss: 0.1552 - acc: 0.9460 - val_loss: 0.9520 - val_acc: 0.7683\n",
      "Epoch 111/500\n",
      "5833/5833 - 2s - loss: 0.1579 - acc: 0.9450 - val_loss: 1.0180 - val_acc: 0.7642\n",
      "Epoch 112/500\n",
      "5833/5833 - 2s - loss: 0.1503 - acc: 0.9475 - val_loss: 0.9478 - val_acc: 0.7820\n",
      "Epoch 113/500\n",
      "5833/5833 - 2s - loss: 0.1466 - acc: 0.9505 - val_loss: 1.0323 - val_acc: 0.7553\n",
      "Epoch 114/500\n",
      "5833/5833 - 2s - loss: 0.1525 - acc: 0.9487 - val_loss: 0.9656 - val_acc: 0.7608\n",
      "Epoch 115/500\n",
      "5833/5833 - 2s - loss: 0.1359 - acc: 0.9534 - val_loss: 0.9648 - val_acc: 0.7676\n",
      "Epoch 116/500\n",
      "5833/5833 - 2s - loss: 0.1468 - acc: 0.9499 - val_loss: 1.0055 - val_acc: 0.7574\n",
      "Epoch 117/500\n",
      "5833/5833 - 2s - loss: 0.1357 - acc: 0.9553 - val_loss: 0.9387 - val_acc: 0.7697\n",
      "Epoch 118/500\n",
      "5833/5833 - 2s - loss: 0.1397 - acc: 0.9506 - val_loss: 0.9114 - val_acc: 0.7875\n",
      "Epoch 119/500\n",
      "5833/5833 - 2s - loss: 0.1379 - acc: 0.9506 - val_loss: 0.9744 - val_acc: 0.7793\n",
      "Epoch 120/500\n",
      "5833/5833 - 2s - loss: 0.1412 - acc: 0.9496 - val_loss: 1.0026 - val_acc: 0.7718\n",
      "Epoch 121/500\n",
      "5833/5833 - 2s - loss: 0.1334 - acc: 0.9532 - val_loss: 0.9907 - val_acc: 0.7683\n",
      "Epoch 122/500\n",
      "5833/5833 - 2s - loss: 0.1341 - acc: 0.9511 - val_loss: 1.0532 - val_acc: 0.7443\n",
      "Epoch 123/500\n",
      "5833/5833 - 2s - loss: 0.1265 - acc: 0.9547 - val_loss: 1.0097 - val_acc: 0.7711\n",
      "Epoch 124/500\n",
      "5833/5833 - 2s - loss: 0.1241 - acc: 0.9597 - val_loss: 0.9971 - val_acc: 0.7670\n",
      "Epoch 125/500\n",
      "5833/5833 - 2s - loss: 0.1294 - acc: 0.9530 - val_loss: 0.9481 - val_acc: 0.7896\n",
      "Epoch 126/500\n",
      "5833/5833 - 2s - loss: 0.1178 - acc: 0.9611 - val_loss: 1.0783 - val_acc: 0.7437\n",
      "Epoch 127/500\n",
      "5833/5833 - 2s - loss: 0.1266 - acc: 0.9573 - val_loss: 0.9950 - val_acc: 0.7752\n",
      "Epoch 128/500\n",
      "5833/5833 - 2s - loss: 0.1287 - acc: 0.9546 - val_loss: 1.0421 - val_acc: 0.7622\n",
      "Epoch 129/500\n",
      "5833/5833 - 2s - loss: 0.1303 - acc: 0.9570 - val_loss: 1.0526 - val_acc: 0.7601\n",
      "Epoch 130/500\n",
      "5833/5833 - 2s - loss: 0.1250 - acc: 0.9559 - val_loss: 0.9992 - val_acc: 0.7738\n",
      "Epoch 131/500\n",
      "5833/5833 - 2s - loss: 0.1234 - acc: 0.9570 - val_loss: 0.9644 - val_acc: 0.7868\n",
      "Epoch 132/500\n",
      "5833/5833 - 2s - loss: 0.1276 - acc: 0.9554 - val_loss: 1.0074 - val_acc: 0.7704\n",
      "Epoch 133/500\n",
      "5833/5833 - 2s - loss: 0.1202 - acc: 0.9590 - val_loss: 1.0635 - val_acc: 0.7642\n",
      "Epoch 134/500\n",
      "5833/5833 - 2s - loss: 0.1168 - acc: 0.9599 - val_loss: 0.9651 - val_acc: 0.7807\n",
      "Epoch 135/500\n",
      "5833/5833 - 2s - loss: 0.1242 - acc: 0.9559 - val_loss: 0.9695 - val_acc: 0.7882\n",
      "Epoch 136/500\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "5833/5833 - 2s - loss: 0.1154 - acc: 0.9607 - val_loss: 1.0197 - val_acc: 0.7690\n",
      "Epoch 137/500\n",
      "5833/5833 - 2s - loss: 0.1064 - acc: 0.9643 - val_loss: 1.0418 - val_acc: 0.7642\n",
      "Epoch 138/500\n",
      "5833/5833 - 2s - loss: 0.1020 - acc: 0.9659 - val_loss: 0.9474 - val_acc: 0.7916\n",
      "Epoch 139/500\n",
      "5833/5833 - 2s - loss: 0.1025 - acc: 0.9676 - val_loss: 1.0209 - val_acc: 0.7656\n",
      "Epoch 140/500\n",
      "5833/5833 - 2s - loss: 0.1030 - acc: 0.9669 - val_loss: 0.9673 - val_acc: 0.7910\n",
      "Epoch 141/500\n",
      "5833/5833 - 2s - loss: 0.1062 - acc: 0.9633 - val_loss: 0.9412 - val_acc: 0.7827\n",
      "Epoch 142/500\n",
      "5833/5833 - 2s - loss: 0.1007 - acc: 0.9652 - val_loss: 0.9644 - val_acc: 0.7964\n",
      "Epoch 143/500\n",
      "5833/5833 - 2s - loss: 0.1008 - acc: 0.9652 - val_loss: 0.9962 - val_acc: 0.7814\n",
      "Epoch 144/500\n",
      "5833/5833 - 2s - loss: 0.0997 - acc: 0.9659 - val_loss: 0.9628 - val_acc: 0.7951\n",
      "Epoch 145/500\n",
      "5833/5833 - 2s - loss: 0.1017 - acc: 0.9635 - val_loss: 0.9863 - val_acc: 0.7841\n",
      "Epoch 146/500\n",
      "5833/5833 - 2s - loss: 0.0977 - acc: 0.9640 - val_loss: 1.0357 - val_acc: 0.7772\n",
      "Epoch 147/500\n",
      "5833/5833 - 2s - loss: 0.0946 - acc: 0.9679 - val_loss: 0.9524 - val_acc: 0.7896\n",
      "Epoch 148/500\n",
      "5833/5833 - 2s - loss: 0.0984 - acc: 0.9673 - val_loss: 0.9432 - val_acc: 0.7923\n",
      "Epoch 149/500\n",
      "5833/5833 - 2s - loss: 0.0945 - acc: 0.9697 - val_loss: 0.9749 - val_acc: 0.7820\n",
      "Epoch 150/500\n",
      "5833/5833 - 2s - loss: 0.0933 - acc: 0.9686 - val_loss: 0.9914 - val_acc: 0.7896\n",
      "Epoch 151/500\n",
      "5833/5833 - 2s - loss: 0.0963 - acc: 0.9685 - val_loss: 0.9715 - val_acc: 0.7882\n",
      "Epoch 152/500\n",
      "5833/5833 - 2s - loss: 0.0915 - acc: 0.9719 - val_loss: 0.9825 - val_acc: 0.7916\n",
      "Epoch 153/500\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "5833/5833 - 2s - loss: 0.0932 - acc: 0.9683 - val_loss: 1.0352 - val_acc: 0.7820\n",
      "Epoch 154/500\n",
      "5833/5833 - 2s - loss: 0.0964 - acc: 0.9678 - val_loss: 0.9663 - val_acc: 0.7958\n",
      "Epoch 155/500\n",
      "5833/5833 - 2s - loss: 0.0829 - acc: 0.9707 - val_loss: 0.9689 - val_acc: 0.7930\n",
      "Epoch 156/500\n",
      "5833/5833 - 2s - loss: 0.0853 - acc: 0.9707 - val_loss: 0.9597 - val_acc: 0.7868\n",
      "Epoch 157/500\n",
      "5833/5833 - 2s - loss: 0.0931 - acc: 0.9709 - val_loss: 1.0033 - val_acc: 0.7834\n",
      "Epoch 158/500\n",
      "5833/5833 - 2s - loss: 0.0840 - acc: 0.9733 - val_loss: 0.9496 - val_acc: 0.7930\n",
      "Epoch 159/500\n",
      "5833/5833 - 2s - loss: 0.0844 - acc: 0.9731 - val_loss: 0.9449 - val_acc: 0.7916\n",
      "Epoch 160/500\n",
      "5833/5833 - 2s - loss: 0.0919 - acc: 0.9686 - val_loss: 0.9640 - val_acc: 0.7889\n",
      "Epoch 00160: early stopping\n",
      "============================================================train 2th fold============================================================\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 60, 22, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 60, 22, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 60, 22, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 60, 22, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 60, 22, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 15, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 15, 5, 256)        295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 15, 5, 256)        1024      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 15, 5, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 5, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 15, 5, 512)        2048      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 1,622,291\n",
      "Trainable params: 1,620,115\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "Train on 5833 samples, validate on 1459 samples\n",
      "Epoch 1/500\n",
      "5833/5833 - 4s - loss: 2.2048 - acc: 0.3082 - val_loss: 3.2720 - val_acc: 0.0624\n",
      "Epoch 2/500\n",
      "5833/5833 - 2s - loss: 1.8344 - acc: 0.3833 - val_loss: 2.4626 - val_acc: 0.1748\n",
      "Epoch 3/500\n",
      "5833/5833 - 2s - loss: 1.6885 - acc: 0.4175 - val_loss: 2.3265 - val_acc: 0.2632\n",
      "Epoch 4/500\n",
      "5833/5833 - 2s - loss: 1.5882 - acc: 0.4495 - val_loss: 3.2494 - val_acc: 0.1768\n",
      "Epoch 5/500\n",
      "5833/5833 - 2s - loss: 1.5192 - acc: 0.4646 - val_loss: 2.3764 - val_acc: 0.2858\n",
      "Epoch 6/500\n",
      "5833/5833 - 2s - loss: 1.4593 - acc: 0.4823 - val_loss: 2.0251 - val_acc: 0.3749\n",
      "Epoch 7/500\n",
      "5833/5833 - 2s - loss: 1.4065 - acc: 0.4985 - val_loss: 2.2450 - val_acc: 0.3920\n",
      "Epoch 8/500\n",
      "5833/5833 - 2s - loss: 1.3454 - acc: 0.5241 - val_loss: 1.6804 - val_acc: 0.4606\n",
      "Epoch 9/500\n",
      "5833/5833 - 2s - loss: 1.3202 - acc: 0.5277 - val_loss: 1.9892 - val_acc: 0.4154\n",
      "Epoch 10/500\n",
      "5833/5833 - 2s - loss: 1.2780 - acc: 0.5345 - val_loss: 1.5223 - val_acc: 0.4798\n",
      "Epoch 11/500\n",
      "5833/5833 - 2s - loss: 1.2468 - acc: 0.5503 - val_loss: 2.2727 - val_acc: 0.3729\n",
      "Epoch 12/500\n",
      "5833/5833 - 2s - loss: 1.1987 - acc: 0.5735 - val_loss: 1.3278 - val_acc: 0.5147\n",
      "Epoch 13/500\n",
      "5833/5833 - 2s - loss: 1.1933 - acc: 0.5690 - val_loss: 1.3666 - val_acc: 0.5154\n",
      "Epoch 14/500\n",
      "5833/5833 - 2s - loss: 1.1686 - acc: 0.5820 - val_loss: 1.4047 - val_acc: 0.5367\n",
      "Epoch 15/500\n",
      "5833/5833 - 2s - loss: 1.1467 - acc: 0.5839 - val_loss: 1.5762 - val_acc: 0.4770\n",
      "Epoch 16/500\n",
      "5833/5833 - 2s - loss: 1.1014 - acc: 0.6021 - val_loss: 1.2055 - val_acc: 0.5778\n",
      "Epoch 17/500\n",
      "5833/5833 - 2s - loss: 1.0723 - acc: 0.6132 - val_loss: 1.8969 - val_acc: 0.4380\n",
      "Epoch 18/500\n",
      "5833/5833 - 2s - loss: 1.0493 - acc: 0.6252 - val_loss: 1.3223 - val_acc: 0.5497\n",
      "Epoch 19/500\n",
      "5833/5833 - 2s - loss: 1.0520 - acc: 0.6218 - val_loss: 1.1084 - val_acc: 0.6196\n",
      "Epoch 20/500\n",
      "5833/5833 - 2s - loss: 1.0175 - acc: 0.6302 - val_loss: 1.2432 - val_acc: 0.5833\n",
      "Epoch 21/500\n",
      "5833/5833 - 2s - loss: 0.9855 - acc: 0.6450 - val_loss: 1.2228 - val_acc: 0.5805\n",
      "Epoch 22/500\n",
      "5833/5833 - 2s - loss: 0.9750 - acc: 0.6480 - val_loss: 1.2451 - val_acc: 0.5805\n",
      "Epoch 23/500\n",
      "5833/5833 - 2s - loss: 0.9464 - acc: 0.6552 - val_loss: 1.1952 - val_acc: 0.5792\n",
      "Epoch 24/500\n",
      "5833/5833 - 2s - loss: 0.9259 - acc: 0.6662 - val_loss: 1.3075 - val_acc: 0.5661\n",
      "Epoch 25/500\n",
      "5833/5833 - 2s - loss: 0.9088 - acc: 0.6681 - val_loss: 1.0587 - val_acc: 0.6265\n",
      "Epoch 26/500\n",
      "5833/5833 - 2s - loss: 0.8862 - acc: 0.6840 - val_loss: 1.1501 - val_acc: 0.6237\n",
      "Epoch 27/500\n",
      "5833/5833 - 2s - loss: 0.8781 - acc: 0.6825 - val_loss: 1.0950 - val_acc: 0.6265\n",
      "Epoch 28/500\n",
      "5833/5833 - 2s - loss: 0.8666 - acc: 0.6847 - val_loss: 1.6690 - val_acc: 0.5230\n",
      "Epoch 29/500\n",
      "5833/5833 - 2s - loss: 0.8592 - acc: 0.6909 - val_loss: 1.1520 - val_acc: 0.6175\n",
      "Epoch 30/500\n",
      "5833/5833 - 2s - loss: 0.8395 - acc: 0.6931 - val_loss: 2.8564 - val_acc: 0.3831\n",
      "Epoch 31/500\n",
      "5833/5833 - 2s - loss: 0.8111 - acc: 0.7044 - val_loss: 1.5237 - val_acc: 0.5524\n",
      "Epoch 32/500\n",
      "5833/5833 - 2s - loss: 0.8050 - acc: 0.7055 - val_loss: 1.3175 - val_acc: 0.5874\n",
      "Epoch 33/500\n",
      "5833/5833 - 2s - loss: 0.7979 - acc: 0.7122 - val_loss: 1.3109 - val_acc: 0.5730\n",
      "Epoch 34/500\n",
      "5833/5833 - 2s - loss: 0.7582 - acc: 0.7214 - val_loss: 1.3954 - val_acc: 0.5757\n",
      "Epoch 35/500\n",
      "5833/5833 - 2s - loss: 0.7567 - acc: 0.7252 - val_loss: 1.3731 - val_acc: 0.5888\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "5833/5833 - 2s - loss: 0.7505 - acc: 0.7242 - val_loss: 1.6245 - val_acc: 0.5490\n",
      "Epoch 37/500\n",
      "5833/5833 - 2s - loss: 0.6763 - acc: 0.7595 - val_loss: 0.9596 - val_acc: 0.6744\n",
      "Epoch 38/500\n",
      "5833/5833 - 2s - loss: 0.6459 - acc: 0.7656 - val_loss: 0.9850 - val_acc: 0.6655\n",
      "Epoch 39/500\n",
      "5833/5833 - 2s - loss: 0.6286 - acc: 0.7732 - val_loss: 1.3111 - val_acc: 0.6038\n",
      "Epoch 40/500\n",
      "5833/5833 - 2s - loss: 0.6251 - acc: 0.7698 - val_loss: 0.9678 - val_acc: 0.6840\n",
      "Epoch 41/500\n",
      "5833/5833 - 2s - loss: 0.6159 - acc: 0.7811 - val_loss: 1.2433 - val_acc: 0.6463\n",
      "Epoch 42/500\n",
      "5833/5833 - 2s - loss: 0.5998 - acc: 0.7890 - val_loss: 0.8840 - val_acc: 0.7169\n",
      "Epoch 43/500\n",
      "5833/5833 - 2s - loss: 0.5911 - acc: 0.7886 - val_loss: 0.9233 - val_acc: 0.7066\n",
      "Epoch 44/500\n",
      "5833/5833 - 2s - loss: 0.5822 - acc: 0.7910 - val_loss: 1.1064 - val_acc: 0.6456\n",
      "Epoch 45/500\n",
      "5833/5833 - 2s - loss: 0.5743 - acc: 0.7910 - val_loss: 1.3174 - val_acc: 0.6319\n",
      "Epoch 46/500\n",
      "5833/5833 - 2s - loss: 0.5684 - acc: 0.7936 - val_loss: 2.2419 - val_acc: 0.5188\n",
      "Epoch 47/500\n",
      "5833/5833 - 2s - loss: 0.5519 - acc: 0.8016 - val_loss: 1.1716 - val_acc: 0.6443\n",
      "Epoch 48/500\n",
      "5833/5833 - 2s - loss: 0.5348 - acc: 0.8092 - val_loss: 0.8576 - val_acc: 0.7005\n",
      "Epoch 49/500\n",
      "5833/5833 - 2s - loss: 0.5286 - acc: 0.8032 - val_loss: 0.8750 - val_acc: 0.7156\n",
      "Epoch 50/500\n",
      "5833/5833 - 2s - loss: 0.5279 - acc: 0.8049 - val_loss: 1.4378 - val_acc: 0.5826\n",
      "Epoch 51/500\n",
      "5833/5833 - 2s - loss: 0.5177 - acc: 0.8087 - val_loss: 1.2940 - val_acc: 0.6306\n",
      "Epoch 52/500\n",
      "5833/5833 - 2s - loss: 0.5172 - acc: 0.8095 - val_loss: 0.9336 - val_acc: 0.7183\n",
      "Epoch 53/500\n",
      "5833/5833 - 2s - loss: 0.5106 - acc: 0.8171 - val_loss: 0.8884 - val_acc: 0.7197\n",
      "Epoch 54/500\n",
      "5833/5833 - 2s - loss: 0.4921 - acc: 0.8246 - val_loss: 1.6401 - val_acc: 0.5757\n",
      "Epoch 55/500\n",
      "5833/5833 - 2s - loss: 0.4945 - acc: 0.8188 - val_loss: 0.8403 - val_acc: 0.7286\n",
      "Epoch 56/500\n",
      "5833/5833 - 2s - loss: 0.4822 - acc: 0.8286 - val_loss: 0.9382 - val_acc: 0.7231\n",
      "Epoch 57/500\n",
      "5833/5833 - 2s - loss: 0.4666 - acc: 0.8284 - val_loss: 0.9535 - val_acc: 0.6971\n",
      "Epoch 58/500\n",
      "5833/5833 - 2s - loss: 0.4627 - acc: 0.8347 - val_loss: 0.8697 - val_acc: 0.7245\n",
      "Epoch 59/500\n",
      "5833/5833 - 2s - loss: 0.4637 - acc: 0.8250 - val_loss: 1.1869 - val_acc: 0.6635\n",
      "Epoch 60/500\n",
      "5833/5833 - 2s - loss: 0.4614 - acc: 0.8306 - val_loss: 1.1701 - val_acc: 0.6758\n",
      "Epoch 61/500\n",
      "5833/5833 - 2s - loss: 0.4368 - acc: 0.8380 - val_loss: 0.8608 - val_acc: 0.7265\n",
      "Epoch 62/500\n",
      "5833/5833 - 2s - loss: 0.4374 - acc: 0.8424 - val_loss: 0.8663 - val_acc: 0.7300\n",
      "Epoch 63/500\n",
      "5833/5833 - 2s - loss: 0.4452 - acc: 0.8349 - val_loss: 1.3709 - val_acc: 0.6539\n",
      "Epoch 64/500\n",
      "5833/5833 - 2s - loss: 0.4325 - acc: 0.8423 - val_loss: 1.6205 - val_acc: 0.6155\n",
      "Epoch 65/500\n",
      "5833/5833 - 2s - loss: 0.4261 - acc: 0.8478 - val_loss: 0.9488 - val_acc: 0.7121\n",
      "Epoch 66/500\n",
      "5833/5833 - 2s - loss: 0.4251 - acc: 0.8411 - val_loss: 0.9390 - val_acc: 0.7217\n",
      "Epoch 67/500\n",
      "5833/5833 - 2s - loss: 0.3996 - acc: 0.8562 - val_loss: 0.9881 - val_acc: 0.6991\n",
      "Epoch 68/500\n",
      "5833/5833 - 2s - loss: 0.4034 - acc: 0.8508 - val_loss: 1.5006 - val_acc: 0.6511\n",
      "Epoch 69/500\n",
      "5833/5833 - 2s - loss: 0.4052 - acc: 0.8580 - val_loss: 0.8954 - val_acc: 0.7341\n",
      "Epoch 70/500\n",
      "5833/5833 - 2s - loss: 0.4087 - acc: 0.8524 - val_loss: 0.8877 - val_acc: 0.7320\n",
      "Epoch 71/500\n",
      "5833/5833 - 2s - loss: 0.4098 - acc: 0.8534 - val_loss: 0.9655 - val_acc: 0.7217\n",
      "Epoch 72/500\n",
      "5833/5833 - 2s - loss: 0.3744 - acc: 0.8652 - val_loss: 0.8988 - val_acc: 0.7395\n",
      "Epoch 73/500\n",
      "5833/5833 - 2s - loss: 0.3881 - acc: 0.8599 - val_loss: 1.3004 - val_acc: 0.6813\n",
      "Epoch 74/500\n",
      "5833/5833 - 2s - loss: 0.3949 - acc: 0.8589 - val_loss: 0.8606 - val_acc: 0.7430\n",
      "Epoch 75/500\n",
      "5833/5833 - 2s - loss: 0.3832 - acc: 0.8580 - val_loss: 0.8894 - val_acc: 0.7437\n",
      "Epoch 76/500\n",
      "5833/5833 - 2s - loss: 0.3711 - acc: 0.8652 - val_loss: 0.9146 - val_acc: 0.7327\n",
      "Epoch 77/500\n",
      "5833/5833 - 2s - loss: 0.3840 - acc: 0.8608 - val_loss: 1.0759 - val_acc: 0.7025\n",
      "Epoch 78/500\n",
      "5833/5833 - 2s - loss: 0.3694 - acc: 0.8632 - val_loss: 1.1598 - val_acc: 0.6964\n",
      "Epoch 79/500\n",
      "5833/5833 - 2s - loss: 0.3690 - acc: 0.8666 - val_loss: 0.8619 - val_acc: 0.7553\n",
      "Epoch 80/500\n",
      "5833/5833 - 2s - loss: 0.3642 - acc: 0.8700 - val_loss: 1.1194 - val_acc: 0.7108\n",
      "Epoch 81/500\n",
      "5833/5833 - 2s - loss: 0.3694 - acc: 0.8649 - val_loss: 0.8387 - val_acc: 0.7567\n",
      "Epoch 82/500\n",
      "5833/5833 - 2s - loss: 0.3297 - acc: 0.8826 - val_loss: 1.0808 - val_acc: 0.7108\n",
      "Epoch 83/500\n",
      "5833/5833 - 2s - loss: 0.3486 - acc: 0.8738 - val_loss: 1.3746 - val_acc: 0.6628\n",
      "Epoch 84/500\n",
      "5833/5833 - 2s - loss: 0.3497 - acc: 0.8745 - val_loss: 0.8193 - val_acc: 0.7560\n",
      "Epoch 85/500\n",
      "5833/5833 - 2s - loss: 0.3248 - acc: 0.8845 - val_loss: 1.0829 - val_acc: 0.7053\n",
      "Epoch 86/500\n",
      "5833/5833 - 2s - loss: 0.3438 - acc: 0.8700 - val_loss: 0.8542 - val_acc: 0.7533\n",
      "Epoch 87/500\n",
      "5833/5833 - 2s - loss: 0.3320 - acc: 0.8819 - val_loss: 1.4919 - val_acc: 0.6285\n",
      "Epoch 88/500\n",
      "5833/5833 - 2s - loss: 0.3396 - acc: 0.8788 - val_loss: 0.9330 - val_acc: 0.7375\n",
      "Epoch 89/500\n",
      "5833/5833 - 2s - loss: 0.3090 - acc: 0.8846 - val_loss: 0.9046 - val_acc: 0.7471\n",
      "Epoch 90/500\n",
      "5833/5833 - 2s - loss: 0.3288 - acc: 0.8821 - val_loss: 0.8943 - val_acc: 0.7423\n",
      "Epoch 91/500\n",
      "5833/5833 - 2s - loss: 0.3181 - acc: 0.8838 - val_loss: 1.3659 - val_acc: 0.6669\n",
      "Epoch 92/500\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "5833/5833 - 2s - loss: 0.3034 - acc: 0.8923 - val_loss: 0.8868 - val_acc: 0.7485\n",
      "Epoch 93/500\n",
      "5833/5833 - 2s - loss: 0.2672 - acc: 0.9047 - val_loss: 0.8267 - val_acc: 0.7697\n",
      "Epoch 94/500\n",
      "5833/5833 - 2s - loss: 0.2571 - acc: 0.9052 - val_loss: 0.7499 - val_acc: 0.7978\n",
      "Epoch 95/500\n",
      "5833/5833 - 2s - loss: 0.2510 - acc: 0.9126 - val_loss: 1.1760 - val_acc: 0.7176\n",
      "Epoch 96/500\n",
      "5833/5833 - 2s - loss: 0.2528 - acc: 0.9105 - val_loss: 0.8082 - val_acc: 0.7656\n",
      "Epoch 97/500\n",
      "5833/5833 - 2s - loss: 0.2459 - acc: 0.9102 - val_loss: 0.7471 - val_acc: 0.7868\n",
      "Epoch 98/500\n",
      "5833/5833 - 2s - loss: 0.2378 - acc: 0.9133 - val_loss: 0.9042 - val_acc: 0.7594\n",
      "Epoch 99/500\n",
      "5833/5833 - 2s - loss: 0.2443 - acc: 0.9088 - val_loss: 0.8184 - val_acc: 0.7752\n",
      "Epoch 100/500\n",
      "5833/5833 - 2s - loss: 0.2359 - acc: 0.9153 - val_loss: 0.7605 - val_acc: 0.8040\n",
      "Epoch 101/500\n",
      "5833/5833 - 2s - loss: 0.2459 - acc: 0.9167 - val_loss: 0.9531 - val_acc: 0.7471\n",
      "Epoch 102/500\n",
      "5833/5833 - 2s - loss: 0.2365 - acc: 0.9177 - val_loss: 0.8942 - val_acc: 0.7546\n",
      "Epoch 103/500\n",
      "5833/5833 - 2s - loss: 0.2233 - acc: 0.9227 - val_loss: 0.9269 - val_acc: 0.7546\n",
      "Epoch 104/500\n",
      "5833/5833 - 2s - loss: 0.2227 - acc: 0.9201 - val_loss: 0.9900 - val_acc: 0.7450\n",
      "Epoch 105/500\n",
      "5833/5833 - 2s - loss: 0.2330 - acc: 0.9169 - val_loss: 0.9210 - val_acc: 0.7567\n",
      "Epoch 106/500\n",
      "5833/5833 - 2s - loss: 0.2183 - acc: 0.9222 - val_loss: 0.7478 - val_acc: 0.7820\n",
      "Epoch 107/500\n",
      "5833/5833 - 2s - loss: 0.2290 - acc: 0.9182 - val_loss: 0.7271 - val_acc: 0.8047\n",
      "Epoch 108/500\n",
      "5833/5833 - 2s - loss: 0.2239 - acc: 0.9167 - val_loss: 0.8101 - val_acc: 0.7889\n",
      "Epoch 109/500\n",
      "5833/5833 - 2s - loss: 0.2122 - acc: 0.9227 - val_loss: 0.8460 - val_acc: 0.7745\n",
      "Epoch 110/500\n",
      "5833/5833 - 2s - loss: 0.2162 - acc: 0.9222 - val_loss: 1.2320 - val_acc: 0.7012\n",
      "Epoch 111/500\n",
      "5833/5833 - 2s - loss: 0.2123 - acc: 0.9225 - val_loss: 0.8327 - val_acc: 0.7923\n",
      "Epoch 112/500\n",
      "5833/5833 - 2s - loss: 0.2118 - acc: 0.9230 - val_loss: 0.8869 - val_acc: 0.7745\n",
      "Epoch 113/500\n",
      "5833/5833 - 2s - loss: 0.2090 - acc: 0.9251 - val_loss: 0.9474 - val_acc: 0.7464\n",
      "Epoch 114/500\n",
      "5833/5833 - 2s - loss: 0.1973 - acc: 0.9297 - val_loss: 0.8325 - val_acc: 0.7896\n",
      "Epoch 115/500\n",
      "5833/5833 - 2s - loss: 0.1980 - acc: 0.9295 - val_loss: 1.0336 - val_acc: 0.7334\n",
      "Epoch 116/500\n",
      "5833/5833 - 2s - loss: 0.1971 - acc: 0.9290 - val_loss: 1.4251 - val_acc: 0.7169\n",
      "Epoch 117/500\n",
      "5833/5833 - 2s - loss: 0.2043 - acc: 0.9247 - val_loss: 0.7502 - val_acc: 0.8149\n",
      "Epoch 118/500\n",
      "5833/5833 - 2s - loss: 0.1984 - acc: 0.9270 - val_loss: 0.7579 - val_acc: 0.7834\n",
      "Epoch 119/500\n",
      "5833/5833 - 2s - loss: 0.1897 - acc: 0.9325 - val_loss: 1.2856 - val_acc: 0.7128\n",
      "Epoch 120/500\n",
      "5833/5833 - 2s - loss: 0.1903 - acc: 0.9331 - val_loss: 0.8225 - val_acc: 0.7882\n",
      "Epoch 121/500\n",
      "5833/5833 - 2s - loss: 0.1900 - acc: 0.9328 - val_loss: 0.8313 - val_acc: 0.7937\n",
      "Epoch 122/500\n",
      "5833/5833 - 2s - loss: 0.1837 - acc: 0.9321 - val_loss: 0.7938 - val_acc: 0.7937\n",
      "Epoch 123/500\n",
      "5833/5833 - 2s - loss: 0.1965 - acc: 0.9283 - val_loss: 0.9957 - val_acc: 0.7594\n",
      "Epoch 124/500\n",
      "5833/5833 - 2s - loss: 0.1913 - acc: 0.9323 - val_loss: 1.0754 - val_acc: 0.7402\n",
      "Epoch 125/500\n",
      "5833/5833 - 2s - loss: 0.1850 - acc: 0.9318 - val_loss: 1.0256 - val_acc: 0.7471\n",
      "Epoch 126/500\n",
      "5833/5833 - 2s - loss: 0.1886 - acc: 0.9326 - val_loss: 0.9605 - val_acc: 0.7656\n",
      "Epoch 127/500\n",
      "5833/5833 - 2s - loss: 0.1841 - acc: 0.9335 - val_loss: 0.8380 - val_acc: 0.7992\n",
      "Epoch 128/500\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "5833/5833 - 2s - loss: 0.1810 - acc: 0.9345 - val_loss: 0.9304 - val_acc: 0.7731\n",
      "Epoch 129/500\n",
      "5833/5833 - 2s - loss: 0.1509 - acc: 0.9441 - val_loss: 0.8148 - val_acc: 0.7978\n",
      "Epoch 130/500\n",
      "5833/5833 - 2s - loss: 0.1558 - acc: 0.9436 - val_loss: 0.8303 - val_acc: 0.7923\n",
      "Epoch 131/500\n",
      "5833/5833 - 2s - loss: 0.1498 - acc: 0.9481 - val_loss: 0.7596 - val_acc: 0.7992\n",
      "Epoch 132/500\n",
      "5833/5833 - 2s - loss: 0.1570 - acc: 0.9448 - val_loss: 0.8402 - val_acc: 0.7889\n",
      "Epoch 133/500\n",
      "5833/5833 - 2s - loss: 0.1468 - acc: 0.9498 - val_loss: 0.7628 - val_acc: 0.8019\n",
      "Epoch 134/500\n",
      "5833/5833 - 2s - loss: 0.1393 - acc: 0.9530 - val_loss: 0.7553 - val_acc: 0.8115\n",
      "Epoch 135/500\n",
      "5833/5833 - 2s - loss: 0.1437 - acc: 0.9518 - val_loss: 0.8323 - val_acc: 0.7944\n",
      "Epoch 00135: early stopping\n",
      "============================================================train 3th fold============================================================\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 60, 22, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 60, 22, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 60, 22, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 60, 22, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 60, 22, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 15, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 15, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 15, 5, 256)        295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 15, 5, 256)        1024      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 15, 5, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 15, 5, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 15, 5, 512)        2048      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 1,622,291\n",
      "Trainable params: 1,620,115\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "Train on 5834 samples, validate on 1458 samples\n",
      "Epoch 1/500\n",
      "5834/5834 - 5s - loss: 2.2246 - acc: 0.3097 - val_loss: 2.8748 - val_acc: 0.1269\n",
      "Epoch 2/500\n",
      "5834/5834 - 2s - loss: 1.8666 - acc: 0.3769 - val_loss: 3.3949 - val_acc: 0.0720\n",
      "Epoch 3/500\n",
      "5834/5834 - 2s - loss: 1.7006 - acc: 0.4176 - val_loss: 2.8235 - val_acc: 0.0912\n",
      "Epoch 4/500\n",
      "5834/5834 - 2s - loss: 1.6027 - acc: 0.4484 - val_loss: 2.6627 - val_acc: 0.1955\n",
      "Epoch 5/500\n",
      "5834/5834 - 2s - loss: 1.5169 - acc: 0.4697 - val_loss: 2.4931 - val_acc: 0.2936\n",
      "Epoch 6/500\n",
      "5834/5834 - 2s - loss: 1.4683 - acc: 0.4851 - val_loss: 2.6937 - val_acc: 0.2593\n",
      "Epoch 7/500\n",
      "5834/5834 - 2s - loss: 1.3977 - acc: 0.5127 - val_loss: 1.8786 - val_acc: 0.4040\n",
      "Epoch 8/500\n",
      "5834/5834 - 2s - loss: 1.3526 - acc: 0.5103 - val_loss: 2.1528 - val_acc: 0.3258\n",
      "Epoch 9/500\n",
      "5834/5834 - 2s - loss: 1.3186 - acc: 0.5300 - val_loss: 1.7207 - val_acc: 0.4335\n",
      "Epoch 10/500\n",
      "5834/5834 - 2s - loss: 1.2849 - acc: 0.5441 - val_loss: 1.5007 - val_acc: 0.4808\n",
      "Epoch 11/500\n",
      "5834/5834 - 2s - loss: 1.2480 - acc: 0.5590 - val_loss: 1.5638 - val_acc: 0.4520\n",
      "Epoch 12/500\n",
      "5834/5834 - 2s - loss: 1.2438 - acc: 0.5531 - val_loss: 1.6636 - val_acc: 0.4294\n",
      "Epoch 13/500\n",
      "5834/5834 - 2s - loss: 1.2020 - acc: 0.5692 - val_loss: 1.4803 - val_acc: 0.4897\n",
      "Epoch 14/500\n",
      "5834/5834 - 2s - loss: 1.1723 - acc: 0.5857 - val_loss: 1.5488 - val_acc: 0.4664\n",
      "Epoch 15/500\n",
      "5834/5834 - 2s - loss: 1.1616 - acc: 0.5795 - val_loss: 2.5652 - val_acc: 0.3422\n",
      "Epoch 16/500\n",
      "5834/5834 - 2s - loss: 1.1311 - acc: 0.5879 - val_loss: 1.4305 - val_acc: 0.4945\n",
      "Epoch 17/500\n",
      "5834/5834 - 2s - loss: 1.1145 - acc: 0.5907 - val_loss: 1.4561 - val_acc: 0.5165\n",
      "Epoch 18/500\n",
      "5834/5834 - 2s - loss: 1.0810 - acc: 0.6154 - val_loss: 1.8497 - val_acc: 0.4348\n",
      "Epoch 19/500\n",
      "5834/5834 - 2s - loss: 1.0609 - acc: 0.6104 - val_loss: 1.2599 - val_acc: 0.5610\n",
      "Epoch 20/500\n",
      "5834/5834 - 2s - loss: 1.0434 - acc: 0.6220 - val_loss: 1.2204 - val_acc: 0.5720\n",
      "Epoch 21/500\n",
      "5834/5834 - 2s - loss: 1.0186 - acc: 0.6241 - val_loss: 1.4446 - val_acc: 0.5151\n",
      "Epoch 22/500\n",
      "5834/5834 - 2s - loss: 0.9977 - acc: 0.6363 - val_loss: 1.2286 - val_acc: 0.5782\n",
      "Epoch 23/500\n",
      "5834/5834 - 2s - loss: 0.9889 - acc: 0.6344 - val_loss: 1.2944 - val_acc: 0.5562\n",
      "Epoch 24/500\n",
      "5834/5834 - 2s - loss: 0.9550 - acc: 0.6543 - val_loss: 1.1603 - val_acc: 0.5892\n",
      "Epoch 25/500\n",
      "5834/5834 - 2s - loss: 0.9611 - acc: 0.6522 - val_loss: 1.4773 - val_acc: 0.5082\n",
      "Epoch 26/500\n",
      "5834/5834 - 2s - loss: 0.9419 - acc: 0.6579 - val_loss: 2.2251 - val_acc: 0.4184\n",
      "Epoch 27/500\n",
      "5834/5834 - 2s - loss: 0.9270 - acc: 0.6572 - val_loss: 1.0932 - val_acc: 0.6084\n",
      "Epoch 28/500\n",
      "5834/5834 - 2s - loss: 0.8923 - acc: 0.6791 - val_loss: 1.0758 - val_acc: 0.6235\n",
      "Epoch 29/500\n",
      "5834/5834 - 2s - loss: 0.8924 - acc: 0.6805 - val_loss: 1.9139 - val_acc: 0.4767\n",
      "Epoch 30/500\n",
      "5834/5834 - 2s - loss: 0.8676 - acc: 0.6856 - val_loss: 1.2885 - val_acc: 0.5727\n",
      "Epoch 31/500\n",
      "5834/5834 - 2s - loss: 0.8533 - acc: 0.6911 - val_loss: 1.1824 - val_acc: 0.6001\n",
      "Epoch 32/500\n",
      "5834/5834 - 2s - loss: 0.8328 - acc: 0.7023 - val_loss: 1.4950 - val_acc: 0.5267\n",
      "Epoch 33/500\n",
      "5834/5834 - 2s - loss: 0.8267 - acc: 0.7045 - val_loss: 1.0385 - val_acc: 0.6331\n",
      "Epoch 34/500\n",
      "5834/5834 - 2s - loss: 0.7989 - acc: 0.7065 - val_loss: 1.2625 - val_acc: 0.5830\n",
      "Epoch 35/500\n",
      "5834/5834 - 2s - loss: 0.8053 - acc: 0.7040 - val_loss: 1.0241 - val_acc: 0.6639\n",
      "Epoch 36/500\n",
      "5834/5834 - 2s - loss: 0.7966 - acc: 0.7137 - val_loss: 1.8730 - val_acc: 0.5041\n",
      "Epoch 37/500\n",
      "5834/5834 - 2s - loss: 0.7861 - acc: 0.7122 - val_loss: 1.4449 - val_acc: 0.5830\n",
      "Epoch 38/500\n",
      "5834/5834 - 2s - loss: 0.7614 - acc: 0.7305 - val_loss: 1.1313 - val_acc: 0.6392\n",
      "Epoch 39/500\n",
      "5834/5834 - 2s - loss: 0.7381 - acc: 0.7324 - val_loss: 1.4133 - val_acc: 0.5466\n",
      "Epoch 40/500\n",
      "5834/5834 - 2s - loss: 0.7361 - acc: 0.7307 - val_loss: 1.0799 - val_acc: 0.6406\n",
      "Epoch 41/500\n",
      "5834/5834 - 2s - loss: 0.7257 - acc: 0.7389 - val_loss: 1.1304 - val_acc: 0.6221\n",
      "Epoch 42/500\n",
      "5834/5834 - 2s - loss: 0.7161 - acc: 0.7455 - val_loss: 1.4458 - val_acc: 0.5535\n",
      "Epoch 43/500\n",
      "5834/5834 - 2s - loss: 0.6976 - acc: 0.7449 - val_loss: 1.1986 - val_acc: 0.6049\n",
      "Epoch 44/500\n",
      "5834/5834 - 2s - loss: 0.7023 - acc: 0.7441 - val_loss: 1.3328 - val_acc: 0.6015\n",
      "Epoch 45/500\n",
      "5834/5834 - 2s - loss: 0.6725 - acc: 0.7559 - val_loss: 1.0225 - val_acc: 0.6701\n",
      "Epoch 46/500\n",
      "5834/5834 - 2s - loss: 0.6784 - acc: 0.7513 - val_loss: 0.9823 - val_acc: 0.6907\n",
      "Epoch 47/500\n",
      "5834/5834 - 2s - loss: 0.6574 - acc: 0.7609 - val_loss: 1.1024 - val_acc: 0.6276\n",
      "Epoch 48/500\n",
      "5834/5834 - 2s - loss: 0.6516 - acc: 0.7619 - val_loss: 0.8801 - val_acc: 0.6948\n",
      "Epoch 49/500\n",
      "5834/5834 - 2s - loss: 0.6479 - acc: 0.7638 - val_loss: 0.9916 - val_acc: 0.6674\n",
      "Epoch 50/500\n",
      "5834/5834 - 2s - loss: 0.6354 - acc: 0.7724 - val_loss: 1.0924 - val_acc: 0.6564\n",
      "Epoch 51/500\n",
      "5834/5834 - 2s - loss: 0.6144 - acc: 0.7770 - val_loss: 0.8843 - val_acc: 0.6968\n",
      "Epoch 52/500\n",
      "5834/5834 - 2s - loss: 0.6041 - acc: 0.7827 - val_loss: 1.6316 - val_acc: 0.5604\n",
      "Epoch 53/500\n",
      "5834/5834 - 2s - loss: 0.5958 - acc: 0.7830 - val_loss: 1.2768 - val_acc: 0.6235\n",
      "Epoch 54/500\n",
      "5834/5834 - 2s - loss: 0.5843 - acc: 0.7869 - val_loss: 0.8652 - val_acc: 0.7160\n",
      "Epoch 55/500\n",
      "5834/5834 - 2s - loss: 0.5809 - acc: 0.7895 - val_loss: 1.2221 - val_acc: 0.6200\n",
      "Epoch 56/500\n",
      "5834/5834 - 2s - loss: 0.5755 - acc: 0.7902 - val_loss: 0.9248 - val_acc: 0.6941\n",
      "Epoch 57/500\n",
      "5834/5834 - 2s - loss: 0.5690 - acc: 0.7950 - val_loss: 1.2632 - val_acc: 0.6139\n",
      "Epoch 58/500\n",
      "5834/5834 - 2s - loss: 0.5585 - acc: 0.7962 - val_loss: 1.6252 - val_acc: 0.5796\n",
      "Epoch 59/500\n",
      "5834/5834 - 2s - loss: 0.5538 - acc: 0.7926 - val_loss: 1.3242 - val_acc: 0.5775\n",
      "Epoch 60/500\n",
      "5834/5834 - 2s - loss: 0.5430 - acc: 0.8010 - val_loss: 1.0567 - val_acc: 0.6529\n",
      "Epoch 61/500\n",
      "5834/5834 - 2s - loss: 0.5362 - acc: 0.7993 - val_loss: 0.9722 - val_acc: 0.6811\n",
      "Epoch 62/500\n",
      "5834/5834 - 2s - loss: 0.5227 - acc: 0.8125 - val_loss: 1.3427 - val_acc: 0.6337\n",
      "Epoch 63/500\n",
      "5834/5834 - 2s - loss: 0.5327 - acc: 0.8085 - val_loss: 1.5086 - val_acc: 0.5947\n",
      "Epoch 64/500\n",
      "5834/5834 - 2s - loss: 0.5094 - acc: 0.8173 - val_loss: 1.4099 - val_acc: 0.6084\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "5834/5834 - 2s - loss: 0.4993 - acc: 0.8162 - val_loss: 1.8475 - val_acc: 0.5439\n",
      "Epoch 66/500\n",
      "5834/5834 - 2s - loss: 0.4264 - acc: 0.8476 - val_loss: 0.7892 - val_acc: 0.7407\n",
      "Epoch 67/500\n",
      "5834/5834 - 2s - loss: 0.4210 - acc: 0.8485 - val_loss: 0.8340 - val_acc: 0.7353\n",
      "Epoch 68/500\n",
      "5834/5834 - 2s - loss: 0.4169 - acc: 0.8507 - val_loss: 1.2583 - val_acc: 0.6385\n",
      "Epoch 69/500\n",
      "5834/5834 - 2s - loss: 0.3926 - acc: 0.8598 - val_loss: 0.9351 - val_acc: 0.6920\n",
      "Epoch 70/500\n",
      "5834/5834 - 2s - loss: 0.3845 - acc: 0.8632 - val_loss: 1.1399 - val_acc: 0.6598\n",
      "Epoch 71/500\n",
      "5834/5834 - 2s - loss: 0.3830 - acc: 0.8608 - val_loss: 0.9253 - val_acc: 0.7195\n",
      "Epoch 72/500\n",
      "5834/5834 - 2s - loss: 0.3864 - acc: 0.8620 - val_loss: 1.0162 - val_acc: 0.6962\n",
      "Epoch 73/500\n",
      "5834/5834 - 2s - loss: 0.3798 - acc: 0.8615 - val_loss: 1.0871 - val_acc: 0.6804\n",
      "Epoch 74/500\n",
      "5834/5834 - 2s - loss: 0.3921 - acc: 0.8572 - val_loss: 1.1706 - val_acc: 0.6584\n",
      "Epoch 75/500\n",
      "5834/5834 - 2s - loss: 0.3615 - acc: 0.8692 - val_loss: 1.6828 - val_acc: 0.6118\n",
      "Epoch 76/500\n",
      "5834/5834 - 2s - loss: 0.3393 - acc: 0.8790 - val_loss: 0.8610 - val_acc: 0.7380\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "5834/5834 - 2s - loss: 0.3549 - acc: 0.8781 - val_loss: 0.9168 - val_acc: 0.7099\n",
      "Epoch 78/500\n",
      "5834/5834 - 2s - loss: 0.3136 - acc: 0.8876 - val_loss: 0.8451 - val_acc: 0.7469\n",
      "Epoch 79/500\n",
      "5834/5834 - 2s - loss: 0.3022 - acc: 0.8922 - val_loss: 0.9520 - val_acc: 0.7298\n",
      "Epoch 80/500\n",
      "5834/5834 - 2s - loss: 0.2921 - acc: 0.8963 - val_loss: 0.8304 - val_acc: 0.7599\n",
      "Epoch 81/500\n",
      "5834/5834 - 2s - loss: 0.2952 - acc: 0.8954 - val_loss: 0.7622 - val_acc: 0.7675\n",
      "Epoch 82/500\n",
      "5834/5834 - 2s - loss: 0.2938 - acc: 0.8966 - val_loss: 1.2494 - val_acc: 0.6626\n",
      "Epoch 83/500\n",
      "5834/5834 - 2s - loss: 0.2846 - acc: 0.8999 - val_loss: 0.9271 - val_acc: 0.7209\n",
      "Epoch 84/500\n",
      "5834/5834 - 2s - loss: 0.2813 - acc: 0.8970 - val_loss: 0.9150 - val_acc: 0.7305\n",
      "Epoch 85/500\n",
      "5834/5834 - 2s - loss: 0.2827 - acc: 0.9013 - val_loss: 0.7765 - val_acc: 0.7798\n",
      "Epoch 86/500\n",
      "5834/5834 - 2s - loss: 0.2851 - acc: 0.8941 - val_loss: 0.7546 - val_acc: 0.7778\n",
      "Epoch 87/500\n",
      "5834/5834 - 2s - loss: 0.2762 - acc: 0.9014 - val_loss: 0.8352 - val_acc: 0.7565\n",
      "Epoch 88/500\n",
      "5834/5834 - 2s - loss: 0.2667 - acc: 0.9021 - val_loss: 0.8859 - val_acc: 0.7407\n",
      "Epoch 89/500\n",
      "5834/5834 - 2s - loss: 0.2673 - acc: 0.9074 - val_loss: 0.9246 - val_acc: 0.7407\n",
      "Epoch 90/500\n",
      "5834/5834 - 2s - loss: 0.2713 - acc: 0.9050 - val_loss: 0.7600 - val_acc: 0.7785\n",
      "Epoch 91/500\n",
      "5834/5834 - 2s - loss: 0.2566 - acc: 0.9114 - val_loss: 0.8597 - val_acc: 0.7476\n",
      "Epoch 92/500\n",
      "5834/5834 - 2s - loss: 0.2538 - acc: 0.9092 - val_loss: 1.4443 - val_acc: 0.6591\n",
      "Epoch 93/500\n",
      "5834/5834 - 2s - loss: 0.2448 - acc: 0.9146 - val_loss: 0.9506 - val_acc: 0.7311\n",
      "Epoch 94/500\n",
      "5834/5834 - 2s - loss: 0.2415 - acc: 0.9169 - val_loss: 0.8026 - val_acc: 0.7743\n",
      "Epoch 95/500\n",
      "5834/5834 - 2s - loss: 0.2488 - acc: 0.9112 - val_loss: 0.7836 - val_acc: 0.7949\n",
      "Epoch 96/500\n",
      "5834/5834 - 2s - loss: 0.2394 - acc: 0.9133 - val_loss: 0.9238 - val_acc: 0.7531\n",
      "Epoch 97/500\n",
      "5834/5834 - 2s - loss: 0.2512 - acc: 0.9085 - val_loss: 0.8380 - val_acc: 0.7586\n",
      "Epoch 98/500\n",
      "5834/5834 - 2s - loss: 0.2349 - acc: 0.9174 - val_loss: 1.5224 - val_acc: 0.6536\n",
      "Epoch 99/500\n",
      "5834/5834 - 2s - loss: 0.2366 - acc: 0.9164 - val_loss: 0.8752 - val_acc: 0.7551\n",
      "Epoch 100/500\n",
      "5834/5834 - 2s - loss: 0.2373 - acc: 0.9193 - val_loss: 0.8541 - val_acc: 0.7606\n",
      "Epoch 101/500\n",
      "5834/5834 - 2s - loss: 0.2319 - acc: 0.9158 - val_loss: 0.8556 - val_acc: 0.7661\n",
      "Epoch 102/500\n",
      "5834/5834 - 2s - loss: 0.2106 - acc: 0.9249 - val_loss: 0.8153 - val_acc: 0.7791\n",
      "Epoch 103/500\n",
      "5834/5834 - 2s - loss: 0.2253 - acc: 0.9179 - val_loss: 1.4280 - val_acc: 0.6660\n",
      "Epoch 104/500\n",
      "5834/5834 - 2s - loss: 0.2389 - acc: 0.9141 - val_loss: 0.8735 - val_acc: 0.7545\n",
      "Epoch 105/500\n",
      "5834/5834 - 2s - loss: 0.2258 - acc: 0.9196 - val_loss: 1.0260 - val_acc: 0.7160\n",
      "Epoch 106/500\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "5834/5834 - 2s - loss: 0.2137 - acc: 0.9273 - val_loss: 1.0521 - val_acc: 0.7401\n",
      "Epoch 107/500\n",
      "5834/5834 - 2s - loss: 0.2035 - acc: 0.9287 - val_loss: 0.9079 - val_acc: 0.7538\n",
      "Epoch 108/500\n",
      "5834/5834 - 2s - loss: 0.1930 - acc: 0.9309 - val_loss: 0.8686 - val_acc: 0.7675\n",
      "Epoch 109/500\n",
      "5834/5834 - 2s - loss: 0.1947 - acc: 0.9306 - val_loss: 0.7909 - val_acc: 0.7867\n",
      "Epoch 110/500\n",
      "5834/5834 - 2s - loss: 0.1798 - acc: 0.9368 - val_loss: 0.7976 - val_acc: 0.7764\n",
      "Epoch 111/500\n",
      "5834/5834 - 2s - loss: 0.1793 - acc: 0.9361 - val_loss: 0.7946 - val_acc: 0.7833\n",
      "Epoch 112/500\n",
      "5834/5834 - 2s - loss: 0.1849 - acc: 0.9344 - val_loss: 0.8034 - val_acc: 0.7888\n",
      "Epoch 113/500\n",
      "5834/5834 - 2s - loss: 0.1816 - acc: 0.9368 - val_loss: 0.8886 - val_acc: 0.7668\n",
      "Epoch 00113: early stopping\n",
      "============================================================train 4th fold============================================================\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 60, 22, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 60, 22, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 60, 22, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 60, 22, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 60, 22, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 15, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 15, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 15, 5, 256)        295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 15, 5, 256)        1024      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 15, 5, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 15, 5, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 15, 5, 512)        2048      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 1,622,291\n",
      "Trainable params: 1,620,115\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "Train on 5834 samples, validate on 1458 samples\n",
      "Epoch 1/500\n",
      "5834/5834 - 4s - loss: 2.2592 - acc: 0.2948 - val_loss: 3.1177 - val_acc: 0.1379\n",
      "Epoch 2/500\n",
      "5834/5834 - 2s - loss: 1.8523 - acc: 0.3697 - val_loss: 2.8100 - val_acc: 0.1646\n",
      "Epoch 3/500\n",
      "5834/5834 - 2s - loss: 1.6829 - acc: 0.4203 - val_loss: 3.0513 - val_acc: 0.2044\n",
      "Epoch 4/500\n",
      "5834/5834 - 2s - loss: 1.5872 - acc: 0.4498 - val_loss: 4.7122 - val_acc: 0.1406\n",
      "Epoch 5/500\n",
      "5834/5834 - 2s - loss: 1.5262 - acc: 0.4626 - val_loss: 2.1033 - val_acc: 0.3395\n",
      "Epoch 6/500\n",
      "5834/5834 - 2s - loss: 1.4546 - acc: 0.4805 - val_loss: 2.5834 - val_acc: 0.2990\n",
      "Epoch 7/500\n",
      "5834/5834 - 2s - loss: 1.3979 - acc: 0.4990 - val_loss: 2.2964 - val_acc: 0.3395\n",
      "Epoch 8/500\n",
      "5834/5834 - 2s - loss: 1.3831 - acc: 0.5000 - val_loss: 3.5219 - val_acc: 0.2702\n",
      "Epoch 9/500\n",
      "5834/5834 - 2s - loss: 1.3151 - acc: 0.5259 - val_loss: 1.6762 - val_acc: 0.4424\n",
      "Epoch 10/500\n",
      "5834/5834 - 2s - loss: 1.2852 - acc: 0.5363 - val_loss: 1.7129 - val_acc: 0.4369\n",
      "Epoch 11/500\n",
      "5834/5834 - 2s - loss: 1.2338 - acc: 0.5526 - val_loss: 1.5208 - val_acc: 0.4945\n",
      "Epoch 12/500\n",
      "5834/5834 - 2s - loss: 1.2375 - acc: 0.5501 - val_loss: 1.5066 - val_acc: 0.4767\n",
      "Epoch 13/500\n",
      "5834/5834 - 2s - loss: 1.1821 - acc: 0.5749 - val_loss: 1.6227 - val_acc: 0.4691\n",
      "Epoch 14/500\n",
      "5834/5834 - 2s - loss: 1.1646 - acc: 0.5732 - val_loss: 1.7129 - val_acc: 0.4664\n",
      "Epoch 15/500\n",
      "5834/5834 - 2s - loss: 1.1512 - acc: 0.5826 - val_loss: 1.3794 - val_acc: 0.5185\n",
      "Epoch 16/500\n",
      "5834/5834 - 2s - loss: 1.1160 - acc: 0.5955 - val_loss: 1.3495 - val_acc: 0.5261\n",
      "Epoch 17/500\n",
      "5834/5834 - 2s - loss: 1.0869 - acc: 0.6085 - val_loss: 1.7339 - val_acc: 0.4348\n",
      "Epoch 18/500\n",
      "5834/5834 - 2s - loss: 1.0675 - acc: 0.6138 - val_loss: 1.4471 - val_acc: 0.5007\n",
      "Epoch 19/500\n",
      "5834/5834 - 2s - loss: 1.0455 - acc: 0.6282 - val_loss: 1.3202 - val_acc: 0.5466\n",
      "Epoch 20/500\n",
      "5834/5834 - 2s - loss: 1.0252 - acc: 0.6296 - val_loss: 1.4426 - val_acc: 0.5007\n",
      "Epoch 21/500\n",
      "5834/5834 - 2s - loss: 1.0119 - acc: 0.6294 - val_loss: 1.3158 - val_acc: 0.5473\n",
      "Epoch 22/500\n",
      "5834/5834 - 2s - loss: 1.0003 - acc: 0.6412 - val_loss: 1.2737 - val_acc: 0.5796\n",
      "Epoch 23/500\n",
      "5834/5834 - 2s - loss: 0.9851 - acc: 0.6394 - val_loss: 1.7475 - val_acc: 0.4863\n",
      "Epoch 24/500\n",
      "5834/5834 - 2s - loss: 0.9593 - acc: 0.6592 - val_loss: 1.3767 - val_acc: 0.5549\n",
      "Epoch 25/500\n",
      "5834/5834 - 2s - loss: 0.9300 - acc: 0.6671 - val_loss: 1.4725 - val_acc: 0.5514\n",
      "Epoch 26/500\n",
      "5834/5834 - 2s - loss: 0.9224 - acc: 0.6678 - val_loss: 1.4108 - val_acc: 0.5384\n",
      "Epoch 27/500\n",
      "5834/5834 - 2s - loss: 0.8896 - acc: 0.6841 - val_loss: 1.4747 - val_acc: 0.5219\n",
      "Epoch 28/500\n",
      "5834/5834 - 2s - loss: 0.8874 - acc: 0.6748 - val_loss: 1.5984 - val_acc: 0.4993\n",
      "Epoch 29/500\n",
      "5834/5834 - 2s - loss: 0.8632 - acc: 0.6868 - val_loss: 1.1387 - val_acc: 0.6166\n",
      "Epoch 30/500\n",
      "5834/5834 - 2s - loss: 0.8459 - acc: 0.6913 - val_loss: 1.1414 - val_acc: 0.6118\n",
      "Epoch 31/500\n",
      "5834/5834 - 2s - loss: 0.8426 - acc: 0.6901 - val_loss: 1.0338 - val_acc: 0.6399\n",
      "Epoch 32/500\n",
      "5834/5834 - 2s - loss: 0.8252 - acc: 0.7012 - val_loss: 1.1442 - val_acc: 0.6200\n",
      "Epoch 33/500\n",
      "5834/5834 - 2s - loss: 0.8065 - acc: 0.7077 - val_loss: 1.1080 - val_acc: 0.6269\n",
      "Epoch 34/500\n",
      "5834/5834 - 2s - loss: 0.7883 - acc: 0.7139 - val_loss: 1.3348 - val_acc: 0.5693\n",
      "Epoch 35/500\n",
      "5834/5834 - 2s - loss: 0.7776 - acc: 0.7168 - val_loss: 1.2903 - val_acc: 0.6029\n",
      "Epoch 36/500\n",
      "5834/5834 - 2s - loss: 0.7634 - acc: 0.7242 - val_loss: 3.2921 - val_acc: 0.3107\n",
      "Epoch 37/500\n",
      "5834/5834 - 2s - loss: 0.7614 - acc: 0.7230 - val_loss: 1.5207 - val_acc: 0.5309\n",
      "Epoch 38/500\n",
      "5834/5834 - 2s - loss: 0.7376 - acc: 0.7343 - val_loss: 1.1236 - val_acc: 0.6303\n",
      "Epoch 39/500\n",
      "5834/5834 - 2s - loss: 0.7381 - acc: 0.7347 - val_loss: 1.1924 - val_acc: 0.6159\n",
      "Epoch 40/500\n",
      "5834/5834 - 2s - loss: 0.7054 - acc: 0.7374 - val_loss: 1.6328 - val_acc: 0.5562\n",
      "Epoch 41/500\n",
      "5834/5834 - 2s - loss: 0.7047 - acc: 0.7413 - val_loss: 1.4710 - val_acc: 0.5672\n",
      "Epoch 42/500\n",
      "5834/5834 - 2s - loss: 0.6868 - acc: 0.7544 - val_loss: 0.9810 - val_acc: 0.6735\n",
      "Epoch 43/500\n",
      "5834/5834 - 2s - loss: 0.6743 - acc: 0.7640 - val_loss: 1.2801 - val_acc: 0.5898\n",
      "Epoch 44/500\n",
      "5834/5834 - 2s - loss: 0.6726 - acc: 0.7641 - val_loss: 1.0771 - val_acc: 0.6632\n",
      "Epoch 45/500\n",
      "5834/5834 - 2s - loss: 0.6588 - acc: 0.7604 - val_loss: 1.1688 - val_acc: 0.6255\n",
      "Epoch 46/500\n",
      "5834/5834 - 2s - loss: 0.6374 - acc: 0.7672 - val_loss: 1.3752 - val_acc: 0.5981\n",
      "Epoch 47/500\n",
      "5834/5834 - 2s - loss: 0.6336 - acc: 0.7744 - val_loss: 1.1052 - val_acc: 0.6420\n",
      "Epoch 48/500\n",
      "5834/5834 - 2s - loss: 0.6328 - acc: 0.7698 - val_loss: 2.3611 - val_acc: 0.4184\n",
      "Epoch 49/500\n",
      "5834/5834 - 2s - loss: 0.6250 - acc: 0.7710 - val_loss: 1.2737 - val_acc: 0.6214\n",
      "Epoch 50/500\n",
      "5834/5834 - 2s - loss: 0.5946 - acc: 0.7775 - val_loss: 1.3427 - val_acc: 0.6070\n",
      "Epoch 51/500\n",
      "5834/5834 - 2s - loss: 0.6141 - acc: 0.7813 - val_loss: 0.9606 - val_acc: 0.6893\n",
      "Epoch 52/500\n",
      "5834/5834 - 2s - loss: 0.5905 - acc: 0.7847 - val_loss: 1.1667 - val_acc: 0.6358\n",
      "Epoch 53/500\n",
      "5834/5834 - 2s - loss: 0.5788 - acc: 0.7868 - val_loss: 1.1496 - val_acc: 0.6475\n",
      "Epoch 54/500\n",
      "5834/5834 - 2s - loss: 0.5679 - acc: 0.7859 - val_loss: 1.4890 - val_acc: 0.6262\n",
      "Epoch 55/500\n",
      "5834/5834 - 2s - loss: 0.5609 - acc: 0.7941 - val_loss: 1.0447 - val_acc: 0.6674\n",
      "Epoch 56/500\n",
      "5834/5834 - 2s - loss: 0.5571 - acc: 0.8015 - val_loss: 1.0789 - val_acc: 0.6667\n",
      "Epoch 57/500\n",
      "5834/5834 - 2s - loss: 0.5314 - acc: 0.8128 - val_loss: 2.0084 - val_acc: 0.5247\n",
      "Epoch 58/500\n",
      "5834/5834 - 2s - loss: 0.5429 - acc: 0.7983 - val_loss: 1.5107 - val_acc: 0.5892\n",
      "Epoch 59/500\n",
      "5834/5834 - 2s - loss: 0.5255 - acc: 0.8061 - val_loss: 1.1890 - val_acc: 0.6653\n",
      "Epoch 60/500\n",
      "5834/5834 - 2s - loss: 0.5279 - acc: 0.8092 - val_loss: 1.5471 - val_acc: 0.5720\n",
      "Epoch 61/500\n",
      "5834/5834 - 2s - loss: 0.5146 - acc: 0.8152 - val_loss: 1.6360 - val_acc: 0.5604\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "5834/5834 - 2s - loss: 0.5052 - acc: 0.8173 - val_loss: 1.5254 - val_acc: 0.6193\n",
      "Epoch 63/500\n",
      "5834/5834 - 2s - loss: 0.4328 - acc: 0.8426 - val_loss: 1.6182 - val_acc: 0.5960\n",
      "Epoch 64/500\n",
      "5834/5834 - 2s - loss: 0.4176 - acc: 0.8521 - val_loss: 0.9098 - val_acc: 0.7133\n",
      "Epoch 65/500\n",
      "5834/5834 - 2s - loss: 0.4073 - acc: 0.8504 - val_loss: 0.7899 - val_acc: 0.7593\n",
      "Epoch 66/500\n",
      "5834/5834 - 2s - loss: 0.4124 - acc: 0.8512 - val_loss: 1.1005 - val_acc: 0.6811\n",
      "Epoch 67/500\n",
      "5834/5834 - 2s - loss: 0.3970 - acc: 0.8600 - val_loss: 0.9516 - val_acc: 0.6996\n",
      "Epoch 68/500\n",
      "5834/5834 - 2s - loss: 0.3901 - acc: 0.8564 - val_loss: 1.0207 - val_acc: 0.6866\n",
      "Epoch 69/500\n",
      "5834/5834 - 2s - loss: 0.3716 - acc: 0.8701 - val_loss: 1.4935 - val_acc: 0.6029\n",
      "Epoch 70/500\n",
      "5834/5834 - 2s - loss: 0.3661 - acc: 0.8648 - val_loss: 1.0523 - val_acc: 0.7140\n",
      "Epoch 71/500\n",
      "5834/5834 - 2s - loss: 0.3638 - acc: 0.8663 - val_loss: 0.8261 - val_acc: 0.7524\n",
      "Epoch 72/500\n",
      "5834/5834 - 2s - loss: 0.3601 - acc: 0.8656 - val_loss: 0.9988 - val_acc: 0.7133\n",
      "Epoch 73/500\n",
      "5834/5834 - 2s - loss: 0.3565 - acc: 0.8740 - val_loss: 1.0104 - val_acc: 0.6968\n",
      "Epoch 74/500\n",
      "5834/5834 - 2s - loss: 0.3526 - acc: 0.8706 - val_loss: 1.0784 - val_acc: 0.7037\n",
      "Epoch 75/500\n",
      "5834/5834 - 2s - loss: 0.3628 - acc: 0.8675 - val_loss: 1.1195 - val_acc: 0.6701\n",
      "Epoch 76/500\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "5834/5834 - 2s - loss: 0.3437 - acc: 0.8769 - val_loss: 1.5713 - val_acc: 0.6344\n",
      "Epoch 77/500\n",
      "5834/5834 - 2s - loss: 0.3170 - acc: 0.8833 - val_loss: 0.9400 - val_acc: 0.7291\n",
      "Epoch 78/500\n",
      "5834/5834 - 2s - loss: 0.2933 - acc: 0.8972 - val_loss: 0.9498 - val_acc: 0.7277\n",
      "Epoch 79/500\n",
      "5834/5834 - 2s - loss: 0.2983 - acc: 0.8927 - val_loss: 0.8360 - val_acc: 0.7654\n",
      "Epoch 80/500\n",
      "5834/5834 - 2s - loss: 0.2819 - acc: 0.8984 - val_loss: 0.9664 - val_acc: 0.7277\n",
      "Epoch 81/500\n",
      "5834/5834 - 2s - loss: 0.2633 - acc: 0.9074 - val_loss: 1.0054 - val_acc: 0.7154\n",
      "Epoch 82/500\n",
      "5834/5834 - 2s - loss: 0.2722 - acc: 0.9030 - val_loss: 0.8886 - val_acc: 0.7572\n",
      "Epoch 83/500\n",
      "5834/5834 - 2s - loss: 0.2759 - acc: 0.9021 - val_loss: 1.2577 - val_acc: 0.6495\n",
      "Epoch 84/500\n",
      "5834/5834 - 2s - loss: 0.2697 - acc: 0.9016 - val_loss: 1.0086 - val_acc: 0.7263\n",
      "Epoch 85/500\n",
      "5834/5834 - 2s - loss: 0.2767 - acc: 0.9013 - val_loss: 0.8886 - val_acc: 0.7517\n",
      "Epoch 86/500\n",
      "5834/5834 - 2s - loss: 0.2626 - acc: 0.9037 - val_loss: 0.9648 - val_acc: 0.7318\n",
      "Epoch 87/500\n",
      "5834/5834 - 2s - loss: 0.2566 - acc: 0.9083 - val_loss: 0.9946 - val_acc: 0.7318\n",
      "Epoch 88/500\n",
      "5834/5834 - 2s - loss: 0.2717 - acc: 0.9021 - val_loss: 0.8941 - val_acc: 0.7517\n",
      "Epoch 89/500\n",
      "5834/5834 - 2s - loss: 0.2593 - acc: 0.9064 - val_loss: 1.2553 - val_acc: 0.6811\n",
      "Epoch 90/500\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "5834/5834 - 2s - loss: 0.2520 - acc: 0.9107 - val_loss: 0.8489 - val_acc: 0.7613\n",
      "Epoch 91/500\n",
      "5834/5834 - 2s - loss: 0.2311 - acc: 0.9179 - val_loss: 0.9219 - val_acc: 0.7524\n",
      "Epoch 92/500\n",
      "5834/5834 - 2s - loss: 0.2199 - acc: 0.9225 - val_loss: 0.8219 - val_acc: 0.7709\n",
      "Epoch 93/500\n",
      "5834/5834 - 2s - loss: 0.2259 - acc: 0.9176 - val_loss: 0.8415 - val_acc: 0.7709\n",
      "Epoch 94/500\n",
      "5834/5834 - 2s - loss: 0.2278 - acc: 0.9189 - val_loss: 0.8113 - val_acc: 0.7764\n",
      "Epoch 95/500\n",
      "5834/5834 - 2s - loss: 0.2142 - acc: 0.9249 - val_loss: 0.8225 - val_acc: 0.7771\n",
      "Epoch 96/500\n",
      "5834/5834 - 2s - loss: 0.2151 - acc: 0.9248 - val_loss: 0.8105 - val_acc: 0.7805\n",
      "Epoch 97/500\n",
      "5834/5834 - 2s - loss: 0.2112 - acc: 0.9232 - val_loss: 0.8238 - val_acc: 0.7709\n",
      "Epoch 98/500\n",
      "5834/5834 - 2s - loss: 0.2100 - acc: 0.9241 - val_loss: 0.8462 - val_acc: 0.7778\n",
      "Epoch 99/500\n",
      "5834/5834 - 2s - loss: 0.2095 - acc: 0.9277 - val_loss: 1.0017 - val_acc: 0.7421\n",
      "Epoch 100/500\n",
      "5834/5834 - 2s - loss: 0.1996 - acc: 0.9302 - val_loss: 0.9651 - val_acc: 0.7490\n",
      "Epoch 101/500\n",
      "5834/5834 - 2s - loss: 0.1932 - acc: 0.9366 - val_loss: 0.8330 - val_acc: 0.7826\n",
      "Epoch 102/500\n",
      "5834/5834 - 2s - loss: 0.2017 - acc: 0.9277 - val_loss: 0.8279 - val_acc: 0.7860\n",
      "Epoch 103/500\n",
      "5834/5834 - 2s - loss: 0.1923 - acc: 0.9290 - val_loss: 0.8955 - val_acc: 0.7675\n",
      "Epoch 104/500\n",
      "5834/5834 - 2s - loss: 0.2011 - acc: 0.9323 - val_loss: 0.8008 - val_acc: 0.7853\n",
      "Epoch 105/500\n",
      "5834/5834 - 2s - loss: 0.1903 - acc: 0.9318 - val_loss: 0.8018 - val_acc: 0.7846\n",
      "Epoch 106/500\n",
      "5834/5834 - 2s - loss: 0.1947 - acc: 0.9304 - val_loss: 0.9398 - val_acc: 0.7510\n",
      "Epoch 107/500\n",
      "5834/5834 - 2s - loss: 0.1892 - acc: 0.9318 - val_loss: 0.8202 - val_acc: 0.7771\n",
      "Epoch 108/500\n",
      "5834/5834 - 2s - loss: 0.1798 - acc: 0.9373 - val_loss: 0.8586 - val_acc: 0.7661\n",
      "Epoch 109/500\n",
      "5834/5834 - 2s - loss: 0.1855 - acc: 0.9357 - val_loss: 0.8679 - val_acc: 0.7798\n",
      "Epoch 110/500\n",
      "5834/5834 - 2s - loss: 0.1831 - acc: 0.9356 - val_loss: 0.8201 - val_acc: 0.7730\n",
      "Epoch 111/500\n",
      "5834/5834 - 2s - loss: 0.1838 - acc: 0.9347 - val_loss: 0.8641 - val_acc: 0.7757\n",
      "Epoch 112/500\n",
      "5834/5834 - 2s - loss: 0.1770 - acc: 0.9390 - val_loss: 0.8184 - val_acc: 0.7833\n",
      "Epoch 113/500\n",
      "\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "5834/5834 - 2s - loss: 0.1895 - acc: 0.9326 - val_loss: 0.8980 - val_acc: 0.7791\n",
      "Epoch 114/500\n",
      "5834/5834 - 2s - loss: 0.1659 - acc: 0.9403 - val_loss: 0.9254 - val_acc: 0.7661\n",
      "Epoch 115/500\n",
      "5834/5834 - 2s - loss: 0.1607 - acc: 0.9462 - val_loss: 0.8243 - val_acc: 0.7929\n",
      "Epoch 116/500\n",
      "5834/5834 - 2s - loss: 0.1647 - acc: 0.9453 - val_loss: 0.8118 - val_acc: 0.7936\n",
      "Epoch 117/500\n",
      "5834/5834 - 2s - loss: 0.1682 - acc: 0.9412 - val_loss: 0.8783 - val_acc: 0.7791\n",
      "Epoch 118/500\n",
      "5834/5834 - 2s - loss: 0.1655 - acc: 0.9431 - val_loss: 0.8377 - val_acc: 0.7860\n",
      "Epoch 119/500\n",
      "5834/5834 - 2s - loss: 0.1551 - acc: 0.9462 - val_loss: 0.8322 - val_acc: 0.7922\n",
      "Epoch 120/500\n",
      "5834/5834 - 2s - loss: 0.1562 - acc: 0.9453 - val_loss: 0.8984 - val_acc: 0.7702\n",
      "Epoch 121/500\n",
      "5834/5834 - 2s - loss: 0.1530 - acc: 0.9472 - val_loss: 0.8453 - val_acc: 0.7860\n",
      "Epoch 122/500\n",
      "5834/5834 - 2s - loss: 0.1628 - acc: 0.9417 - val_loss: 0.8153 - val_acc: 0.7860\n",
      "Epoch 123/500\n",
      "5834/5834 - 2s - loss: 0.1478 - acc: 0.9470 - val_loss: 0.8356 - val_acc: 0.7846\n",
      "Epoch 124/500\n",
      "5834/5834 - 2s - loss: 0.1596 - acc: 0.9450 - val_loss: 0.8576 - val_acc: 0.7840\n",
      "Epoch 125/500\n",
      "5834/5834 - 2s - loss: 0.1519 - acc: 0.9479 - val_loss: 0.8470 - val_acc: 0.7860\n",
      "Epoch 126/500\n",
      "5834/5834 - 2s - loss: 0.1568 - acc: 0.9467 - val_loss: 0.8395 - val_acc: 0.7881\n",
      "Epoch 127/500\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "5834/5834 - 2s - loss: 0.1535 - acc: 0.9479 - val_loss: 0.8415 - val_acc: 0.7888\n",
      "Epoch 128/500\n",
      "5834/5834 - 2s - loss: 0.1505 - acc: 0.9481 - val_loss: 0.8413 - val_acc: 0.7888\n",
      "Epoch 129/500\n",
      "5834/5834 - 2s - loss: 0.1396 - acc: 0.9506 - val_loss: 0.8810 - val_acc: 0.7778\n",
      "Epoch 130/500\n",
      "5834/5834 - 2s - loss: 0.1413 - acc: 0.9537 - val_loss: 0.8204 - val_acc: 0.7963\n",
      "Epoch 131/500\n",
      "5834/5834 - 2s - loss: 0.1416 - acc: 0.9520 - val_loss: 0.8235 - val_acc: 0.7901\n",
      "Epoch 132/500\n",
      "5834/5834 - 2s - loss: 0.1381 - acc: 0.9518 - val_loss: 0.8118 - val_acc: 0.7922\n",
      "Epoch 133/500\n",
      "5834/5834 - 2s - loss: 0.1377 - acc: 0.9559 - val_loss: 0.8218 - val_acc: 0.7963\n",
      "Epoch 134/500\n",
      "5834/5834 - 2s - loss: 0.1418 - acc: 0.9496 - val_loss: 0.8136 - val_acc: 0.7984\n",
      "Epoch 135/500\n",
      "5834/5834 - 2s - loss: 0.1349 - acc: 0.9549 - val_loss: 0.8252 - val_acc: 0.7970\n",
      "Epoch 136/500\n",
      "5834/5834 - 2s - loss: 0.1325 - acc: 0.9544 - val_loss: 0.8242 - val_acc: 0.7949\n",
      "Epoch 137/500\n",
      "5834/5834 - 2s - loss: 0.1340 - acc: 0.9551 - val_loss: 0.8082 - val_acc: 0.7956\n",
      "Epoch 138/500\n",
      "5834/5834 - 2s - loss: 0.1386 - acc: 0.9498 - val_loss: 0.8750 - val_acc: 0.7812\n",
      "Epoch 139/500\n",
      "5834/5834 - 2s - loss: 0.1340 - acc: 0.9571 - val_loss: 0.8265 - val_acc: 0.8038\n",
      "Epoch 140/500\n",
      "5834/5834 - 2s - loss: 0.1335 - acc: 0.9553 - val_loss: 0.8136 - val_acc: 0.8011\n",
      "Epoch 141/500\n",
      "5834/5834 - 2s - loss: 0.1246 - acc: 0.9561 - val_loss: 0.8385 - val_acc: 0.7908\n",
      "Epoch 142/500\n",
      "5834/5834 - 2s - loss: 0.1223 - acc: 0.9577 - val_loss: 0.8510 - val_acc: 0.7956\n",
      "Epoch 143/500\n",
      "5834/5834 - 2s - loss: 0.1312 - acc: 0.9583 - val_loss: 0.8197 - val_acc: 0.7901\n",
      "Epoch 144/500\n",
      "5834/5834 - 2s - loss: 0.1253 - acc: 0.9587 - val_loss: 0.8206 - val_acc: 0.7956\n",
      "Epoch 145/500\n",
      "5834/5834 - 2s - loss: 0.1281 - acc: 0.9558 - val_loss: 0.8769 - val_acc: 0.7778\n",
      "Epoch 146/500\n",
      "5834/5834 - 2s - loss: 0.1302 - acc: 0.9542 - val_loss: 0.8282 - val_acc: 0.8032\n",
      "Epoch 147/500\n",
      "5834/5834 - 2s - loss: 0.1360 - acc: 0.9525 - val_loss: 0.8539 - val_acc: 0.7929\n",
      "Epoch 148/500\n",
      "5834/5834 - 2s - loss: 0.1294 - acc: 0.9566 - val_loss: 0.9004 - val_acc: 0.7840\n",
      "Epoch 149/500\n",
      "5834/5834 - 2s - loss: 0.1316 - acc: 0.9551 - val_loss: 0.8377 - val_acc: 0.7929\n",
      "Epoch 150/500\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "5834/5834 - 2s - loss: 0.1344 - acc: 0.9571 - val_loss: 0.8392 - val_acc: 0.8032\n",
      "Epoch 151/500\n",
      "5834/5834 - 2s - loss: 0.1220 - acc: 0.9601 - val_loss: 0.8419 - val_acc: 0.7970\n",
      "Epoch 152/500\n",
      "5834/5834 - 2s - loss: 0.1285 - acc: 0.9553 - val_loss: 0.8331 - val_acc: 0.7997\n",
      "Epoch 153/500\n",
      "5834/5834 - 2s - loss: 0.1160 - acc: 0.9582 - val_loss: 0.8674 - val_acc: 0.7942\n",
      "Epoch 154/500\n",
      "5834/5834 - 2s - loss: 0.1151 - acc: 0.9628 - val_loss: 0.8671 - val_acc: 0.7922\n",
      "Epoch 155/500\n",
      "5834/5834 - 2s - loss: 0.1236 - acc: 0.9606 - val_loss: 0.8397 - val_acc: 0.7990\n",
      "Epoch 156/500\n",
      "5834/5834 - 2s - loss: 0.1185 - acc: 0.9585 - val_loss: 0.8882 - val_acc: 0.7915\n",
      "Epoch 157/500\n",
      "5834/5834 - 2s - loss: 0.1265 - acc: 0.9556 - val_loss: 0.8506 - val_acc: 0.7990\n",
      "Epoch 00157: early stopping\n",
      "============================================================train 5th fold============================================================\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 60, 22, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 60, 22, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 60, 22, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 60, 22, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 60, 22, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 30, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_4 (Average (None, 15, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 15, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 15, 5, 256)        295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 15, 5, 256)        1024      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 15, 5, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 15, 5, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 15, 5, 512)        2048      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 1,622,291\n",
      "Trainable params: 1,620,115\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "Train on 5834 samples, validate on 1458 samples\n",
      "Epoch 1/500\n",
      "5834/5834 - 4s - loss: 2.1941 - acc: 0.3091 - val_loss: 3.0495 - val_acc: 0.1193\n",
      "Epoch 2/500\n",
      "5834/5834 - 2s - loss: 1.8894 - acc: 0.3738 - val_loss: 2.7735 - val_acc: 0.1303\n",
      "Epoch 3/500\n",
      "5834/5834 - 2s - loss: 1.7317 - acc: 0.4121 - val_loss: 3.5417 - val_acc: 0.1475\n",
      "Epoch 4/500\n",
      "5834/5834 - 2s - loss: 1.6269 - acc: 0.4277 - val_loss: 2.9375 - val_acc: 0.2305\n",
      "Epoch 5/500\n",
      "5834/5834 - 2s - loss: 1.5570 - acc: 0.4558 - val_loss: 2.3156 - val_acc: 0.2764\n",
      "Epoch 6/500\n",
      "5834/5834 - 2s - loss: 1.4786 - acc: 0.4769 - val_loss: 2.2719 - val_acc: 0.3066\n",
      "Epoch 7/500\n",
      "5834/5834 - 2s - loss: 1.4321 - acc: 0.4921 - val_loss: 2.3627 - val_acc: 0.3114\n",
      "Epoch 8/500\n",
      "5834/5834 - 2s - loss: 1.3819 - acc: 0.5067 - val_loss: 1.6780 - val_acc: 0.4273\n",
      "Epoch 9/500\n",
      "5834/5834 - 2s - loss: 1.3493 - acc: 0.5221 - val_loss: 1.7772 - val_acc: 0.4362\n",
      "Epoch 10/500\n",
      "5834/5834 - 2s - loss: 1.3009 - acc: 0.5351 - val_loss: 1.5313 - val_acc: 0.4678\n",
      "Epoch 11/500\n",
      "5834/5834 - 2s - loss: 1.2642 - acc: 0.5465 - val_loss: 1.3197 - val_acc: 0.5267\n",
      "Epoch 12/500\n",
      "5834/5834 - 2s - loss: 1.2458 - acc: 0.5557 - val_loss: 1.3495 - val_acc: 0.5288\n",
      "Epoch 13/500\n",
      "5834/5834 - 2s - loss: 1.2184 - acc: 0.5597 - val_loss: 1.5623 - val_acc: 0.4925\n",
      "Epoch 14/500\n",
      "5834/5834 - 2s - loss: 1.1664 - acc: 0.5794 - val_loss: 1.7223 - val_acc: 0.4506\n",
      "Epoch 15/500\n",
      "5834/5834 - 2s - loss: 1.1629 - acc: 0.5819 - val_loss: 1.4748 - val_acc: 0.5082\n",
      "Epoch 16/500\n",
      "5834/5834 - 2s - loss: 1.1169 - acc: 0.5946 - val_loss: 1.3460 - val_acc: 0.5350\n",
      "Epoch 17/500\n",
      "5834/5834 - 2s - loss: 1.1117 - acc: 0.6047 - val_loss: 2.7607 - val_acc: 0.3059\n",
      "Epoch 18/500\n",
      "5834/5834 - 2s - loss: 1.0878 - acc: 0.6034 - val_loss: 1.7543 - val_acc: 0.4547\n",
      "Epoch 19/500\n",
      "5834/5834 - 2s - loss: 1.0631 - acc: 0.6169 - val_loss: 1.7711 - val_acc: 0.4609\n",
      "Epoch 20/500\n",
      "5834/5834 - 2s - loss: 1.0378 - acc: 0.6179 - val_loss: 1.3841 - val_acc: 0.5357\n",
      "Epoch 21/500\n",
      "5834/5834 - 2s - loss: 1.0057 - acc: 0.6394 - val_loss: 1.2457 - val_acc: 0.5617\n",
      "Epoch 22/500\n",
      "5834/5834 - 2s - loss: 0.9983 - acc: 0.6385 - val_loss: 1.2457 - val_acc: 0.5686\n",
      "Epoch 23/500\n",
      "5834/5834 - 2s - loss: 0.9614 - acc: 0.6539 - val_loss: 1.4347 - val_acc: 0.5014\n",
      "Epoch 24/500\n",
      "5834/5834 - 2s - loss: 0.9524 - acc: 0.6606 - val_loss: 1.9650 - val_acc: 0.4945\n",
      "Epoch 25/500\n",
      "5834/5834 - 2s - loss: 0.9319 - acc: 0.6616 - val_loss: 1.2792 - val_acc: 0.5700\n",
      "Epoch 26/500\n",
      "5834/5834 - 2s - loss: 0.9129 - acc: 0.6661 - val_loss: 1.3975 - val_acc: 0.5494\n",
      "Epoch 27/500\n",
      "5834/5834 - 2s - loss: 0.9066 - acc: 0.6719 - val_loss: 1.0219 - val_acc: 0.6344\n",
      "Epoch 28/500\n",
      "5834/5834 - 2s - loss: 0.8900 - acc: 0.6810 - val_loss: 1.3040 - val_acc: 0.5679\n",
      "Epoch 29/500\n",
      "5834/5834 - 2s - loss: 0.8561 - acc: 0.6891 - val_loss: 1.1906 - val_acc: 0.6008\n",
      "Epoch 30/500\n",
      "5834/5834 - 2s - loss: 0.8521 - acc: 0.6915 - val_loss: 1.3393 - val_acc: 0.5775\n",
      "Epoch 31/500\n",
      "5834/5834 - 2s - loss: 0.8488 - acc: 0.6952 - val_loss: 1.3641 - val_acc: 0.5508\n",
      "Epoch 32/500\n",
      "5834/5834 - 2s - loss: 0.8084 - acc: 0.7098 - val_loss: 1.2660 - val_acc: 0.5686\n",
      "Epoch 33/500\n",
      "5834/5834 - 2s - loss: 0.8168 - acc: 0.7019 - val_loss: 1.4936 - val_acc: 0.5556\n",
      "Epoch 34/500\n",
      "5834/5834 - 2s - loss: 0.7969 - acc: 0.7153 - val_loss: 1.2517 - val_acc: 0.5857\n",
      "Epoch 35/500\n",
      "5834/5834 - 2s - loss: 0.7777 - acc: 0.7230 - val_loss: 1.3733 - val_acc: 0.5857\n",
      "Epoch 36/500\n",
      "5834/5834 - 2s - loss: 0.7664 - acc: 0.7220 - val_loss: 1.1233 - val_acc: 0.6344\n",
      "Epoch 37/500\n",
      "5834/5834 - 2s - loss: 0.7680 - acc: 0.7252 - val_loss: 1.3193 - val_acc: 0.5892\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "5834/5834 - 2s - loss: 0.7397 - acc: 0.7283 - val_loss: 1.4163 - val_acc: 0.5535\n",
      "Epoch 39/500\n",
      "5834/5834 - 2s - loss: 0.6858 - acc: 0.7516 - val_loss: 0.8561 - val_acc: 0.7106\n",
      "Epoch 40/500\n",
      "5834/5834 - 2s - loss: 0.6585 - acc: 0.7611 - val_loss: 0.8206 - val_acc: 0.7133\n",
      "Epoch 41/500\n",
      "5834/5834 - 2s - loss: 0.6513 - acc: 0.7616 - val_loss: 1.2265 - val_acc: 0.6008\n",
      "Epoch 42/500\n",
      "5834/5834 - 2s - loss: 0.6244 - acc: 0.7789 - val_loss: 0.8801 - val_acc: 0.6996\n",
      "Epoch 43/500\n",
      "5834/5834 - 2s - loss: 0.6230 - acc: 0.7775 - val_loss: 0.9923 - val_acc: 0.6646\n",
      "Epoch 44/500\n",
      "5834/5834 - 2s - loss: 0.6011 - acc: 0.7857 - val_loss: 1.0572 - val_acc: 0.6365\n",
      "Epoch 45/500\n",
      "5834/5834 - 2s - loss: 0.5915 - acc: 0.7917 - val_loss: 0.8221 - val_acc: 0.6996\n",
      "Epoch 46/500\n",
      "5834/5834 - 2s - loss: 0.5787 - acc: 0.7921 - val_loss: 0.8350 - val_acc: 0.7044\n",
      "Epoch 47/500\n",
      "5834/5834 - 2s - loss: 0.5699 - acc: 0.7936 - val_loss: 0.8472 - val_acc: 0.7202\n",
      "Epoch 48/500\n",
      "5834/5834 - 2s - loss: 0.5587 - acc: 0.7993 - val_loss: 0.9760 - val_acc: 0.6824\n",
      "Epoch 49/500\n",
      "5834/5834 - 2s - loss: 0.5535 - acc: 0.7950 - val_loss: 1.2955 - val_acc: 0.6036\n",
      "Epoch 50/500\n",
      "5834/5834 - 2s - loss: 0.5375 - acc: 0.8068 - val_loss: 1.1914 - val_acc: 0.6214\n",
      "Epoch 51/500\n",
      "5834/5834 - 2s - loss: 0.5392 - acc: 0.8125 - val_loss: 0.9288 - val_acc: 0.6927\n",
      "Epoch 52/500\n",
      "5834/5834 - 2s - loss: 0.5310 - acc: 0.8079 - val_loss: 0.8672 - val_acc: 0.7140\n",
      "Epoch 53/500\n",
      "5834/5834 - 2s - loss: 0.5240 - acc: 0.8115 - val_loss: 0.9673 - val_acc: 0.6818\n",
      "Epoch 54/500\n",
      "5834/5834 - 2s - loss: 0.5255 - acc: 0.8082 - val_loss: 1.0653 - val_acc: 0.6626\n",
      "Epoch 55/500\n",
      "5834/5834 - 2s - loss: 0.5059 - acc: 0.8152 - val_loss: 0.7838 - val_acc: 0.7428\n",
      "Epoch 56/500\n",
      "5834/5834 - 2s - loss: 0.4936 - acc: 0.8228 - val_loss: 1.6351 - val_acc: 0.5713\n",
      "Epoch 57/500\n",
      "5834/5834 - 2s - loss: 0.5019 - acc: 0.8190 - val_loss: 0.9897 - val_acc: 0.6941\n",
      "Epoch 58/500\n",
      "5834/5834 - 2s - loss: 0.4802 - acc: 0.8224 - val_loss: 1.1215 - val_acc: 0.6619\n",
      "Epoch 59/500\n",
      "5834/5834 - 2s - loss: 0.4749 - acc: 0.8322 - val_loss: 0.9178 - val_acc: 0.7078\n",
      "Epoch 60/500\n",
      "5834/5834 - 2s - loss: 0.4750 - acc: 0.8313 - val_loss: 0.9149 - val_acc: 0.7209\n",
      "Epoch 61/500\n",
      "5834/5834 - 2s - loss: 0.4518 - acc: 0.8394 - val_loss: 0.9391 - val_acc: 0.6920\n",
      "Epoch 62/500\n",
      "5834/5834 - 2s - loss: 0.4520 - acc: 0.8324 - val_loss: 0.8849 - val_acc: 0.7236\n",
      "Epoch 63/500\n",
      "5834/5834 - 2s - loss: 0.4559 - acc: 0.8370 - val_loss: 1.1310 - val_acc: 0.6639\n",
      "Epoch 64/500\n",
      "5834/5834 - 2s - loss: 0.4561 - acc: 0.8329 - val_loss: 0.8748 - val_acc: 0.7174\n",
      "Epoch 65/500\n",
      "5834/5834 - 2s - loss: 0.4351 - acc: 0.8408 - val_loss: 0.8514 - val_acc: 0.7243\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "5834/5834 - 2s - loss: 0.4300 - acc: 0.8469 - val_loss: 0.9122 - val_acc: 0.7106\n",
      "Epoch 67/500\n",
      "5834/5834 - 2s - loss: 0.3899 - acc: 0.8588 - val_loss: 0.9381 - val_acc: 0.7078\n",
      "Epoch 68/500\n",
      "5834/5834 - 2s - loss: 0.3856 - acc: 0.8605 - val_loss: 0.7278 - val_acc: 0.7531\n",
      "Epoch 69/500\n",
      "5834/5834 - 2s - loss: 0.3758 - acc: 0.8649 - val_loss: 0.7892 - val_acc: 0.7613\n",
      "Epoch 70/500\n",
      "5834/5834 - 2s - loss: 0.3604 - acc: 0.8706 - val_loss: 0.8690 - val_acc: 0.7291\n",
      "Epoch 71/500\n",
      "5834/5834 - 2s - loss: 0.3629 - acc: 0.8720 - val_loss: 0.7966 - val_acc: 0.7346\n",
      "Epoch 72/500\n",
      "5834/5834 - 2s - loss: 0.3553 - acc: 0.8749 - val_loss: 1.2072 - val_acc: 0.6756\n",
      "Epoch 73/500\n",
      "5834/5834 - 2s - loss: 0.3439 - acc: 0.8726 - val_loss: 0.7364 - val_acc: 0.7579\n",
      "Epoch 74/500\n",
      "5834/5834 - 2s - loss: 0.3391 - acc: 0.8809 - val_loss: 0.8436 - val_acc: 0.7428\n",
      "Epoch 75/500\n",
      "5834/5834 - 2s - loss: 0.3273 - acc: 0.8855 - val_loss: 0.8875 - val_acc: 0.7318\n",
      "Epoch 76/500\n",
      "5834/5834 - 2s - loss: 0.3271 - acc: 0.8855 - val_loss: 0.9472 - val_acc: 0.7181\n",
      "Epoch 77/500\n",
      "5834/5834 - 2s - loss: 0.3248 - acc: 0.8848 - val_loss: 0.8077 - val_acc: 0.7647\n",
      "Epoch 78/500\n",
      "5834/5834 - 2s - loss: 0.3208 - acc: 0.8869 - val_loss: 0.9415 - val_acc: 0.7277\n",
      "Epoch 79/500\n",
      "5834/5834 - 2s - loss: 0.3196 - acc: 0.8874 - val_loss: 0.9944 - val_acc: 0.7071\n",
      "Epoch 80/500\n",
      "5834/5834 - 2s - loss: 0.3261 - acc: 0.8790 - val_loss: 0.9384 - val_acc: 0.7270\n",
      "Epoch 81/500\n",
      "5834/5834 - 2s - loss: 0.3173 - acc: 0.8867 - val_loss: 1.0569 - val_acc: 0.7023\n",
      "Epoch 82/500\n",
      "5834/5834 - 2s - loss: 0.3033 - acc: 0.8936 - val_loss: 0.7763 - val_acc: 0.7716\n",
      "Epoch 83/500\n",
      "5834/5834 - 2s - loss: 0.3081 - acc: 0.8857 - val_loss: 0.8523 - val_acc: 0.7394\n",
      "Epoch 84/500\n",
      "5834/5834 - 2s - loss: 0.3123 - acc: 0.8862 - val_loss: 0.8250 - val_acc: 0.7401\n",
      "Epoch 85/500\n",
      "5834/5834 - 2s - loss: 0.3043 - acc: 0.8877 - val_loss: 1.2789 - val_acc: 0.6811\n",
      "Epoch 86/500\n",
      "5834/5834 - 2s - loss: 0.2987 - acc: 0.8894 - val_loss: 0.9223 - val_acc: 0.7346\n",
      "Epoch 87/500\n",
      "5834/5834 - 2s - loss: 0.2868 - acc: 0.9004 - val_loss: 0.8194 - val_acc: 0.7558\n",
      "Epoch 88/500\n",
      "5834/5834 - 2s - loss: 0.2862 - acc: 0.8992 - val_loss: 0.7506 - val_acc: 0.7757\n",
      "Epoch 89/500\n",
      "5834/5834 - 2s - loss: 0.2870 - acc: 0.8958 - val_loss: 0.9248 - val_acc: 0.7236\n",
      "Epoch 90/500\n",
      "5834/5834 - 2s - loss: 0.2921 - acc: 0.8958 - val_loss: 0.7888 - val_acc: 0.7627\n",
      "Epoch 91/500\n",
      "5834/5834 - 2s - loss: 0.2822 - acc: 0.8984 - val_loss: 1.5862 - val_acc: 0.6385\n",
      "Epoch 92/500\n",
      "5834/5834 - 2s - loss: 0.2718 - acc: 0.9025 - val_loss: 1.1005 - val_acc: 0.7154\n",
      "Epoch 93/500\n",
      "5834/5834 - 2s - loss: 0.2672 - acc: 0.9056 - val_loss: 0.7853 - val_acc: 0.7620\n",
      "Epoch 94/500\n",
      "5834/5834 - 2s - loss: 0.2802 - acc: 0.8972 - val_loss: 0.9405 - val_acc: 0.7325\n",
      "Epoch 95/500\n",
      "5834/5834 - 2s - loss: 0.2625 - acc: 0.9056 - val_loss: 0.8501 - val_acc: 0.7627\n",
      "Epoch 96/500\n",
      "5834/5834 - 2s - loss: 0.2574 - acc: 0.9074 - val_loss: 0.9331 - val_acc: 0.7394\n",
      "Epoch 97/500\n",
      "5834/5834 - 2s - loss: 0.2723 - acc: 0.9047 - val_loss: 0.8181 - val_acc: 0.7586\n",
      "Epoch 98/500\n",
      "5834/5834 - 2s - loss: 0.2690 - acc: 0.9069 - val_loss: 0.8847 - val_acc: 0.7497\n",
      "Epoch 99/500\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "5834/5834 - 2s - loss: 0.2490 - acc: 0.9133 - val_loss: 0.8619 - val_acc: 0.7414\n",
      "Epoch 100/500\n",
      "5834/5834 - 2s - loss: 0.2290 - acc: 0.9215 - val_loss: 0.7805 - val_acc: 0.7812\n",
      "Epoch 101/500\n",
      "5834/5834 - 2s - loss: 0.2220 - acc: 0.9206 - val_loss: 0.8971 - val_acc: 0.7455\n",
      "Epoch 102/500\n",
      "5834/5834 - 2s - loss: 0.2095 - acc: 0.9256 - val_loss: 0.7886 - val_acc: 0.7771\n",
      "Epoch 103/500\n",
      "5834/5834 - 2s - loss: 0.2090 - acc: 0.9249 - val_loss: 0.7986 - val_acc: 0.7791\n",
      "Epoch 104/500\n",
      "5834/5834 - 2s - loss: 0.2170 - acc: 0.9241 - val_loss: 0.8138 - val_acc: 0.7730\n",
      "Epoch 105/500\n",
      "5834/5834 - 2s - loss: 0.2156 - acc: 0.9232 - val_loss: 0.8226 - val_acc: 0.7771\n",
      "Epoch 106/500\n",
      "5834/5834 - 2s - loss: 0.2121 - acc: 0.9275 - val_loss: 0.8634 - val_acc: 0.7524\n",
      "Epoch 107/500\n",
      "5834/5834 - 2s - loss: 0.2163 - acc: 0.9218 - val_loss: 0.7413 - val_acc: 0.7853\n",
      "Epoch 108/500\n",
      "5834/5834 - 2s - loss: 0.2100 - acc: 0.9272 - val_loss: 0.7415 - val_acc: 0.7853\n",
      "Epoch 109/500\n",
      "5834/5834 - 2s - loss: 0.2080 - acc: 0.9290 - val_loss: 0.8744 - val_acc: 0.7627\n",
      "Epoch 110/500\n",
      "5834/5834 - 2s - loss: 0.1993 - acc: 0.9294 - val_loss: 0.9306 - val_acc: 0.7442\n",
      "Epoch 111/500\n",
      "5834/5834 - 2s - loss: 0.1977 - acc: 0.9301 - val_loss: 0.8801 - val_acc: 0.7572\n",
      "Epoch 112/500\n",
      "5834/5834 - 2s - loss: 0.1955 - acc: 0.9294 - val_loss: 0.8544 - val_acc: 0.7695\n",
      "Epoch 113/500\n",
      "5834/5834 - 2s - loss: 0.1973 - acc: 0.9308 - val_loss: 0.7808 - val_acc: 0.7833\n",
      "Epoch 114/500\n",
      "5834/5834 - 2s - loss: 0.1897 - acc: 0.9349 - val_loss: 0.7441 - val_acc: 0.7805\n",
      "Epoch 115/500\n",
      "5834/5834 - 2s - loss: 0.1934 - acc: 0.9332 - val_loss: 1.4277 - val_acc: 0.6680\n",
      "Epoch 116/500\n",
      "5834/5834 - 2s - loss: 0.1980 - acc: 0.9325 - val_loss: 0.8185 - val_acc: 0.7689\n",
      "Epoch 117/500\n",
      "5834/5834 - 2s - loss: 0.1930 - acc: 0.9330 - val_loss: 1.0088 - val_acc: 0.7346\n",
      "Epoch 118/500\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "5834/5834 - 2s - loss: 0.1928 - acc: 0.9304 - val_loss: 0.9266 - val_acc: 0.7476\n",
      "Epoch 119/500\n",
      "5834/5834 - 2s - loss: 0.1680 - acc: 0.9410 - val_loss: 0.7488 - val_acc: 0.7888\n",
      "Epoch 120/500\n",
      "5834/5834 - 2s - loss: 0.1693 - acc: 0.9419 - val_loss: 0.7076 - val_acc: 0.7888\n",
      "Epoch 121/500\n",
      "5834/5834 - 2s - loss: 0.1625 - acc: 0.9427 - val_loss: 0.7523 - val_acc: 0.7874\n",
      "Epoch 122/500\n",
      "5834/5834 - 2s - loss: 0.1684 - acc: 0.9443 - val_loss: 0.7328 - val_acc: 0.8011\n",
      "Epoch 123/500\n",
      "5834/5834 - 2s - loss: 0.1538 - acc: 0.9434 - val_loss: 0.7430 - val_acc: 0.7908\n",
      "Epoch 124/500\n",
      "5834/5834 - 2s - loss: 0.1648 - acc: 0.9393 - val_loss: 0.7407 - val_acc: 0.7936\n",
      "Epoch 125/500\n",
      "5834/5834 - 2s - loss: 0.1598 - acc: 0.9458 - val_loss: 0.7754 - val_acc: 0.7840\n",
      "Epoch 126/500\n",
      "5834/5834 - 2s - loss: 0.1654 - acc: 0.9405 - val_loss: 0.7574 - val_acc: 0.7894\n",
      "Epoch 127/500\n",
      "5834/5834 - 2s - loss: 0.1746 - acc: 0.9421 - val_loss: 0.8738 - val_acc: 0.7606\n",
      "Epoch 128/500\n",
      "5834/5834 - 2s - loss: 0.1685 - acc: 0.9414 - val_loss: 0.7853 - val_acc: 0.7867\n",
      "Epoch 129/500\n",
      "5834/5834 - 2s - loss: 0.1681 - acc: 0.9417 - val_loss: 0.7433 - val_acc: 0.8045\n",
      "Epoch 130/500\n",
      "5834/5834 - 2s - loss: 0.1564 - acc: 0.9479 - val_loss: 0.7635 - val_acc: 0.7949\n",
      "Epoch 131/500\n",
      "5834/5834 - 2s - loss: 0.1562 - acc: 0.9453 - val_loss: 0.7948 - val_acc: 0.7874\n",
      "Epoch 132/500\n",
      "5834/5834 - 2s - loss: 0.1637 - acc: 0.9448 - val_loss: 0.7300 - val_acc: 0.8018\n",
      "Epoch 133/500\n",
      "5834/5834 - 2s - loss: 0.1495 - acc: 0.9448 - val_loss: 0.7724 - val_acc: 0.7853\n",
      "Epoch 134/500\n",
      "5834/5834 - 2s - loss: 0.1533 - acc: 0.9477 - val_loss: 0.7898 - val_acc: 0.7846\n",
      "Epoch 135/500\n",
      "5834/5834 - 2s - loss: 0.1498 - acc: 0.9465 - val_loss: 0.7502 - val_acc: 0.7915\n",
      "Epoch 136/500\n",
      "5834/5834 - 2s - loss: 0.1561 - acc: 0.9445 - val_loss: 0.7679 - val_acc: 0.7888\n",
      "Epoch 137/500\n",
      "5834/5834 - 2s - loss: 0.1527 - acc: 0.9429 - val_loss: 0.7489 - val_acc: 0.7942\n",
      "Epoch 138/500\n",
      "5834/5834 - 2s - loss: 0.1486 - acc: 0.9458 - val_loss: 0.8142 - val_acc: 0.7833\n",
      "Epoch 139/500\n",
      "5834/5834 - 2s - loss: 0.1426 - acc: 0.9496 - val_loss: 0.7374 - val_acc: 0.8025\n",
      "Epoch 140/500\n",
      "\n",
      "Epoch 00140: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "5834/5834 - 2s - loss: 0.1396 - acc: 0.9518 - val_loss: 0.7744 - val_acc: 0.7970\n",
      "Epoch 141/500\n",
      "5834/5834 - 2s - loss: 0.1384 - acc: 0.9511 - val_loss: 0.7335 - val_acc: 0.8059\n",
      "Epoch 142/500\n",
      "5834/5834 - 2s - loss: 0.1371 - acc: 0.9518 - val_loss: 0.7439 - val_acc: 0.8018\n",
      "Epoch 143/500\n",
      "5834/5834 - 2s - loss: 0.1292 - acc: 0.9554 - val_loss: 0.7741 - val_acc: 0.7922\n",
      "Epoch 144/500\n",
      "5834/5834 - 2s - loss: 0.1285 - acc: 0.9539 - val_loss: 0.7540 - val_acc: 0.7990\n",
      "Epoch 145/500\n",
      "5834/5834 - 2s - loss: 0.1316 - acc: 0.9542 - val_loss: 0.7288 - val_acc: 0.8004\n",
      "Epoch 146/500\n",
      "5834/5834 - 2s - loss: 0.1358 - acc: 0.9518 - val_loss: 0.7498 - val_acc: 0.7963\n",
      "Epoch 147/500\n",
      "5834/5834 - 2s - loss: 0.1286 - acc: 0.9582 - val_loss: 0.8284 - val_acc: 0.7867\n",
      "Epoch 148/500\n",
      "5834/5834 - 2s - loss: 0.1365 - acc: 0.9534 - val_loss: 0.7505 - val_acc: 0.8032\n",
      "Epoch 149/500\n",
      "5834/5834 - 2s - loss: 0.1341 - acc: 0.9566 - val_loss: 0.7580 - val_acc: 0.7977\n",
      "Epoch 150/500\n",
      "5834/5834 - 2s - loss: 0.1297 - acc: 0.9558 - val_loss: 0.7532 - val_acc: 0.8025\n",
      "Epoch 151/500\n",
      "5834/5834 - 2s - loss: 0.1301 - acc: 0.9566 - val_loss: 0.7407 - val_acc: 0.8025\n",
      "Epoch 152/500\n",
      "\n",
      "Epoch 00152: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "5834/5834 - 2s - loss: 0.1282 - acc: 0.9558 - val_loss: 0.7394 - val_acc: 0.8004\n",
      "Epoch 153/500\n",
      "5834/5834 - 2s - loss: 0.1215 - acc: 0.9577 - val_loss: 0.7477 - val_acc: 0.8052\n",
      "Epoch 154/500\n",
      "5834/5834 - 2s - loss: 0.1297 - acc: 0.9534 - val_loss: 0.7992 - val_acc: 0.7929\n",
      "Epoch 155/500\n",
      "5834/5834 - 2s - loss: 0.1115 - acc: 0.9652 - val_loss: 0.7699 - val_acc: 0.7970\n",
      "Epoch 156/500\n",
      "5834/5834 - 2s - loss: 0.1197 - acc: 0.9625 - val_loss: 0.8225 - val_acc: 0.7942\n",
      "Epoch 157/500\n",
      "5834/5834 - 2s - loss: 0.1229 - acc: 0.9580 - val_loss: 0.7611 - val_acc: 0.7984\n",
      "Epoch 158/500\n",
      "5834/5834 - 2s - loss: 0.1229 - acc: 0.9554 - val_loss: 0.7720 - val_acc: 0.8038\n",
      "Epoch 159/500\n",
      "5834/5834 - 2s - loss: 0.1104 - acc: 0.9631 - val_loss: 0.7424 - val_acc: 0.8011\n",
      "Epoch 00159: early stopping\n"
     ]
    }
   ],
   "source": [
    "proba_x = np.zeros((7292, 19))\n",
    "proba_t = np.zeros((7500, 19))\n",
    "for fold, (xx, yy) in enumerate(kfold.split(x, y)):\n",
    "    print(\"{}train {}th fold{}\".format('==' * 30, fold + 1, '==' * 30))\n",
    "    y_ = to_categorical(y, num_classes=19)\n",
    "    model = Net()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "#                   optimizer = Adam(),\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                verbose=1,\n",
    "                                mode='max',\n",
    "                                factor=0.6,\n",
    "                                patience=11)\n",
    "    early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                                   verbose=1,\n",
    "                                   mode='max',\n",
    "                                   patience=18)\n",
    "    checkpoint = ModelCheckpoint(f'fold{fold}.h5',\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=0,\n",
    "                                 mode='max',\n",
    "                                 save_best_only=True)\n",
    "\n",
    "#     csv_logger = CSVLogger('log.csv', separator=',', append=True)\n",
    "    model.fit(x[xx], y_[xx],\n",
    "              epochs=500,\n",
    "              batch_size=128,\n",
    "              verbose=2,\n",
    "              shuffle=True,\n",
    "              validation_data=(x[yy], y_[yy]),\n",
    "              callbacks=[plateau, early_stopping, checkpoint])\n",
    "    model.load_weights(f'fold{fold}.h5')\n",
    "    proba_x[yy] += model.predict(x[yy], verbose=0, batch_size=128)\n",
    "    proba_t += model.predict(t, verbose=0, batch_size=128) / 5.    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score  0.80321 accuracy_score  0.83238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "oof_y = np.argmax(proba_x, axis=1)\n",
    "score1 = round(accuracy_score(y, oof_y), 5)\n",
    "# print('accuracy_score ', score1)\n",
    "\n",
    "score = round(sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y, oof_y)) / oof_y.shape[0], 5)\n",
    "print('accuracy_score ', score1, 'accuracy_score ', score)\n",
    "\n",
    "# ]accuracy_score  0.78799 accuracy_score  0.81902    up  757\n",
    "# accuracy_score  0.79361 accuracy_score  0.82357     up  764  \n",
    "# accuracy_score  0.80156 accuracy_score  0.83124\n",
    "# accuracy_score  79923\n",
    "\n",
    "# accuracy_score  0.80801 accuracy_score  0.83674\n",
    "# accuracy_score  0.80965 accuracy_score  0.83811\n",
    "\n",
    "# accuracy_score  0.80911 accuracy_score  0.8378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(proba_x, columns = ['pred_{}'.format(i) for i in range(19)]).to_csv(data_path + 'sub/proba_x_forward_{}.csv'.format(score1), index = False)\n",
    "pd.DataFrame(proba_t, columns = ['pred_{}'.format(i) for i in range(19)]).to_csv(data_path + 'sub/proba_t_forward_{}.csv'.format(score1), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(proba_x, columns = ['pred_{}'.format(i) for i in range(19)]).to_csv(data_path + 'sub/proba_x_backward_{}.csv'.format(score1), index = False)\n",
    "# pd.DataFrame(proba_t, columns = ['pred_{}'.format(i) for i in range(19)]).to_csv(data_path + 'sub/proba_t_backward_{}.csv'.format(score1), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.behavior_id = np.argmax(proba_t, axis=1)\n",
    "sub.to_csv(data_path + 'sub/submit_{}.csv'.format(score1), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 前向和后向融合\n",
    "# proba_x_f = pd.read_csv(data_path + 'sub/proba_x_forward_0.80801.csv')\n",
    "# proba_t_f = pd.read_csv(data_path + 'sub/proba_t_forward_0.80801.csv')\n",
    "\n",
    "# proba_x_b = pd.read_csv(data_path + 'sub/proba_x_backward_0.80965.csv')\n",
    "# proba_t_b = pd.read_csv(data_path + 'sub/proba_t_backward_0.80965.csv')\n",
    "\n",
    "# proba_x_f, proba_x_b, proba_t_f, proba_t_b = np.array(proba_x_f),np.array(proba_x_b),np.array(proba_t_f),np.array(proba_t_b)\n",
    "\n",
    "# proba_x = (proba_x_f + proba_x_b) / 2\n",
    "# proba_t = (proba_t_f + proba_t_b) / 2\n",
    "\n",
    "\n",
    "# oof_y = np.argmax(proba_x, axis=1)\n",
    "# score1 = round(accuracy_score(y, oof_y), 5)\n",
    "# # print('accuracy_score ', score1)\n",
    "\n",
    "# score = round(sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y, oof_y)) / oof_y.shape[0], 5)\n",
    "# print('accuracy_score ', score1, 'accuracy_score ', score)\n",
    "\n",
    "# sub.behavior_id = np.argmax(proba_t, axis=1)\n",
    "# sub.to_csv(data_path + 'sub/submit_{}.csv'.format(score1), index=False)\n",
    "\n",
    "\n",
    "# # accuracy_score  0.80787 accuracy_score  0.83623  up 772\n",
    "# # accuracy_score  0.81418 accuracy_score  0.84175\n",
    "# # accuracy_score  0.82021 accuracy_score  0.84703  up 776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_py3",
   "language": "python",
   "name": "conda_tensorflow2_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
