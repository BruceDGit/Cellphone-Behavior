{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "from scipy.signal import welch\n",
    "from scipy.interpolate import  interp1d as interp\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "class Preprocessing(object):\n",
    "    \"\"\"\n",
    "    对序列进行提取特征等预处理\n",
    "    为了可能处理变长序列,因此输入为list\n",
    "    \"\"\"\n",
    "    def __init__(self,with_label=True):\n",
    "        self.with_label=with_label\n",
    "    def __call__(self,sequence):\n",
    "        for i in range(len(sequence)):\n",
    "            time_point=np.expand_dims(sequence[i][0],axis=0)\n",
    "            if self.with_label:\n",
    "                label=np.expand_dims(sequence[i][-1],axis=0)\n",
    "                new_sequence=self.for_each(sequence[i][1:-1])\n",
    "                sequence[i]=np.concatenate([time_point,new_sequence,label],axis=0)\n",
    "            else:\n",
    "                new_sequence=self.for_each(sequence[i][1:])\n",
    "                sequence[i]=np.concatenate([time_point,new_sequence],axis=0)\n",
    "        sequence=self.for_all(sequence)\n",
    "        return sequence\n",
    "    def smooth(self,array,decay_rate=0.9):\n",
    "        _smooth=np.zeros(array.shape)\n",
    "        for i in range(1,len(array)-1):\n",
    "            decay=min(decay_rate,(i+1)/(i+10))\n",
    "            _smooth[i]=_smooth[i-1]*decay+(1-decay)*array[i]\n",
    "        return _smooth\n",
    "    def for_all(self,sequence):#变长序列可以numpy吗#\n",
    "        return sequence\n",
    "        mean=np.zeros(shape=sequence[0].shape[0])\n",
    "        std=np.zeros(shape=sequence[0].shape[0])\n",
    "        lenth=len(sequence)\n",
    "        for index in range(lenth):\n",
    "            if self.with_label:\n",
    "                _range=range(1,sequence[index].shape[0]-1)\n",
    "            else:\n",
    "                _range=range(1,sequence[index].shape[0])\n",
    "            for i in _range:\n",
    "                mean[i]+=sequence[index][i].mean()/lenth\n",
    "                std[i]+=sequence[index][i].std()/lenth\n",
    "        for index in range(lenth):\n",
    "            if self.with_label:\n",
    "                _range=range(1,sequence[index].shape[0]-1)\n",
    "            else:\n",
    "                _range=range(1,sequence[index].shape[0])\n",
    "            for i in _range:\n",
    "                sequence[index][i]=(sequence[index][i]-mean[i])/std[i]\n",
    "        return sequence\n",
    "    def for_each(self,sequence):\n",
    "        acc=(sequence[0]**2+sequence[1]**2+sequence[2]**2)**0.5\n",
    "        acc=np.expand_dims(acc,axis=0)\n",
    "        acc_g=(sequence[3]**2+sequence[4]**2+sequence[5]**2)**0.5\n",
    "        acc_g=np.expand_dims(acc_g,axis=0)\n",
    "        sequence=np.concatenate([sequence,acc,acc_g],axis=0)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2471ce3401ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "#加载数据\n",
    "class DatasetLoader(object):\n",
    "    def __init__(self,csv_file,with_label=True,num_classes=19):\n",
    "        self.csv_file=csv_file\n",
    "        self.with_label=with_label\n",
    "        self.format=\"channel_last\"\n",
    "        self.split=False\n",
    "        self.names=self.get_feature_names()\n",
    "        self.num_classes=num_classes\n",
    "        self.data_split=False\n",
    "    def get_feature_names(self):\n",
    "         with open(self.csv_file) as f:\n",
    "            examples={}\n",
    "            names=f.readline().split(',')[1:]\n",
    "            names[-1]=names[-1][:-1]\n",
    "            return names\n",
    "    def make_numpy(self,num_interpolation=200,with_label=True):\n",
    "        '''将数据读取并保存为Numpy数组\n",
    "               Args:\n",
    "                 num_interpolation:差值法采样点个数\n",
    "                 with_label：是否带标签\n",
    "               Returns:\n",
    "                 A list,shape=[num_examples,keys,length]\n",
    "        '''\n",
    "        #数据读取\n",
    "        if self.csv_file is None:\n",
    "            raise ValueError(\"sub dataset cannnot get numpy data\")\n",
    "        print(\"Loading date...\")\n",
    "        line={}\n",
    "        with open(self.csv_file) as f:\n",
    "            examples={}\n",
    "            names=f.readline().split(',')[1:]\n",
    "            names[-1]=names[-1][:-1]\n",
    "            while True:\n",
    "                try:\n",
    "                    line=f.readline().split(\",\")\n",
    "                    if line is None:\n",
    "                        break\n",
    "                    for i in range(len(line)):\n",
    "                        line[i]=eval(line[i])\n",
    "                    if not line[0] in examples:\n",
    "                        examples[line[0]]=[]\n",
    "                    examples[line[0]].append(line[1:])\n",
    "                except:\n",
    "                    break\n",
    "        print(\"done\")\n",
    "        #格式转换            \n",
    "        for i in range(len(examples)):\n",
    "            examples[i]=np.array(examples[i]).transpose([1,0])\n",
    "        self.examples=examples=list(examples.values())\n",
    "        return self\n",
    "    def resample(self,num_interpolation=200):\n",
    "        examples=self.examples\n",
    "        print(\"interpolate\")\n",
    "        bar=Progbar(len(examples))#进度条\n",
    "        if num_interpolation and num_interpolation is not None:\n",
    "            for i in range(len(examples)):\n",
    "                range_len=examples[i][0][-1]-examples[i][0][0]\n",
    "                range_start=examples[i][0][0]\n",
    "                range_interval=range_len/num_interpolation\n",
    "                interp_x=[range_start+range_interval*i for i in range(num_interpolation)]\n",
    "                interp_data=[interp_x]\n",
    "                for feature_id in range(1,len(self.names)):\n",
    "                    try:\n",
    "                        interp_f=interp(examples[i][0],examples[i][feature_id],kind=\"cubic\")\n",
    "                        interp_data.append([interp_f(x)for x in interp_x])\n",
    "                    except:\n",
    "                        raise ValueError(\"%d %d\"%(i,feature_id),examples[i])\n",
    "                bar.update(i)\n",
    "                examples[i]=np.array(interp_data)\n",
    "        print(\"\\ndone\")\n",
    "        #数据预处理\n",
    "        preprocession=Preprocessing(with_label=self.with_label)\n",
    "        examples=preprocession(examples)\n",
    "        self.examples=np.array(examples,dtype=\"float32\")    \n",
    "        if self.with_label:\n",
    "            self.y=self.examples[::,-1,0].tolist()\n",
    "            self.x=self.examples[::,1:-1,::]\n",
    "        else:\n",
    "            self.x=self.examples[::,1:,::]\n",
    "        return self\n",
    "    def apply_class_weights(self):\n",
    "        weights=np.zeros([self.num_classes])\n",
    "        for i in range(len(weights)):\n",
    "            weights[i]=(self.examples[::,-1:,0]==i).sum()\n",
    "        return weights/weights.sum()\n",
    "    def data_format(string=\"channel_last\"):\n",
    "        if not string in [\"channel_first\",\"channel_last\"]:\n",
    "            raise ValueError(\"either channel_last or channel_first are supported\")\n",
    "        self.format=string\n",
    "    def apply_data(self):\n",
    "        if self.with_label:\n",
    "            if self.split:\n",
    "                if self.format==\"channel_first\":\n",
    "                    return self.x_train,self.y_train,self.x_test,self.y_test\n",
    "                else:\n",
    "                    return self.x_train.transpose([0,2,1]),self.y_train,self.x_test.transpose([0,2,1]),self.y_test\n",
    "                    \n",
    "            else:\n",
    "                if self.format==\"channel_first\":\n",
    "                    return self.x,self.y\n",
    "                else:\n",
    "                    return self.x.transpose([0,2,1]),self.y\n",
    "        else:\n",
    "            if self.format==\"channel_first\":\n",
    "                return self.x\n",
    "            else:\n",
    "                return self.x.transpose([0,2,1])\n",
    "if False:\n",
    "    train_csv_file=\"data\\\\sensor_train.csv\"\n",
    "    test_csv_file=\"data\\\\sensor_test.csv\"\n",
    "\n",
    "    dataset=DatasetLoader(train_csv_file)\n",
    "    print(dataset.names)\n",
    "    data=dataset.make_numpy()\n",
    "    data=dataset.resample(num_interpolation=60)\n",
    "    x_train,y_train=data.apply_data()\n",
    "\n",
    "plt.plot(x_train[5,::,8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "def single_score(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "def py_score(y_true,y_pred):\n",
    "    y_true=np.argmax(y_true,axis=-1)\n",
    "    y_pred=np.argmax(y_pred,axis=-1)\n",
    "    scores=[]\n",
    "    for i in range(len(y_true)):\n",
    "        scores.append(single_score(y_true[i],y_pred[i]))\n",
    "    mean_score=np.array(scores,dtype=\"float32\").mean()\n",
    "    return mean_score,mean_score\n",
    "def score(y_true,y_pred):\n",
    "    \"\"\"线上评测所使用的评测方法\n",
    "    Args:\n",
    "      y_true:one_hot编码的标签\n",
    "      y_pred:网络类别置信度预测\n",
    "    Returns:\n",
    "      Tensor标量\n",
    "    \"\"\"\n",
    "    mean_score=tf.py_func(py_score,[y_true,y_pred],[tf.float32,tf.float32])[0]\n",
    "    return tf.reshape(mean_score,shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#网络结构\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import UpSampling1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.reset_default_graph()\n",
    "class ASPP(object):\n",
    "    def __init__(self,filters,kernel_size,activation,dilation_rates=[1,2,5,9]):\n",
    "        self.filters=filters\n",
    "        self.kernel_size=kernel_size\n",
    "        self.dilation_rates=dilation_rates\n",
    "        self.activation=activation\n",
    "    def branch(self,tensor,dilation_rate):\n",
    "        x=Conv1D(self.filters,self.kernel_size,1,padding=\"same\",kernel_regularizer=l2(0.011),\n",
    "                 activation=self.activation,dilation_rate=dilation_rate)(tensor)\n",
    "        x=BatchNormalization()(x)\n",
    "        x=Activation(self.activation)(x)\n",
    "        return x\n",
    "    def pool(self,tensor):\n",
    "        shape=tensor.shape.as_list()#[batch,lenth,channels]\n",
    "        scaler=GlobalAveragePooling1D()(tensor)#[batch,channels]\n",
    "        scaler=Reshape([1]+[shape[2]])(scaler)\n",
    "        scaler=UpSampling1D(shape[1])(scaler)\n",
    "        return scaler\n",
    "    def __call__(self,tensor):\n",
    "        output=[Conv1D(self.filters,1,1,padding=\"same\",activation=self.activation,kernel_regularizer=l2(0.011))(tensor)]\n",
    "        output.append(self.pool(tensor))\n",
    "        for rate in self.dilation_rates:\n",
    "            output.append(self.branch(tensor,rate))\n",
    "        output=tf.keras.layers.concatenate(output)\n",
    "        return output\n",
    "def CNN(inputs,num_classes):\n",
    "    x=Conv1D(64,3,1,padding=\"same\",kernel_regularizer=l2(0.01))(inputs)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation(tf.nn.relu)(x)\n",
    "    \n",
    "    x=Conv1D(128,3,1,padding=\"same\",kernel_regularizer=l2(0.01))(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation(tf.nn.relu)(x)\n",
    "    \n",
    "    x=MaxPooling1D(2,padding=\"same\")(x)\n",
    "    \n",
    "    x=Conv1D(256,3,1,padding=\"same\",kernel_regularizer=l2(0.01))(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation(tf.nn.relu)(x)\n",
    "    \n",
    "    x=ASPP(128,3,activation=tf.nn.relu)(x)\n",
    "    \n",
    "    x=GlobalAveragePooling1D()(x)\n",
    "    x=Dropout(0.4)(x)\n",
    "\n",
    "    x=Dense(num_classes,kernel_regularizer=l2(0.01))(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Softmax()(x)\n",
    "    return x\n",
    "inputs=Input(shape=[200,8])\n",
    "outputs=CNN(inputs,num_classes=19)\n",
    "tf.keras.utils.plot_model(Model(inputs,outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别权重\n",
    "\n",
    "class_weight：字典，将不同的类别映射为不同的权值，该参数用来在训练过程中调整损失函数（只能用于训练）。该参数在处理非平衡的训练数据（某些类的训练样本数很少）时，可以使得损失函数对样本数不足的数据更加关注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5c535a28d249>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"data\\\\sensor_train.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtest_csv_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"data\\\\sensor_test.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#加载数据和模型\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "tf.reset_default_graph()\n",
    "csv_file=\"data\\\\sensor_train.csv\"\n",
    "test_csv_file=\"data\\\\sensor_test.csv\"\n",
    "filepath='best_weights_aspp_raw'\n",
    "batch_size=24\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess=tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "if False:\n",
    "    dataset=DatasetLoader(csv_file,with_label=True,num_classes=19)\n",
    "    dataset=dataset.make_numpy()\n",
    "    dataset=dataset.resample(num_interpolation=64)\n",
    "    x,y=dataset.apply_data()\n",
    "    class_weight=dataset.apply_class_weights()\n",
    "    dataset=DatasetLoader(test_csv_file,with_label=False)\n",
    "    data=dataset.make_numpy()\n",
    "    data=dataset.resample(num_interpolation=64)\n",
    "    x_val=data.apply_data()\n",
    "plt.plot(x[5])\n",
    "plt.legend([\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90409104, 0.74687257, 0.84099484, 0.86953634, 0.83150041,\n",
       "       0.86542005, 0.72474282, 0.90601573, 0.87103641, 0.88651163,\n",
       "       0.90601573, 0.82641776, 0.72740161, 0.88499385, 0.94905292,\n",
       "       0.81595538, 0.86467304, 0.89718473, 0.86467304])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-class_weight)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5825 samples, validate on 1467 samples\n",
      "Epoch 1/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 8.7624 - acc: 0.3346 - score: 0.4169\n",
      "Epoch 00001: val_score improved from -inf to 0.29866, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 3s 589us/sample - loss: 8.5832 - acc: 0.3360 - score: 0.4249 - val_loss: 5.3648 - val_acc: 0.2052 - val_score: 0.2987\n",
      "Epoch 2/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 3.3922 - acc: 0.4082 - score: 0.4872\n",
      "Epoch 00002: val_score did not improve from 0.29866\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 3.3766 - acc: 0.4093 - score: 0.4827 - val_loss: 3.6843 - val_acc: 0.1513 - val_score: 0.2675\n",
      "Epoch 3/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 2.4971 - acc: 0.4388 - score: 0.5150\n",
      "Epoch 00003: val_score did not improve from 0.29866\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 2.4898 - acc: 0.4395 - score: 0.5144 - val_loss: 3.2455 - val_acc: 0.2004 - val_score: 0.2941\n",
      "Epoch 4/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 2.2158 - acc: 0.4575 - score: 0.5318\n",
      "Epoch 00004: val_score improved from 0.29866 to 0.31904, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 218us/sample - loss: 2.2128 - acc: 0.4584 - score: 0.5376 - val_loss: 2.9028 - val_acc: 0.2086 - val_score: 0.3190\n",
      "Epoch 5/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 2.0791 - acc: 0.4708 - score: 0.5444\n",
      "Epoch 00005: val_score improved from 0.31904 to 0.36130, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 218us/sample - loss: 2.0793 - acc: 0.4707 - score: 0.5422 - val_loss: 2.7855 - val_acc: 0.2556 - val_score: 0.3613\n",
      "Epoch 6/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 2.0060 - acc: 0.4801 - score: 0.5519\n",
      "Epoch 00006: val_score improved from 0.36130 to 0.38517, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 219us/sample - loss: 2.0044 - acc: 0.4802 - score: 0.5461 - val_loss: 2.6271 - val_acc: 0.2883 - val_score: 0.3852\n",
      "Epoch 7/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.9460 - acc: 0.4917 - score: 0.5630\n",
      "Epoch 00007: val_score improved from 0.38517 to 0.45583, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 218us/sample - loss: 1.9449 - acc: 0.4918 - score: 0.5587 - val_loss: 2.3296 - val_acc: 0.3756 - val_score: 0.4558\n",
      "Epoch 8/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 1.9023 - acc: 0.4993 - score: 0.5707\n",
      "Epoch 00008: val_score did not improve from 0.45583\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 1.9002 - acc: 0.4994 - score: 0.5683 - val_loss: 2.3490 - val_acc: 0.3415 - val_score: 0.4283\n",
      "Epoch 9/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.8711 - acc: 0.4954 - score: 0.5666\n",
      "Epoch 00009: val_score did not improve from 0.45583\n",
      "5825/5825 [==============================] - 1s 209us/sample - loss: 1.8701 - acc: 0.4963 - score: 0.5613 - val_loss: 2.4534 - val_acc: 0.3142 - val_score: 0.4100\n",
      "Epoch 10/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 1.8218 - acc: 0.5091 - score: 0.5781\n",
      "Epoch 00010: val_score did not improve from 0.45583\n",
      "5825/5825 [==============================] - 1s 208us/sample - loss: 1.8179 - acc: 0.5102 - score: 0.5729 - val_loss: 2.5532 - val_acc: 0.3013 - val_score: 0.3693\n",
      "Epoch 11/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 1.8237 - acc: 0.4948 - score: 0.5683\n",
      "Epoch 00011: val_score improved from 0.45583 to 0.48023, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 220us/sample - loss: 1.8232 - acc: 0.4956 - score: 0.5629 - val_loss: 2.1724 - val_acc: 0.4090 - val_score: 0.4802\n",
      "Epoch 12/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.8052 - acc: 0.5079 - score: 0.5791\n",
      "Epoch 00012: val_score improved from 0.48023 to 0.50844, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 217us/sample - loss: 1.8055 - acc: 0.5078 - score: 0.5728 - val_loss: 2.1053 - val_acc: 0.4267 - val_score: 0.5084\n",
      "Epoch 13/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.7687 - acc: 0.5185 - score: 0.5863\n",
      "Epoch 00013: val_score did not improve from 0.50844\n",
      "5825/5825 [==============================] - 1s 208us/sample - loss: 1.7690 - acc: 0.5185 - score: 0.5799 - val_loss: 2.4086 - val_acc: 0.3456 - val_score: 0.4211\n",
      "Epoch 14/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.7416 - acc: 0.5270 - score: 0.5955\n",
      "Epoch 00014: val_score did not improve from 0.50844\n",
      "5825/5825 [==============================] - 1s 210us/sample - loss: 1.7415 - acc: 0.5265 - score: 0.5886 - val_loss: 2.2846 - val_acc: 0.3620 - val_score: 0.4478\n",
      "Epoch 15/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.7285 - acc: 0.5330 - score: 0.6003\n",
      "Epoch 00015: val_score did not improve from 0.50844\n",
      "5825/5825 [==============================] - 1s 208us/sample - loss: 1.7293 - acc: 0.5322 - score: 0.6038 - val_loss: 2.4495 - val_acc: 0.3367 - val_score: 0.4170\n",
      "Epoch 16/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.7325 - acc: 0.5261 - score: 0.5931\n",
      "Epoch 00016: val_score did not improve from 0.50844\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 1.7327 - acc: 0.5260 - score: 0.5903 - val_loss: 3.3790 - val_acc: 0.2413 - val_score: 0.3161\n",
      "Epoch 17/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.7036 - acc: 0.5295 - score: 0.5977\n",
      "Epoch 00017: val_score did not improve from 0.50844\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 1.7040 - acc: 0.5294 - score: 0.5912 - val_loss: 2.2268 - val_acc: 0.3640 - val_score: 0.4464\n",
      "Epoch 18/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.6838 - acc: 0.5436 - score: 0.6082\n",
      "Epoch 00018: val_score improved from 0.50844 to 0.54114, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 219us/sample - loss: 1.6861 - acc: 0.5427 - score: 0.6024 - val_loss: 1.9162 - val_acc: 0.4663 - val_score: 0.5411\n",
      "Epoch 19/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.6844 - acc: 0.5436 - score: 0.6085\n",
      "Epoch 00019: val_score did not improve from 0.54114\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 1.6838 - acc: 0.5437 - score: 0.6022 - val_loss: 2.7425 - val_acc: 0.2870 - val_score: 0.3726\n",
      "Epoch 20/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 1.6766 - acc: 0.5460 - score: 0.6112\n",
      "Epoch 00020: val_score did not improve from 0.54114\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 1.6762 - acc: 0.5461 - score: 0.6048 - val_loss: 2.7428 - val_acc: 0.2802 - val_score: 0.3641\n",
      "Epoch 21/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.6486 - acc: 0.5549 - score: 0.6188\n",
      "Epoch 00021: val_score did not improve from 0.54114\n",
      "5825/5825 [==============================] - 1s 208us/sample - loss: 1.6489 - acc: 0.5548 - score: 0.6121 - val_loss: 2.1983 - val_acc: 0.3845 - val_score: 0.4531\n",
      "Epoch 22/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.6338 - acc: 0.5589 - score: 0.6231\n",
      "Epoch 00022: val_score did not improve from 0.54114\n",
      "5825/5825 [==============================] - 1s 210us/sample - loss: 1.6344 - acc: 0.5586 - score: 0.6160 - val_loss: 2.3194 - val_acc: 0.3647 - val_score: 0.4470\n",
      "Epoch 23/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.6120 - acc: 0.5637 - score: 0.6266\n",
      "Epoch 00023: val_score did not improve from 0.54114\n",
      "5825/5825 [==============================] - 1s 208us/sample - loss: 1.6103 - acc: 0.5638 - score: 0.6201 - val_loss: 2.0493 - val_acc: 0.4294 - val_score: 0.5151\n",
      "Epoch 24/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.5966 - acc: 0.5610 - score: 0.6250\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00024: val_score did not improve from 0.54114\n",
      "5825/5825 [==============================] - 1s 220us/sample - loss: 1.5969 - acc: 0.5609 - score: 0.6183 - val_loss: 1.9865 - val_acc: 0.4608 - val_score: 0.5327\n",
      "Epoch 25/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5632/5825 [============================>.] - ETA: 0s - loss: 1.4835 - acc: 0.5985 - score: 0.6585\n",
      "Epoch 00025: val_score did not improve from 0.54114\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 1.4865 - acc: 0.5969 - score: 0.6515 - val_loss: 1.9263 - val_acc: 0.4588 - val_score: 0.5397\n",
      "Epoch 26/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.4072 - acc: 0.6192 - score: 0.6758\n",
      "Epoch 00026: val_score improved from 0.54114 to 0.57223, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 220us/sample - loss: 1.4066 - acc: 0.6184 - score: 0.6678 - val_loss: 1.8704 - val_acc: 0.5003 - val_score: 0.5722\n",
      "Epoch 27/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.3797 - acc: 0.6254 - score: 0.6817\n",
      "Epoch 00027: val_score improved from 0.57223 to 0.57259, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 215us/sample - loss: 1.3840 - acc: 0.6228 - score: 0.6722 - val_loss: 1.7872 - val_acc: 0.5051 - val_score: 0.5726\n",
      "Epoch 28/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.3662 - acc: 0.6264 - score: 0.6808\n",
      "Epoch 00028: val_score did not improve from 0.57259\n",
      "5825/5825 [==============================] - 1s 208us/sample - loss: 1.3677 - acc: 0.6249 - score: 0.6724 - val_loss: 1.9152 - val_acc: 0.4594 - val_score: 0.5386\n",
      "Epoch 29/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 1.3616 - acc: 0.6266 - score: 0.6833\n",
      "Epoch 00029: val_score did not improve from 0.57259\n",
      "5825/5825 [==============================] - 1s 209us/sample - loss: 1.3589 - acc: 0.6273 - score: 0.6874 - val_loss: 2.3611 - val_acc: 0.3947 - val_score: 0.4704\n",
      "Epoch 30/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 1.3675 - acc: 0.6328 - score: 0.6880\n",
      "Epoch 00030: val_score improved from 0.57259 to 0.59888, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 220us/sample - loss: 1.3711 - acc: 0.6319 - score: 0.6814 - val_loss: 1.6697 - val_acc: 0.5399 - val_score: 0.5989\n",
      "Epoch 31/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.3407 - acc: 0.6373 - score: 0.6920\n",
      "Epoch 00031: val_score did not improve from 0.59888\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 1.3406 - acc: 0.6381 - score: 0.6867 - val_loss: 1.8845 - val_acc: 0.4758 - val_score: 0.5438\n",
      "Epoch 32/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 1.3280 - acc: 0.6360 - score: 0.6900\n",
      "Epoch 00032: val_score improved from 0.59888 to 0.62518, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 218us/sample - loss: 1.3307 - acc: 0.6361 - score: 0.6830 - val_loss: 1.6056 - val_acc: 0.5583 - val_score: 0.6252\n",
      "Epoch 33/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.3102 - acc: 0.6587 - score: 0.7080\n",
      "Epoch 00033: val_score did not improve from 0.62518\n",
      "5825/5825 [==============================] - 1s 208us/sample - loss: 1.3095 - acc: 0.6580 - score: 0.7037 - val_loss: 2.1302 - val_acc: 0.4254 - val_score: 0.4916\n",
      "Epoch 34/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.2946 - acc: 0.6531 - score: 0.7051\n",
      "Epoch 00034: val_score did not improve from 0.62518\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 1.2950 - acc: 0.6532 - score: 0.6976 - val_loss: 2.4006 - val_acc: 0.3497 - val_score: 0.4197\n",
      "Epoch 35/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 1.2981 - acc: 0.6571 - score: 0.7070\n",
      "Epoch 00035: val_score did not improve from 0.62518\n",
      "5825/5825 [==============================] - 1s 208us/sample - loss: 1.3025 - acc: 0.6548 - score: 0.6993 - val_loss: 1.7136 - val_acc: 0.5085 - val_score: 0.5698\n",
      "Epoch 36/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.2815 - acc: 0.6591 - score: 0.7081\n",
      "Epoch 00036: val_score did not improve from 0.62518\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 1.2813 - acc: 0.6601 - score: 0.7013 - val_loss: 1.7715 - val_acc: 0.5106 - val_score: 0.5864\n",
      "Epoch 37/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.2638 - acc: 0.6608 - score: 0.7133\n",
      "Epoch 00037: val_score improved from 0.62518 to 0.63951, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 218us/sample - loss: 1.2656 - acc: 0.6608 - score: 0.7092 - val_loss: 1.5802 - val_acc: 0.5767 - val_score: 0.6395\n",
      "Epoch 38/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 1.2500 - acc: 0.6733 - score: 0.7221\n",
      "Epoch 00038: val_score did not improve from 0.63951\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 1.2487 - acc: 0.6736 - score: 0.7181 - val_loss: 2.1413 - val_acc: 0.3885 - val_score: 0.4718\n",
      "Epoch 39/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 1.2375 - acc: 0.6758 - score: 0.7242\n",
      "Epoch 00039: val_score did not improve from 0.63951\n",
      "5825/5825 [==============================] - 1s 210us/sample - loss: 1.2378 - acc: 0.6757 - score: 0.7199 - val_loss: 2.0796 - val_acc: 0.4424 - val_score: 0.5142\n",
      "Epoch 40/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.2311 - acc: 0.6740 - score: 0.7241\n",
      "Epoch 00040: val_score improved from 0.63951 to 0.63953, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 222us/sample - loss: 1.2318 - acc: 0.6735 - score: 0.7158 - val_loss: 1.5728 - val_acc: 0.5699 - val_score: 0.6395\n",
      "Epoch 41/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 1.2199 - acc: 0.6887 - score: 0.7364\n",
      "Epoch 00041: val_score did not improve from 0.63953\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 1.2217 - acc: 0.6869 - score: 0.7269 - val_loss: 1.8946 - val_acc: 0.4819 - val_score: 0.5530\n",
      "Epoch 42/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.2250 - acc: 0.6798 - score: 0.7277\n",
      "Epoch 00042: val_score did not improve from 0.63953\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 1.2273 - acc: 0.6781 - score: 0.7203 - val_loss: 1.7241 - val_acc: 0.5297 - val_score: 0.5874\n",
      "Epoch 43/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.2178 - acc: 0.6870 - score: 0.7339\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00043: val_score did not improve from 0.63953\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 1.2185 - acc: 0.6862 - score: 0.7253 - val_loss: 1.9222 - val_acc: 0.4819 - val_score: 0.5529\n",
      "Epoch 44/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.1085 - acc: 0.7191 - score: 0.7621\n",
      "Epoch 00044: val_score improved from 0.63953 to 0.66676, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 223us/sample - loss: 1.1083 - acc: 0.7190 - score: 0.7553 - val_loss: 1.4246 - val_acc: 0.6087 - val_score: 0.6668\n",
      "Epoch 45/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.0516 - acc: 0.7412 - score: 0.7821\n",
      "Epoch 00045: val_score did not improve from 0.66676\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 1.0511 - acc: 0.7409 - score: 0.7736 - val_loss: 1.6099 - val_acc: 0.5631 - val_score: 0.6281\n",
      "Epoch 46/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 1.0205 - acc: 0.7465 - score: 0.7856\n",
      "Epoch 00046: val_score did not improve from 0.66676\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 1.0218 - acc: 0.7464 - score: 0.7770 - val_loss: 1.6156 - val_acc: 0.5685 - val_score: 0.6209\n",
      "Epoch 47/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.0100 - acc: 0.7502 - score: 0.7892\n",
      "Epoch 00047: val_score did not improve from 0.66676\n",
      "5825/5825 [==============================] - 1s 211us/sample - loss: 1.0113 - acc: 0.7490 - score: 0.7797 - val_loss: 1.4552 - val_acc: 0.6019 - val_score: 0.6602\n",
      "Epoch 48/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.9951 - acc: 0.7569 - score: 0.7938\n",
      "Epoch 00048: val_score did not improve from 0.66676\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.9955 - acc: 0.7566 - score: 0.7849 - val_loss: 1.7217 - val_acc: 0.5358 - val_score: 0.6001\n",
      "Epoch 49/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.9857 - acc: 0.7610 - score: 0.7973\n",
      "Epoch 00049: val_score did not improve from 0.66676\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.9868 - acc: 0.7609 - score: 0.7885 - val_loss: 1.5278 - val_acc: 0.5958 - val_score: 0.6546\n",
      "Epoch 50/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 1.0015 - acc: 0.7526 - score: 0.7908\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00050: val_score did not improve from 0.66676\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 1.0014 - acc: 0.7530 - score: 0.7841 - val_loss: 1.4950 - val_acc: 0.6012 - val_score: 0.6579\n",
      "Epoch 51/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.8942 - acc: 0.7906 - score: 0.8225\n",
      "Epoch 00051: val_score improved from 0.66676 to 0.73563, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 216us/sample - loss: 0.8939 - acc: 0.7907 - score: 0.8138 - val_loss: 1.1806 - val_acc: 0.6905 - val_score: 0.7356\n",
      "Epoch 52/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.8759 - acc: 0.8011 - score: 0.8318\n",
      "Epoch 00052: val_score did not improve from 0.73563\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.8797 - acc: 0.7986 - score: 0.8211 - val_loss: 1.2310 - val_acc: 0.6728 - val_score: 0.7193\n",
      "Epoch 53/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.8336 - acc: 0.8152 - score: 0.8441\n",
      "Epoch 00053: val_score improved from 0.73563 to 0.74788, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 218us/sample - loss: 0.8340 - acc: 0.8151 - score: 0.8385 - val_loss: 1.1588 - val_acc: 0.7021 - val_score: 0.7479\n",
      "Epoch 54/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.8188 - acc: 0.8214 - score: 0.8491\n",
      "Epoch 00054: val_score improved from 0.74788 to 0.75770, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 215us/sample - loss: 0.8215 - acc: 0.8194 - score: 0.8419 - val_loss: 1.1072 - val_acc: 0.7144 - val_score: 0.7577\n",
      "Epoch 55/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.8051 - acc: 0.8295 - score: 0.8560\n",
      "Epoch 00055: val_score did not improve from 0.75770\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 0.8080 - acc: 0.8280 - score: 0.8456 - val_loss: 1.2260 - val_acc: 0.6721 - val_score: 0.7265\n",
      "Epoch 56/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.8108 - acc: 0.8172 - score: 0.8450\n",
      "Epoch 00056: val_score did not improve from 0.75770\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.8117 - acc: 0.8165 - score: 0.8354 - val_loss: 1.1605 - val_acc: 0.6905 - val_score: 0.7394\n",
      "Epoch 57/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.7884 - acc: 0.8257 - score: 0.8539\n",
      "Epoch 00057: val_score improved from 0.75770 to 0.78013, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 216us/sample - loss: 0.7887 - acc: 0.8256 - score: 0.8483 - val_loss: 1.0498 - val_acc: 0.7416 - val_score: 0.7801\n",
      "Epoch 58/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.7862 - acc: 0.8227 - score: 0.8497\n",
      "Epoch 00058: val_score did not improve from 0.78013\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.7855 - acc: 0.8228 - score: 0.8408 - val_loss: 1.1301 - val_acc: 0.7178 - val_score: 0.7635\n",
      "Epoch 59/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.7751 - acc: 0.8311 - score: 0.8567\n",
      "Epoch 00059: val_score improved from 0.78013 to 0.79222, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 218us/sample - loss: 0.7765 - acc: 0.8309 - score: 0.8472 - val_loss: 1.0156 - val_acc: 0.7546 - val_score: 0.7922\n",
      "Epoch 60/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.7876 - acc: 0.8265 - score: 0.8525\n",
      "Epoch 00060: val_score did not improve from 0.79222\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.7887 - acc: 0.8266 - score: 0.8434 - val_loss: 1.1461 - val_acc: 0.7035 - val_score: 0.7487\n",
      "Epoch 61/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.7664 - acc: 0.8317 - score: 0.8574\n",
      "Epoch 00061: val_score did not improve from 0.79222\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.7663 - acc: 0.8326 - score: 0.8527 - val_loss: 1.0468 - val_acc: 0.7192 - val_score: 0.7665\n",
      "Epoch 62/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.7403 - acc: 0.8403 - score: 0.8656\n",
      "Epoch 00062: val_score did not improve from 0.79222\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.7421 - acc: 0.8400 - score: 0.8596 - val_loss: 1.0276 - val_acc: 0.7416 - val_score: 0.7833\n",
      "Epoch 63/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.7335 - acc: 0.8457 - score: 0.8699\n",
      "Epoch 00063: val_score did not improve from 0.79222\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.7327 - acc: 0.8462 - score: 0.8608 - val_loss: 1.2155 - val_acc: 0.6796 - val_score: 0.7305\n",
      "Epoch 64/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.7307 - acc: 0.8418 - score: 0.8677\n",
      "Epoch 00064: val_score did not improve from 0.79222\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.7328 - acc: 0.8415 - score: 0.8595 - val_loss: 1.3003 - val_acc: 0.6537 - val_score: 0.7072\n",
      "Epoch 65/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.7253 - acc: 0.8508 - score: 0.8731\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00065: val_score did not improve from 0.79222\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.7256 - acc: 0.8508 - score: 0.8745 - val_loss: 1.0887 - val_acc: 0.7321 - val_score: 0.7736\n",
      "Epoch 66/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.6788 - acc: 0.8649 - score: 0.8859\n",
      "Epoch 00066: val_score improved from 0.79222 to 0.80576, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 217us/sample - loss: 0.6780 - acc: 0.8658 - score: 0.8773 - val_loss: 0.9506 - val_acc: 0.7682 - val_score: 0.8058\n",
      "Epoch 67/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.6490 - acc: 0.8737 - score: 0.8929\n",
      "Epoch 00067: val_score improved from 0.80576 to 0.80665, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 215us/sample - loss: 0.6480 - acc: 0.8745 - score: 0.8840 - val_loss: 0.9466 - val_acc: 0.7689 - val_score: 0.8067\n",
      "Epoch 68/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.6397 - acc: 0.8804 - score: 0.8987\n",
      "Epoch 00068: val_score did not improve from 0.80665\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.6414 - acc: 0.8802 - score: 0.8890 - val_loss: 0.9571 - val_acc: 0.7662 - val_score: 0.8004\n",
      "Epoch 69/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.6352 - acc: 0.8827 - score: 0.9007\n",
      "Epoch 00069: val_score improved from 0.80665 to 0.80752, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 217us/sample - loss: 0.6345 - acc: 0.8831 - score: 0.8929 - val_loss: 0.9266 - val_acc: 0.7710 - val_score: 0.8075\n",
      "Epoch 70/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.6300 - acc: 0.8831 - score: 0.9011\n",
      "Epoch 00070: val_score did not improve from 0.80752\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.6304 - acc: 0.8829 - score: 0.8913 - val_loss: 0.9693 - val_acc: 0.7532 - val_score: 0.7940\n",
      "Epoch 71/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.6195 - acc: 0.8893 - score: 0.9068\n",
      "Epoch 00071: val_score improved from 0.80752 to 0.81704, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 216us/sample - loss: 0.6199 - acc: 0.8891 - score: 0.8970 - val_loss: 0.9109 - val_acc: 0.7832 - val_score: 0.8170\n",
      "Epoch 72/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.6114 - acc: 0.8851 - score: 0.9033\n",
      "Epoch 00072: val_score did not improve from 0.81704\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.6107 - acc: 0.8850 - score: 0.8934 - val_loss: 0.9394 - val_acc: 0.7723 - val_score: 0.8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.6246 - acc: 0.8784 - score: 0.8967\n",
      "Epoch 00073: val_score did not improve from 0.81704\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.6248 - acc: 0.8779 - score: 0.8866 - val_loss: 0.9565 - val_acc: 0.7682 - val_score: 0.8033\n",
      "Epoch 74/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.8982 - score: 0.9136\n",
      "Epoch 00074: val_score did not improve from 0.81704\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.6016 - acc: 0.8980 - score: 0.9036 - val_loss: 0.9477 - val_acc: 0.7689 - val_score: 0.8053\n",
      "Epoch 75/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.5792 - acc: 0.8972 - score: 0.9144\n",
      "Epoch 00075: val_score did not improve from 0.81704\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.5831 - acc: 0.8968 - score: 0.9058 - val_loss: 0.9349 - val_acc: 0.7771 - val_score: 0.8116\n",
      "Epoch 76/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.5830 - acc: 0.8976 - score: 0.9139\n",
      "Epoch 00076: val_score did not improve from 0.81704\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.5858 - acc: 0.8972 - score: 0.9053 - val_loss: 0.9921 - val_acc: 0.7505 - val_score: 0.7882\n",
      "Epoch 77/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.5823 - acc: 0.8989 - score: 0.9149\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00077: val_score did not improve from 0.81704\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.5830 - acc: 0.8994 - score: 0.9093 - val_loss: 0.9642 - val_acc: 0.7621 - val_score: 0.8000\n",
      "Epoch 78/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.9076 - score: 0.9219\n",
      "Epoch 00078: val_score improved from 0.81704 to 0.81858, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 218us/sample - loss: 0.5502 - acc: 0.9075 - score: 0.9119 - val_loss: 0.8891 - val_acc: 0.7846 - val_score: 0.8186\n",
      "Epoch 79/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.5392 - acc: 0.9134 - score: 0.9268\n",
      "Epoch 00079: val_score did not improve from 0.81858\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.5461 - acc: 0.9128 - score: 0.9163 - val_loss: 0.9008 - val_acc: 0.7764 - val_score: 0.8130\n",
      "Epoch 80/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.5447 - acc: 0.9142 - score: 0.9272\n",
      "Epoch 00080: val_score did not improve from 0.81858\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 0.5460 - acc: 0.9136 - score: 0.9204 - val_loss: 0.8932 - val_acc: 0.7812 - val_score: 0.8161\n",
      "Epoch 81/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.9231 - score: 0.9354\n",
      "Epoch 00081: val_score improved from 0.81858 to 0.82323, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 217us/sample - loss: 0.5193 - acc: 0.9231 - score: 0.9269 - val_loss: 0.8917 - val_acc: 0.7887 - val_score: 0.8232\n",
      "Epoch 82/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.9174 - score: 0.9299\n",
      "Epoch 00082: val_score improved from 0.82323 to 0.82574, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 216us/sample - loss: 0.5398 - acc: 0.9178 - score: 0.9239 - val_loss: 0.9005 - val_acc: 0.7921 - val_score: 0.8257\n",
      "Epoch 83/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.5281 - acc: 0.9203 - score: 0.9330\n",
      "Epoch 00083: val_score did not improve from 0.82574\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.5293 - acc: 0.9198 - score: 0.9241 - val_loss: 0.9041 - val_acc: 0.7771 - val_score: 0.8121\n",
      "Epoch 84/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.9260 - score: 0.9377\n",
      "Epoch 00084: val_score did not improve from 0.82574\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.5133 - acc: 0.9260 - score: 0.9384 - val_loss: 0.9183 - val_acc: 0.7778 - val_score: 0.8133\n",
      "Epoch 85/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.9196 - score: 0.9313\n",
      "Epoch 00085: val_score did not improve from 0.82574\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.5232 - acc: 0.9193 - score: 0.9319 - val_loss: 0.8741 - val_acc: 0.7846 - val_score: 0.8190\n",
      "Epoch 86/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.9185 - score: 0.9317\n",
      "Epoch 00086: val_score did not improve from 0.82574\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.5238 - acc: 0.9188 - score: 0.9219 - val_loss: 0.9004 - val_acc: 0.7846 - val_score: 0.8191\n",
      "Epoch 87/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.5018 - acc: 0.9264 - score: 0.9379\n",
      "Epoch 00087: val_score did not improve from 0.82574\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 0.5016 - acc: 0.9265 - score: 0.9280 - val_loss: 0.8984 - val_acc: 0.7798 - val_score: 0.8160\n",
      "Epoch 88/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.9245 - score: 0.9364\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00088: val_score did not improve from 0.82574\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 0.5055 - acc: 0.9243 - score: 0.9277 - val_loss: 0.8886 - val_acc: 0.7928 - val_score: 0.8255\n",
      "Epoch 89/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.5006 - acc: 0.9271 - score: 0.9381\n",
      "Epoch 00089: val_score did not improve from 0.82574\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 0.5004 - acc: 0.9276 - score: 0.9322 - val_loss: 0.8648 - val_acc: 0.7900 - val_score: 0.8240\n",
      "Epoch 90/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.4914 - acc: 0.9335 - score: 0.9444\n",
      "Epoch 00090: val_score did not improve from 0.82574\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 0.4923 - acc: 0.9330 - score: 0.9376 - val_loss: 0.8679 - val_acc: 0.7900 - val_score: 0.8234\n",
      "Epoch 91/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.9317 - score: 0.9415\n",
      "Epoch 00091: val_score did not improve from 0.82574\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.4873 - acc: 0.9310 - score: 0.9323 - val_loss: 0.8777 - val_acc: 0.7860 - val_score: 0.8199\n",
      "Epoch 92/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.4873 - acc: 0.9334 - score: 0.9436\n",
      "Epoch 00092: val_score did not improve from 0.82574\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.4871 - acc: 0.9332 - score: 0.9334 - val_loss: 0.8715 - val_acc: 0.7825 - val_score: 0.8167\n",
      "Epoch 93/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.4851 - acc: 0.9334 - score: 0.9437\n",
      "Epoch 00093: val_score improved from 0.82574 to 0.83347, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 214us/sample - loss: 0.4831 - acc: 0.9344 - score: 0.9345 - val_loss: 0.8704 - val_acc: 0.8016 - val_score: 0.8335\n",
      "Epoch 94/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.4758 - acc: 0.9331 - score: 0.9430\n",
      "Epoch 00094: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.4770 - acc: 0.9327 - score: 0.9326 - val_loss: 0.8606 - val_acc: 0.7935 - val_score: 0.8272\n",
      "Epoch 95/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.4779 - acc: 0.9368 - score: 0.9466\n",
      "Epoch 00095: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.4785 - acc: 0.9365 - score: 0.9377 - val_loss: 0.8734 - val_acc: 0.7825 - val_score: 0.8174\n",
      "Epoch 96/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4736 - acc: 0.9358 - score: 0.9459\n",
      "Epoch 00096: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 0.4741 - acc: 0.9356 - score: 0.9357 - val_loss: 0.8739 - val_acc: 0.7846 - val_score: 0.8190\n",
      "Epoch 97/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.9389 - score: 0.9481\n",
      "Epoch 00097: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.4737 - acc: 0.9387 - score: 0.9378 - val_loss: 0.8609 - val_acc: 0.7989 - val_score: 0.8311\n",
      "Epoch 98/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4697 - acc: 0.9370 - score: 0.9470\n",
      "Epoch 00098: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 0.4701 - acc: 0.9368 - score: 0.9403 - val_loss: 0.8548 - val_acc: 0.7955 - val_score: 0.8280\n",
      "Epoch 99/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.4826 - acc: 0.9316 - score: 0.9421\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00099: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 202us/sample - loss: 0.4839 - acc: 0.9315 - score: 0.9332 - val_loss: 0.8615 - val_acc: 0.7935 - val_score: 0.8274\n",
      "Epoch 100/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.4704 - acc: 0.9377 - score: 0.9478\n",
      "Epoch 00100: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.4729 - acc: 0.9361 - score: 0.9401 - val_loss: 0.8487 - val_acc: 0.7962 - val_score: 0.8289\n",
      "Epoch 101/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.9393 - score: 0.9498\n",
      "Epoch 00101: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 202us/sample - loss: 0.4590 - acc: 0.9394 - score: 0.9395 - val_loss: 0.8523 - val_acc: 0.7996 - val_score: 0.8322\n",
      "Epoch 102/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.9417 - score: 0.9511\n",
      "Epoch 00102: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.4569 - acc: 0.9415 - score: 0.9406 - val_loss: 0.8562 - val_acc: 0.7996 - val_score: 0.8313\n",
      "Epoch 103/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.4558 - acc: 0.9407 - score: 0.9505\n",
      "Epoch 00103: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.4535 - acc: 0.9420 - score: 0.9413 - val_loss: 0.8509 - val_acc: 0.7962 - val_score: 0.8289\n",
      "Epoch 104/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4597 - acc: 0.9402 - score: 0.9492\n",
      "Epoch 00104: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.4602 - acc: 0.9401 - score: 0.9388 - val_loss: 0.8586 - val_acc: 0.7989 - val_score: 0.8314\n",
      "Epoch 105/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4566 - acc: 0.9440 - score: 0.9522\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00105: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.4570 - acc: 0.9439 - score: 0.9418 - val_loss: 0.8508 - val_acc: 0.7996 - val_score: 0.8315\n",
      "Epoch 106/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.4552 - acc: 0.9430 - score: 0.9516\n",
      "Epoch 00106: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.4552 - acc: 0.9435 - score: 0.9433 - val_loss: 0.8515 - val_acc: 0.8010 - val_score: 0.8327\n",
      "Epoch 107/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.4549 - acc: 0.9425 - score: 0.9516\n",
      "Epoch 00107: val_score did not improve from 0.83347\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.4544 - acc: 0.9421 - score: 0.9410 - val_loss: 0.8494 - val_acc: 0.7996 - val_score: 0.8319\n",
      "Epoch 108/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.4648 - acc: 0.9382 - score: 0.9474\n",
      "Epoch 00108: val_score improved from 0.83347 to 0.83465, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 217us/sample - loss: 0.4654 - acc: 0.9382 - score: 0.9387 - val_loss: 0.8473 - val_acc: 0.8030 - val_score: 0.8346\n",
      "Epoch 109/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.4473 - acc: 0.9463 - score: 0.9543\n",
      "Epoch 00109: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.4479 - acc: 0.9463 - score: 0.9441 - val_loss: 0.8523 - val_acc: 0.7969 - val_score: 0.8301\n",
      "Epoch 110/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.4645 - acc: 0.9389 - score: 0.9487\n",
      "Epoch 00110: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 0.4651 - acc: 0.9387 - score: 0.9490 - val_loss: 0.8490 - val_acc: 0.8003 - val_score: 0.8317\n",
      "Epoch 111/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.4577 - acc: 0.9422 - score: 0.9522\n",
      "Epoch 00111: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.4573 - acc: 0.9420 - score: 0.9418 - val_loss: 0.8524 - val_acc: 0.7962 - val_score: 0.8294\n",
      "Epoch 112/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.9473 - score: 0.9556\n",
      "Epoch 00112: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.4429 - acc: 0.9471 - score: 0.9489 - val_loss: 0.8539 - val_acc: 0.7996 - val_score: 0.8321\n",
      "Epoch 113/300\n",
      "5760/5825 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.9441 - score: 0.9529\n",
      "Epoch 00113: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 205us/sample - loss: 0.4510 - acc: 0.9430 - score: 0.9417 - val_loss: 0.8547 - val_acc: 0.7969 - val_score: 0.8300\n",
      "Epoch 114/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.4512 - acc: 0.9447 - score: 0.9535\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00114: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 207us/sample - loss: 0.4521 - acc: 0.9449 - score: 0.9451 - val_loss: 0.8516 - val_acc: 0.7989 - val_score: 0.8310\n",
      "Epoch 115/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.4396 - acc: 0.9494 - score: 0.9571\n",
      "Epoch 00115: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.4406 - acc: 0.9492 - score: 0.9466 - val_loss: 0.8509 - val_acc: 0.7996 - val_score: 0.8316\n",
      "Epoch 116/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.9450 - score: 0.9535\n",
      "Epoch 00116: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.4492 - acc: 0.9456 - score: 0.9454 - val_loss: 0.8462 - val_acc: 0.7996 - val_score: 0.8318\n",
      "Epoch 117/300\n",
      "5696/5825 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.9484 - score: 0.9565\n",
      "Epoch 00117: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 206us/sample - loss: 0.4419 - acc: 0.9488 - score: 0.9503 - val_loss: 0.8490 - val_acc: 0.7996 - val_score: 0.8316\n",
      "Epoch 118/300\n",
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4454 - acc: 0.9442 - score: 0.9523\n",
      "Epoch 00118: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 204us/sample - loss: 0.4459 - acc: 0.9440 - score: 0.9435 - val_loss: 0.8463 - val_acc: 0.8010 - val_score: 0.8329\n",
      "Epoch 119/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.4337 - acc: 0.9512 - score: 0.9590\n",
      "Epoch 00119: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 201us/sample - loss: 0.4328 - acc: 0.9519 - score: 0.9493 - val_loss: 0.8447 - val_acc: 0.8010 - val_score: 0.8331\n",
      "Epoch 120/300\n",
      "5632/5825 [============================>.] - ETA: 0s - loss: 0.4587 - acc: 0.9446 - score: 0.9529\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\n",
      "Epoch 00120: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.4578 - acc: 0.9451 - score: 0.9538 - val_loss: 0.8433 - val_acc: 0.7996 - val_score: 0.8316\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5824/5825 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.9478 - score: 0.9556\n",
      "Epoch 00121: val_score did not improve from 0.83465\n",
      "5825/5825 [==============================] - 1s 202us/sample - loss: 0.4499 - acc: 0.9476 - score: 0.9468 - val_loss: 0.8458 - val_acc: 0.8003 - val_score: 0.8323\n",
      "Epoch 122/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.4399 - acc: 0.9445 - score: 0.9532\n",
      "Epoch 00122: val_score improved from 0.83465 to 0.83475, saving model to fold0.h5\n",
      "5825/5825 [==============================] - 1s 214us/sample - loss: 0.4429 - acc: 0.9435 - score: 0.9422 - val_loss: 0.8498 - val_acc: 0.8030 - val_score: 0.8347\n",
      "Epoch 123/300\n",
      "5568/5825 [===========================>..] - ETA: 0s - loss: 0.4387 - acc: 0.9474 - score: 0.9565\n",
      "Epoch 00123: val_score did not improve from 0.83475\n",
      "5825/5825 [==============================] - 1s 203us/sample - loss: 0.4415 - acc: 0.9456 - score: 0.9483 - val_loss: 0.8486 - val_acc: 0.8010 - val_score: 0.8329\n",
      "Epoch 00123: early stopping\n",
      "Train on 5831 samples, validate on 1461 samples\n",
      "Epoch 1/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 8.3791 - acc: 0.3333 - score: 0.4166\n",
      "Epoch 00001: val_score improved from -inf to 0.25917, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 3s 502us/sample - loss: 8.3744 - acc: 0.3330 - score: 0.4148 - val_loss: 5.2069 - val_acc: 0.1636 - val_score: 0.2592\n",
      "Epoch 2/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 3.3516 - acc: 0.4023 - score: 0.4806\n",
      "Epoch 00002: val_score improved from 0.25917 to 0.28541, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 214us/sample - loss: 3.3311 - acc: 0.4022 - score: 0.4816 - val_loss: 3.5826 - val_acc: 0.1732 - val_score: 0.2854\n",
      "Epoch 3/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 2.4995 - acc: 0.4356 - score: 0.5104\n",
      "Epoch 00003: val_score improved from 0.28541 to 0.35449, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 214us/sample - loss: 2.4999 - acc: 0.4354 - score: 0.5082 - val_loss: 3.0453 - val_acc: 0.2601 - val_score: 0.3545\n",
      "Epoch 4/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 2.2616 - acc: 0.4380 - score: 0.5148\n",
      "Epoch 00004: val_score improved from 0.35449 to 0.41911, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 216us/sample - loss: 2.2622 - acc: 0.4375 - score: 0.5098 - val_loss: 2.6667 - val_acc: 0.3258 - val_score: 0.4191\n",
      "Epoch 5/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 2.1202 - acc: 0.4555 - score: 0.5333\n",
      "Epoch 00005: val_score did not improve from 0.41911\n",
      "5831/5831 [==============================] - 1s 204us/sample - loss: 2.1179 - acc: 0.4555 - score: 0.5327 - val_loss: 3.1124 - val_acc: 0.2245 - val_score: 0.3293\n",
      "Epoch 6/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 2.0314 - acc: 0.4695 - score: 0.5450\n",
      "Epoch 00006: val_score did not improve from 0.41911\n",
      "5831/5831 [==============================] - 1s 203us/sample - loss: 2.0305 - acc: 0.4704 - score: 0.5438 - val_loss: 2.7922 - val_acc: 0.2437 - val_score: 0.3367\n",
      "Epoch 7/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.9807 - acc: 0.4763 - score: 0.5500\n",
      "Epoch 00007: val_score did not improve from 0.41911\n",
      "5831/5831 [==============================] - 1s 202us/sample - loss: 1.9811 - acc: 0.4761 - score: 0.5481 - val_loss: 2.4206 - val_acc: 0.3121 - val_score: 0.4088\n",
      "Epoch 8/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.9350 - acc: 0.4790 - score: 0.5546\n",
      "Epoch 00008: val_score improved from 0.41911 to 0.45077, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 216us/sample - loss: 1.9342 - acc: 0.4778 - score: 0.5546 - val_loss: 2.2485 - val_acc: 0.3819 - val_score: 0.4508\n",
      "Epoch 9/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 1.9018 - acc: 0.4896 - score: 0.5631\n",
      "Epoch 00009: val_score did not improve from 0.45077\n",
      "5831/5831 [==============================] - 1s 203us/sample - loss: 1.9002 - acc: 0.4893 - score: 0.5610 - val_loss: 2.8328 - val_acc: 0.2040 - val_score: 0.2716\n",
      "Epoch 10/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 1.8850 - acc: 0.4939 - score: 0.5673\n",
      "Epoch 00010: val_score did not improve from 0.45077\n",
      "5831/5831 [==============================] - 1s 202us/sample - loss: 1.8836 - acc: 0.4932 - score: 0.5688 - val_loss: 2.4314 - val_acc: 0.3183 - val_score: 0.3975\n",
      "Epoch 11/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 1.8240 - acc: 0.4977 - score: 0.5735\n",
      "Epoch 00011: val_score improved from 0.45077 to 0.45869, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 213us/sample - loss: 1.8219 - acc: 0.4999 - score: 0.5732 - val_loss: 2.1105 - val_acc: 0.3799 - val_score: 0.4587\n",
      "Epoch 12/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 1.8449 - acc: 0.4958 - score: 0.5689\n",
      "Epoch 00012: val_score did not improve from 0.45869\n",
      "5831/5831 [==============================] - 1s 204us/sample - loss: 1.8458 - acc: 0.4958 - score: 0.5717 - val_loss: 2.4221 - val_acc: 0.3183 - val_score: 0.4067\n",
      "Epoch 13/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 1.8141 - acc: 0.5018 - score: 0.5724\n",
      "Epoch 00013: val_score did not improve from 0.45869\n",
      "5831/5831 [==============================] - 1s 202us/sample - loss: 1.8105 - acc: 0.5033 - score: 0.5696 - val_loss: 2.2196 - val_acc: 0.3600 - val_score: 0.4424\n",
      "Epoch 14/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.7744 - acc: 0.5094 - score: 0.5796\n",
      "Epoch 00014: val_score improved from 0.45869 to 0.47304, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 217us/sample - loss: 1.7766 - acc: 0.5080 - score: 0.5756 - val_loss: 2.2039 - val_acc: 0.3908 - val_score: 0.4730\n",
      "Epoch 15/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.8150 - acc: 0.5028 - score: 0.5742\n",
      "Epoch 00015: val_score did not improve from 0.47304\n",
      "5831/5831 [==============================] - 1s 204us/sample - loss: 1.8144 - acc: 0.5021 - score: 0.5724 - val_loss: 2.7626 - val_acc: 0.2888 - val_score: 0.3757\n",
      "Epoch 16/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 1.7949 - acc: 0.5095 - score: 0.5796\n",
      "Epoch 00016: val_score did not improve from 0.47304\n",
      "5831/5831 [==============================] - 1s 205us/sample - loss: 1.7934 - acc: 0.5099 - score: 0.5777 - val_loss: 2.7205 - val_acc: 0.2690 - val_score: 0.3495\n",
      "Epoch 17/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.7305 - acc: 0.5222 - score: 0.5926\n",
      "Epoch 00017: val_score did not improve from 0.47304\n",
      "5831/5831 [==============================] - 1s 206us/sample - loss: 1.7310 - acc: 0.5217 - score: 0.5875 - val_loss: 2.5105 - val_acc: 0.3279 - val_score: 0.4067\n",
      "Epoch 18/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.7142 - acc: 0.5261 - score: 0.5958\n",
      "Epoch 00018: val_score improved from 0.47304 to 0.56917, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 216us/sample - loss: 1.7144 - acc: 0.5262 - score: 0.5965 - val_loss: 1.7693 - val_acc: 0.4969 - val_score: 0.5692\n",
      "Epoch 19/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 1.6982 - acc: 0.5256 - score: 0.5957\n",
      "Epoch 00019: val_score did not improve from 0.56917\n",
      "5831/5831 [==============================] - 1s 202us/sample - loss: 1.7004 - acc: 0.5251 - score: 0.5898 - val_loss: 3.0223 - val_acc: 0.2642 - val_score: 0.3384\n",
      "Epoch 20/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.7285 - acc: 0.5211 - score: 0.5907\n",
      "Epoch 00020: val_score did not improve from 0.56917\n",
      "5831/5831 [==============================] - 1s 206us/sample - loss: 1.7295 - acc: 0.5208 - score: 0.5888 - val_loss: 2.6192 - val_acc: 0.2793 - val_score: 0.3529\n",
      "Epoch 21/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.6824 - acc: 0.5355 - score: 0.6036\n",
      "Epoch 00021: val_score did not improve from 0.56917\n",
      "5831/5831 [==============================] - 1s 210us/sample - loss: 1.6800 - acc: 0.5363 - score: 0.6035 - val_loss: 2.2533 - val_acc: 0.3867 - val_score: 0.4625\n",
      "Epoch 22/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 1.6903 - acc: 0.5399 - score: 0.6067\n",
      "Epoch 00022: val_score did not improve from 0.56917\n",
      "5831/5831 [==============================] - 1s 203us/sample - loss: 1.6905 - acc: 0.5406 - score: 0.6059 - val_loss: 2.8491 - val_acc: 0.3169 - val_score: 0.3907\n",
      "Epoch 23/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 1.6550 - acc: 0.5408 - score: 0.6090\n",
      "Epoch 00023: val_score did not improve from 0.56917\n",
      "5831/5831 [==============================] - 1s 202us/sample - loss: 1.6537 - acc: 0.5423 - score: 0.6117 - val_loss: 2.1547 - val_acc: 0.4285 - val_score: 0.5041\n",
      "Epoch 24/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.6221 - acc: 0.5511 - score: 0.6164\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00024: val_score did not improve from 0.56917\n",
      "5831/5831 [==============================] - 1s 222us/sample - loss: 1.6265 - acc: 0.5481 - score: 0.6118 - val_loss: 1.9514 - val_acc: 0.4422 - val_score: 0.5222\n",
      "Epoch 25/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.5306 - acc: 0.5738 - score: 0.6366\n",
      "Epoch 00025: val_score did not improve from 0.56917\n",
      "5831/5831 [==============================] - 1s 209us/sample - loss: 1.5306 - acc: 0.5737 - score: 0.6358 - val_loss: 2.2006 - val_acc: 0.3847 - val_score: 0.4609\n",
      "Epoch 26/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.4623 - acc: 0.5948 - score: 0.6550\n",
      "Epoch 00026: val_score did not improve from 0.56917\n",
      "5831/5831 [==============================] - 1s 212us/sample - loss: 1.4628 - acc: 0.5946 - score: 0.6535 - val_loss: 1.8871 - val_acc: 0.4798 - val_score: 0.5531\n",
      "Epoch 27/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.4484 - acc: 0.5943 - score: 0.6549\n",
      "Epoch 00027: val_score improved from 0.56917 to 0.60471, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 218us/sample - loss: 1.4470 - acc: 0.5942 - score: 0.6534 - val_loss: 1.6552 - val_acc: 0.5298 - val_score: 0.6047\n",
      "Epoch 28/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.4401 - acc: 0.5950 - score: 0.6567\n",
      "Epoch 00028: val_score did not improve from 0.60471\n",
      "5831/5831 [==============================] - 1s 202us/sample - loss: 1.4411 - acc: 0.5949 - score: 0.6552 - val_loss: 1.9466 - val_acc: 0.4716 - val_score: 0.5315\n",
      "Epoch 29/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.3899 - acc: 0.6215 - score: 0.6785\n",
      "Epoch 00029: val_score did not improve from 0.60471\n",
      "5831/5831 [==============================] - 1s 204us/sample - loss: 1.3909 - acc: 0.6205 - score: 0.6738 - val_loss: 1.8108 - val_acc: 0.4819 - val_score: 0.5533\n",
      "Epoch 30/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.4288 - acc: 0.6076 - score: 0.6657\n",
      "Epoch 00030: val_score improved from 0.60471 to 0.61264, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 222us/sample - loss: 1.4310 - acc: 0.6066 - score: 0.6586 - val_loss: 1.6477 - val_acc: 0.5503 - val_score: 0.6126\n",
      "Epoch 31/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.4178 - acc: 0.6087 - score: 0.6664\n",
      "Epoch 00031: val_score did not improve from 0.61264\n",
      "5831/5831 [==============================] - 1s 204us/sample - loss: 1.4189 - acc: 0.6086 - score: 0.6658 - val_loss: 1.7115 - val_acc: 0.5243 - val_score: 0.5831\n",
      "Epoch 32/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.3678 - acc: 0.6295 - score: 0.6850\n",
      "Epoch 00032: val_score did not improve from 0.61264\n",
      "5831/5831 [==============================] - 1s 210us/sample - loss: 1.3683 - acc: 0.6292 - score: 0.6831 - val_loss: 1.9796 - val_acc: 0.4381 - val_score: 0.5016\n",
      "Epoch 33/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.3599 - acc: 0.6213 - score: 0.6777\n",
      "Epoch 00033: val_score did not improve from 0.61264\n",
      "5831/5831 [==============================] - 1s 208us/sample - loss: 1.3605 - acc: 0.6217 - score: 0.6764 - val_loss: 2.2654 - val_acc: 0.4483 - val_score: 0.5193\n",
      "Epoch 34/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.3483 - acc: 0.6291 - score: 0.6864\n",
      "Epoch 00034: val_score did not improve from 0.61264\n",
      "5831/5831 [==============================] - 1s 206us/sample - loss: 1.3499 - acc: 0.6294 - score: 0.6858 - val_loss: 2.6766 - val_acc: 0.3744 - val_score: 0.4444\n",
      "Epoch 35/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.3497 - acc: 0.6236 - score: 0.6812\n",
      "Epoch 00035: val_score did not improve from 0.61264\n",
      "5831/5831 [==============================] - 1s 213us/sample - loss: 1.3504 - acc: 0.6234 - score: 0.6795 - val_loss: 1.7656 - val_acc: 0.4949 - val_score: 0.5683\n",
      "Epoch 36/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.3191 - acc: 0.6358 - score: 0.6908\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00036: val_score did not improve from 0.61264\n",
      "5831/5831 [==============================] - 1s 204us/sample - loss: 1.3200 - acc: 0.6354 - score: 0.6876 - val_loss: 2.0363 - val_acc: 0.4182 - val_score: 0.4871\n",
      "Epoch 37/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 1.2328 - acc: 0.6692 - score: 0.7189\n",
      "Epoch 00037: val_score improved from 0.61264 to 0.68568, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 220us/sample - loss: 1.2304 - acc: 0.6706 - score: 0.7184 - val_loss: 1.4819 - val_acc: 0.6338 - val_score: 0.6857\n",
      "Epoch 38/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.1885 - acc: 0.6837 - score: 0.7329\n",
      "Epoch 00038: val_score improved from 0.68568 to 0.69126, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 217us/sample - loss: 1.1895 - acc: 0.6832 - score: 0.7290 - val_loss: 1.3733 - val_acc: 0.6345 - val_score: 0.6913\n",
      "Epoch 39/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.1720 - acc: 0.6838 - score: 0.7336\n",
      "Epoch 00039: val_score improved from 0.69126 to 0.71144, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 216us/sample - loss: 1.1711 - acc: 0.6839 - score: 0.7309 - val_loss: 1.3212 - val_acc: 0.6578 - val_score: 0.7114\n",
      "Epoch 40/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.1634 - acc: 0.6911 - score: 0.7389\n",
      "Epoch 00040: val_score did not improve from 0.71144\n",
      "5831/5831 [==============================] - 1s 209us/sample - loss: 1.1671 - acc: 0.6896 - score: 0.7344 - val_loss: 1.3197 - val_acc: 0.6297 - val_score: 0.6844\n",
      "Epoch 41/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 1.1434 - acc: 0.6900 - score: 0.7395\n",
      "Epoch 00041: val_score did not improve from 0.71144\n",
      "5831/5831 [==============================] - 1s 207us/sample - loss: 1.1502 - acc: 0.6868 - score: 0.7319 - val_loss: 1.7710 - val_acc: 0.5380 - val_score: 0.5944\n",
      "Epoch 42/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 1.1439 - acc: 0.6924 - score: 0.7399\n",
      "Epoch 00042: val_score did not improve from 0.71144\n",
      "5831/5831 [==============================] - 1s 209us/sample - loss: 1.1468 - acc: 0.6913 - score: 0.7390 - val_loss: 1.3487 - val_acc: 0.6454 - val_score: 0.6973\n",
      "Epoch 43/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 1.1199 - acc: 0.7079 - score: 0.7536\n",
      "Epoch 00043: val_score did not improve from 0.71144\n",
      "5831/5831 [==============================] - 1s 205us/sample - loss: 1.1204 - acc: 0.7067 - score: 0.7479 - val_loss: 1.3430 - val_acc: 0.6516 - val_score: 0.7002\n",
      "Epoch 44/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 1.1200 - acc: 0.7068 - score: 0.7514\n",
      "Epoch 00044: val_score did not improve from 0.71144\n",
      "5831/5831 [==============================] - 1s 209us/sample - loss: 1.1171 - acc: 0.7085 - score: 0.7491 - val_loss: 1.5624 - val_acc: 0.5633 - val_score: 0.6237\n",
      "Epoch 45/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.1022 - acc: 0.7060 - score: 0.7521\n",
      "Epoch 00045: val_score improved from 0.71144 to 0.71407, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 218us/sample - loss: 1.1035 - acc: 0.7066 - score: 0.7523 - val_loss: 1.1873 - val_acc: 0.6687 - val_score: 0.7141\n",
      "Epoch 46/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.0795 - acc: 0.7209 - score: 0.7640\n",
      "Epoch 00046: val_score did not improve from 0.71407\n",
      "5831/5831 [==============================] - 1s 210us/sample - loss: 1.0821 - acc: 0.7194 - score: 0.7594 - val_loss: 1.3973 - val_acc: 0.6283 - val_score: 0.6879\n",
      "Epoch 47/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.0902 - acc: 0.7126 - score: 0.7575\n",
      "Epoch 00047: val_score did not improve from 0.71407\n",
      "5831/5831 [==============================] - 1s 207us/sample - loss: 1.0902 - acc: 0.7129 - score: 0.7601 - val_loss: 1.4017 - val_acc: 0.6153 - val_score: 0.6677\n",
      "Epoch 48/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 1.0766 - acc: 0.7141 - score: 0.7587\n",
      "Epoch 00048: val_score did not improve from 0.71407\n",
      "5831/5831 [==============================] - 1s 203us/sample - loss: 1.0738 - acc: 0.7155 - score: 0.7581 - val_loss: 1.4172 - val_acc: 0.6071 - val_score: 0.6645\n",
      "Epoch 49/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.0610 - acc: 0.7273 - score: 0.7684\n",
      "Epoch 00049: val_score improved from 0.71407 to 0.71788, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 215us/sample - loss: 1.0629 - acc: 0.7268 - score: 0.7644 - val_loss: 1.2431 - val_acc: 0.6715 - val_score: 0.7179\n",
      "Epoch 50/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 1.0720 - acc: 0.7188 - score: 0.7622\n",
      "Epoch 00050: val_score did not improve from 0.71788\n",
      "5831/5831 [==============================] - 1s 206us/sample - loss: 1.0735 - acc: 0.7184 - score: 0.7596 - val_loss: 1.2840 - val_acc: 0.6564 - val_score: 0.7056\n",
      "Epoch 51/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.0283 - acc: 0.7365 - score: 0.7779\n",
      "Epoch 00051: val_score did not improve from 0.71788\n",
      "5831/5831 [==============================] - 1s 206us/sample - loss: 1.0306 - acc: 0.7352 - score: 0.7738 - val_loss: 1.3206 - val_acc: 0.6571 - val_score: 0.7049\n",
      "Epoch 52/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 1.0261 - acc: 0.7340 - score: 0.7755\n",
      "Epoch 00052: val_score did not improve from 0.71788\n",
      "5831/5831 [==============================] - 1s 208us/sample - loss: 1.0265 - acc: 0.7340 - score: 0.7739 - val_loss: 1.2746 - val_acc: 0.6537 - val_score: 0.7031\n",
      "Epoch 53/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.9905 - acc: 0.7507 - score: 0.7893\n",
      "Epoch 00053: val_score did not improve from 0.71788\n",
      "5831/5831 [==============================] - 1s 206us/sample - loss: 0.9922 - acc: 0.7491 - score: 0.7853 - val_loss: 1.3856 - val_acc: 0.6407 - val_score: 0.6878\n",
      "Epoch 54/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 1.0117 - acc: 0.7468 - score: 0.7859\n",
      "Epoch 00054: val_score did not improve from 0.71788\n",
      "5831/5831 [==============================] - 1s 210us/sample - loss: 1.0126 - acc: 0.7460 - score: 0.7851 - val_loss: 1.6440 - val_acc: 0.5667 - val_score: 0.6280\n",
      "Epoch 55/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.9908 - acc: 0.7557 - score: 0.7948\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00055: val_score did not improve from 0.71788\n",
      "5831/5831 [==============================] - 1s 210us/sample - loss: 0.9890 - acc: 0.7561 - score: 0.7961 - val_loss: 1.2690 - val_acc: 0.6543 - val_score: 0.7060\n",
      "Epoch 56/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 0.9164 - acc: 0.7827 - score: 0.8166\n",
      "Epoch 00056: val_score improved from 0.71788 to 0.75951, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 219us/sample - loss: 0.9167 - acc: 0.7815 - score: 0.8136 - val_loss: 1.1136 - val_acc: 0.7139 - val_score: 0.7595\n",
      "Epoch 57/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 0.8750 - acc: 0.7912 - score: 0.8239\n",
      "Epoch 00057: val_score improved from 0.75951 to 0.76341, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 219us/sample - loss: 0.8782 - acc: 0.7892 - score: 0.8176 - val_loss: 1.0686 - val_acc: 0.7228 - val_score: 0.7634\n",
      "Epoch 58/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.8704 - acc: 0.7948 - score: 0.8264\n",
      "Epoch 00058: val_score improved from 0.76341 to 0.77542, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 216us/sample - loss: 0.8730 - acc: 0.7940 - score: 0.8207 - val_loss: 1.0219 - val_acc: 0.7379 - val_score: 0.7754\n",
      "Epoch 59/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 0.8988 - acc: 0.7805 - score: 0.8147\n",
      "Epoch 00059: val_score did not improve from 0.77542\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.8996 - acc: 0.7810 - score: 0.8123 - val_loss: 1.0649 - val_acc: 0.7194 - val_score: 0.7599\n",
      "Epoch 60/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.8637 - acc: 0.7979 - score: 0.8297\n",
      "Epoch 00060: val_score improved from 0.77542 to 0.78346, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 212us/sample - loss: 0.8657 - acc: 0.7978 - score: 0.8241 - val_loss: 1.0354 - val_acc: 0.7474 - val_score: 0.7835\n",
      "Epoch 61/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.8390 - acc: 0.8035 - score: 0.8346\n",
      "Epoch 00061: val_score did not improve from 0.78346\n",
      "5831/5831 [==============================] - 1s 208us/sample - loss: 0.8414 - acc: 0.8021 - score: 0.8283 - val_loss: 1.1079 - val_acc: 0.7091 - val_score: 0.7499\n",
      "Epoch 62/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.8224 - acc: 0.8100 - score: 0.8397\n",
      "Epoch 00062: val_score did not improve from 0.78346\n",
      "5831/5831 [==============================] - 1s 203us/sample - loss: 0.8264 - acc: 0.8084 - score: 0.8348 - val_loss: 1.0245 - val_acc: 0.7406 - val_score: 0.7739\n",
      "Epoch 63/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 0.8134 - acc: 0.8185 - score: 0.8471\n",
      "Epoch 00063: val_score did not improve from 0.78346\n",
      "5831/5831 [==============================] - 1s 203us/sample - loss: 0.8194 - acc: 0.8162 - score: 0.8415 - val_loss: 1.0386 - val_acc: 0.7344 - val_score: 0.7762\n",
      "Epoch 64/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 0.8256 - acc: 0.8089 - score: 0.8391\n",
      "Epoch 00064: val_score did not improve from 0.78346\n",
      "5831/5831 [==============================] - 1s 202us/sample - loss: 0.8259 - acc: 0.8084 - score: 0.8370 - val_loss: 1.0649 - val_acc: 0.7255 - val_score: 0.7625\n",
      "Epoch 65/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.7863 - acc: 0.8238 - score: 0.8520\n",
      "Epoch 00065: val_score did not improve from 0.78346\n",
      "5831/5831 [==============================] - 1s 205us/sample - loss: 0.7878 - acc: 0.8227 - score: 0.8485 - val_loss: 1.0631 - val_acc: 0.7276 - val_score: 0.7655\n",
      "Epoch 66/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.7900 - acc: 0.8179 - score: 0.8466\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00066: val_score did not improve from 0.78346\n",
      "5831/5831 [==============================] - 1s 204us/sample - loss: 0.7917 - acc: 0.8175 - score: 0.8455 - val_loss: 1.0258 - val_acc: 0.7399 - val_score: 0.7756\n",
      "Epoch 67/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.7553 - acc: 0.8332 - score: 0.8585\n",
      "Epoch 00067: val_score improved from 0.78346 to 0.81004, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 216us/sample - loss: 0.7568 - acc: 0.8330 - score: 0.8576 - val_loss: 0.8921 - val_acc: 0.7796 - val_score: 0.8100\n",
      "Epoch 68/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.7325 - acc: 0.8444 - score: 0.8679\n",
      "Epoch 00068: val_score improved from 0.81004 to 0.81214, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 219us/sample - loss: 0.7350 - acc: 0.8431 - score: 0.8630 - val_loss: 0.9224 - val_acc: 0.7803 - val_score: 0.8121\n",
      "Epoch 69/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.7229 - acc: 0.8483 - score: 0.8720\n",
      "Epoch 00069: val_score did not improve from 0.81214\n",
      "5831/5831 [==============================] - 1s 201us/sample - loss: 0.7231 - acc: 0.8486 - score: 0.8702 - val_loss: 0.9375 - val_acc: 0.7769 - val_score: 0.8093\n",
      "Epoch 70/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5632/5831 [===========================>..] - ETA: 0s - loss: 0.7143 - acc: 0.8477 - score: 0.8716\n",
      "Epoch 00070: val_score did not improve from 0.81214\n",
      "5831/5831 [==============================] - 1s 206us/sample - loss: 0.7170 - acc: 0.8463 - score: 0.8696 - val_loss: 0.9456 - val_acc: 0.7591 - val_score: 0.7945\n",
      "Epoch 71/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.7016 - acc: 0.8583 - score: 0.8806\n",
      "Epoch 00071: val_score did not improve from 0.81214\n",
      "5831/5831 [==============================] - 1s 211us/sample - loss: 0.7030 - acc: 0.8580 - score: 0.8773 - val_loss: 0.8859 - val_acc: 0.7803 - val_score: 0.8118\n",
      "Epoch 72/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.7062 - acc: 0.8553 - score: 0.8778\n",
      "Epoch 00072: val_score improved from 0.81214 to 0.82140, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 215us/sample - loss: 0.7064 - acc: 0.8561 - score: 0.8771 - val_loss: 0.9047 - val_acc: 0.7912 - val_score: 0.8214\n",
      "Epoch 73/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.6781 - acc: 0.8628 - score: 0.8836\n",
      "Epoch 00073: val_score did not improve from 0.82140\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.6807 - acc: 0.8628 - score: 0.8818 - val_loss: 0.9573 - val_acc: 0.7598 - val_score: 0.7947\n",
      "Epoch 74/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.6908 - acc: 0.8559 - score: 0.8794\n",
      "Epoch 00074: val_score did not improve from 0.82140\n",
      "5831/5831 [==============================] - 1s 201us/sample - loss: 0.6901 - acc: 0.8561 - score: 0.8756 - val_loss: 0.8897 - val_acc: 0.7817 - val_score: 0.8140\n",
      "Epoch 75/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 0.6688 - acc: 0.8677 - score: 0.8887\n",
      "Epoch 00075: val_score did not improve from 0.82140\n",
      "5831/5831 [==============================] - 1s 199us/sample - loss: 0.6713 - acc: 0.8666 - score: 0.8837 - val_loss: 0.8947 - val_acc: 0.7864 - val_score: 0.8182\n",
      "Epoch 76/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.6711 - acc: 0.8645 - score: 0.8866\n",
      "Epoch 00076: val_score did not improve from 0.82140\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.6702 - acc: 0.8647 - score: 0.8817 - val_loss: 0.8800 - val_acc: 0.7830 - val_score: 0.8140\n",
      "Epoch 77/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.6603 - acc: 0.8716 - score: 0.8916\n",
      "Epoch 00077: val_score did not improve from 0.82140\n",
      "5831/5831 [==============================] - 1s 199us/sample - loss: 0.6623 - acc: 0.8707 - score: 0.8846 - val_loss: 0.9095 - val_acc: 0.7803 - val_score: 0.8122\n",
      "Epoch 78/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 0.6655 - acc: 0.8668 - score: 0.8876\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00078: val_score did not improve from 0.82140\n",
      "5831/5831 [==============================] - 1s 202us/sample - loss: 0.6663 - acc: 0.8667 - score: 0.8833 - val_loss: 0.9232 - val_acc: 0.7687 - val_score: 0.8021\n",
      "Epoch 79/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 0.6281 - acc: 0.8786 - score: 0.8973\n",
      "Epoch 00079: val_score did not improve from 0.82140\n",
      "5831/5831 [==============================] - 1s 203us/sample - loss: 0.6324 - acc: 0.8764 - score: 0.8939 - val_loss: 0.8663 - val_acc: 0.7878 - val_score: 0.8185\n",
      "Epoch 80/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.6177 - acc: 0.8852 - score: 0.9042\n",
      "Epoch 00080: val_score improved from 0.82140 to 0.82993, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 211us/sample - loss: 0.6192 - acc: 0.8846 - score: 0.8973 - val_loss: 0.8479 - val_acc: 0.8022 - val_score: 0.8299\n",
      "Epoch 81/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.6308 - acc: 0.8840 - score: 0.9022\n",
      "Epoch 00081: val_score did not improve from 0.82993\n",
      "5831/5831 [==============================] - 1s 199us/sample - loss: 0.6321 - acc: 0.8836 - score: 0.8961 - val_loss: 0.8571 - val_acc: 0.7953 - val_score: 0.8243\n",
      "Epoch 82/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.6181 - acc: 0.8887 - score: 0.9068\n",
      "Epoch 00082: val_score improved from 0.82993 to 0.83497, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 212us/sample - loss: 0.6206 - acc: 0.8882 - score: 0.9038 - val_loss: 0.8468 - val_acc: 0.8084 - val_score: 0.8350\n",
      "Epoch 83/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.6154 - acc: 0.8873 - score: 0.9055\n",
      "Epoch 00083: val_score did not improve from 0.83497\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.6171 - acc: 0.8863 - score: 0.9027 - val_loss: 0.8502 - val_acc: 0.7967 - val_score: 0.8263\n",
      "Epoch 84/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.6108 - acc: 0.8845 - score: 0.9025\n",
      "Epoch 00084: val_score did not improve from 0.83497\n",
      "5831/5831 [==============================] - 1s 199us/sample - loss: 0.6115 - acc: 0.8846 - score: 0.8976 - val_loss: 0.8548 - val_acc: 0.7995 - val_score: 0.8292\n",
      "Epoch 85/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5980 - acc: 0.8920 - score: 0.9092\n",
      "Epoch 00085: val_score improved from 0.83497 to 0.83590, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 208us/sample - loss: 0.5994 - acc: 0.8916 - score: 0.9066 - val_loss: 0.8586 - val_acc: 0.8084 - val_score: 0.8359\n",
      "Epoch 86/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.6111 - acc: 0.8834 - score: 0.9029\n",
      "Epoch 00086: val_score did not improve from 0.83590\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.6121 - acc: 0.8837 - score: 0.9012 - val_loss: 0.8365 - val_acc: 0.8015 - val_score: 0.8299\n",
      "Epoch 87/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5957 - acc: 0.8964 - score: 0.9127\n",
      "Epoch 00087: val_score did not improve from 0.83590\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.5965 - acc: 0.8961 - score: 0.9108 - val_loss: 0.8396 - val_acc: 0.8077 - val_score: 0.8351\n",
      "Epoch 88/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5913 - acc: 0.8934 - score: 0.9110\n",
      "Epoch 00088: val_score did not improve from 0.83590\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.5929 - acc: 0.8926 - score: 0.9057 - val_loss: 0.8377 - val_acc: 0.8042 - val_score: 0.8334\n",
      "Epoch 89/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5803 - acc: 0.9034 - score: 0.9182\n",
      "Epoch 00089: val_score did not improve from 0.83590\n",
      "5831/5831 [==============================] - 1s 199us/sample - loss: 0.5826 - acc: 0.9016 - score: 0.9119 - val_loss: 0.8416 - val_acc: 0.8070 - val_score: 0.8348\n",
      "Epoch 90/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 0.5750 - acc: 0.9023 - score: 0.9175\n",
      "Epoch 00090: val_score did not improve from 0.83590\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.5764 - acc: 0.9012 - score: 0.9111 - val_loss: 0.8632 - val_acc: 0.7960 - val_score: 0.8250\n",
      "Epoch 91/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5809 - acc: 0.8951 - score: 0.9112\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00091: val_score did not improve from 0.83590\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.5826 - acc: 0.8947 - score: 0.9078 - val_loss: 0.8754 - val_acc: 0.7912 - val_score: 0.8208\n",
      "Epoch 92/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5655 - acc: 0.9045 - score: 0.9193\n",
      "Epoch 00092: val_score improved from 0.83590 to 0.83736, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 211us/sample - loss: 0.5657 - acc: 0.9045 - score: 0.9192 - val_loss: 0.8141 - val_acc: 0.8111 - val_score: 0.8374\n",
      "Epoch 93/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5614 - acc: 0.9045 - score: 0.9198\n",
      "Epoch 00093: val_score did not improve from 0.83736\n",
      "5831/5831 [==============================] - 1s 201us/sample - loss: 0.5618 - acc: 0.9043 - score: 0.9169 - val_loss: 0.8142 - val_acc: 0.8084 - val_score: 0.8369\n",
      "Epoch 94/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.9127 - score: 0.9268\n",
      "Epoch 00094: val_score improved from 0.83736 to 0.84016, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 209us/sample - loss: 0.5437 - acc: 0.9129 - score: 0.9267 - val_loss: 0.8126 - val_acc: 0.8138 - val_score: 0.8402\n",
      "Epoch 95/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.9045 - score: 0.9200\n",
      "Epoch 00095: val_score did not improve from 0.84016\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.5646 - acc: 0.9043 - score: 0.9178 - val_loss: 0.8239 - val_acc: 0.8084 - val_score: 0.8366\n",
      "Epoch 96/300\n",
      "5632/5831 [===========================>..] - ETA: 0s - loss: 0.5551 - acc: 0.9096 - score: 0.9236\n",
      "Epoch 00096: val_score did not improve from 0.84016\n",
      "5831/5831 [==============================] - 1s 203us/sample - loss: 0.5566 - acc: 0.9086 - score: 0.9184 - val_loss: 0.8192 - val_acc: 0.8084 - val_score: 0.8352\n",
      "Epoch 97/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 0.5463 - acc: 0.9084 - score: 0.9232\n",
      "Epoch 00097: val_score did not improve from 0.84016\n",
      "5831/5831 [==============================] - 1s 201us/sample - loss: 0.5468 - acc: 0.9086 - score: 0.9206 - val_loss: 0.8247 - val_acc: 0.8042 - val_score: 0.8322\n",
      "Epoch 98/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.9175 - score: 0.9308\n",
      "Epoch 00098: val_score improved from 0.84016 to 0.84078, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 212us/sample - loss: 0.5453 - acc: 0.9160 - score: 0.9245 - val_loss: 0.8048 - val_acc: 0.8138 - val_score: 0.8408\n",
      "Epoch 99/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5525 - acc: 0.9122 - score: 0.9257\n",
      "Epoch 00099: val_score improved from 0.84078 to 0.84131, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 210us/sample - loss: 0.5530 - acc: 0.9115 - score: 0.9223 - val_loss: 0.8215 - val_acc: 0.8138 - val_score: 0.8413\n",
      "Epoch 100/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5447 - acc: 0.9126 - score: 0.9270\n",
      "Epoch 00100: val_score did not improve from 0.84131\n",
      "5831/5831 [==============================] - 1s 198us/sample - loss: 0.5453 - acc: 0.9124 - score: 0.9249 - val_loss: 0.8165 - val_acc: 0.8138 - val_score: 0.8408\n",
      "Epoch 101/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5614 - acc: 0.9047 - score: 0.9195\n",
      "Epoch 00101: val_score did not improve from 0.84131\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.5625 - acc: 0.9043 - score: 0.9162 - val_loss: 0.8138 - val_acc: 0.8104 - val_score: 0.8377\n",
      "Epoch 102/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.9100 - score: 0.9249\n",
      "Epoch 00102: val_score did not improve from 0.84131\n",
      "5831/5831 [==============================] - 1s 198us/sample - loss: 0.5530 - acc: 0.9098 - score: 0.9234 - val_loss: 0.8305 - val_acc: 0.8111 - val_score: 0.8392\n",
      "Epoch 103/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.9128 - score: 0.9260\n",
      "Epoch 00103: val_score did not improve from 0.84131\n",
      "5831/5831 [==============================] - 1s 198us/sample - loss: 0.5474 - acc: 0.9124 - score: 0.9223 - val_loss: 0.8160 - val_acc: 0.8077 - val_score: 0.8355\n",
      "Epoch 104/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.9140 - score: 0.9270\n",
      "Epoch 00104: val_score did not improve from 0.84131\n",
      "5831/5831 [==============================] - 1s 199us/sample - loss: 0.5335 - acc: 0.9144 - score: 0.9223 - val_loss: 0.8115 - val_acc: 0.8131 - val_score: 0.8390\n",
      "Epoch 105/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.9115 - score: 0.9251\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00105: val_score did not improve from 0.84131\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.5446 - acc: 0.9110 - score: 0.9234 - val_loss: 0.8158 - val_acc: 0.8097 - val_score: 0.8381\n",
      "Epoch 106/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.9160 - score: 0.9297\n",
      "Epoch 00106: val_score did not improve from 0.84131\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.5392 - acc: 0.9158 - score: 0.9279 - val_loss: 0.8084 - val_acc: 0.8111 - val_score: 0.8390\n",
      "Epoch 107/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.9198 - score: 0.9331\n",
      "Epoch 00107: val_score improved from 0.84131 to 0.84511, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 211us/sample - loss: 0.5280 - acc: 0.9194 - score: 0.9302 - val_loss: 0.8042 - val_acc: 0.8172 - val_score: 0.8451\n",
      "Epoch 108/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.9194 - score: 0.9329\n",
      "Epoch 00108: val_score did not improve from 0.84511\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.5213 - acc: 0.9189 - score: 0.9281 - val_loss: 0.7994 - val_acc: 0.8145 - val_score: 0.8412\n",
      "Epoch 109/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.9127 - score: 0.9262\n",
      "Epoch 00109: val_score did not improve from 0.84511\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.5260 - acc: 0.9132 - score: 0.9235 - val_loss: 0.8046 - val_acc: 0.8138 - val_score: 0.8416\n",
      "Epoch 110/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 0.5129 - acc: 0.9230 - score: 0.9352\n",
      "Epoch 00110: val_score did not improve from 0.84511\n",
      "5831/5831 [==============================] - 1s 203us/sample - loss: 0.5164 - acc: 0.9218 - score: 0.9287 - val_loss: 0.8051 - val_acc: 0.8138 - val_score: 0.8417\n",
      "Epoch 111/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.9205 - score: 0.9334\n",
      "Epoch 00111: val_score did not improve from 0.84511\n",
      "5831/5831 [==============================] - 1s 202us/sample - loss: 0.5212 - acc: 0.9201 - score: 0.9302 - val_loss: 0.8058 - val_acc: 0.8166 - val_score: 0.8432\n",
      "Epoch 112/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5244 - acc: 0.9174 - score: 0.9304\n",
      "Epoch 00112: val_score did not improve from 0.84511\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.5256 - acc: 0.9173 - score: 0.9301 - val_loss: 0.8050 - val_acc: 0.8159 - val_score: 0.8439\n",
      "Epoch 113/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.9249 - score: 0.9372\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00113: val_score did not improve from 0.84511\n",
      "5831/5831 [==============================] - 1s 202us/sample - loss: 0.5144 - acc: 0.9245 - score: 0.9321 - val_loss: 0.8061 - val_acc: 0.8159 - val_score: 0.8421\n",
      "Epoch 114/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 0.5050 - acc: 0.9267 - score: 0.9385\n",
      "Epoch 00114: val_score did not improve from 0.84511\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.5099 - acc: 0.9242 - score: 0.9338 - val_loss: 0.8056 - val_acc: 0.8138 - val_score: 0.8407\n",
      "Epoch 115/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5137 - acc: 0.9221 - score: 0.9338\n",
      "Epoch 00115: val_score did not improve from 0.84511\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.5139 - acc: 0.9218 - score: 0.9299 - val_loss: 0.8012 - val_acc: 0.8159 - val_score: 0.8426\n",
      "Epoch 116/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.9231 - score: 0.9353\n",
      "Epoch 00116: val_score improved from 0.84511 to 0.84525, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 208us/sample - loss: 0.5130 - acc: 0.9227 - score: 0.9320 - val_loss: 0.7983 - val_acc: 0.8179 - val_score: 0.8452\n",
      "Epoch 117/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.9194 - score: 0.9317\n",
      "Epoch 00117: val_score did not improve from 0.84525\n",
      "5831/5831 [==============================] - 1s 206us/sample - loss: 0.5182 - acc: 0.9191 - score: 0.9287 - val_loss: 0.8001 - val_acc: 0.8145 - val_score: 0.8411\n",
      "Epoch 118/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.9241 - score: 0.9364\n",
      "Epoch 00118: val_score did not improve from 0.84525\n",
      "5831/5831 [==============================] - 1s 196us/sample - loss: 0.5106 - acc: 0.9240 - score: 0.9355 - val_loss: 0.8035 - val_acc: 0.8152 - val_score: 0.8429\n",
      "Epoch 119/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5080 - acc: 0.9241 - score: 0.9365\n",
      "Epoch 00119: val_score did not improve from 0.84525\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.5086 - acc: 0.9239 - score: 0.9343 - val_loss: 0.8048 - val_acc: 0.8131 - val_score: 0.8396\n",
      "Epoch 120/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.9191 - score: 0.9319\n",
      "Epoch 00120: val_score did not improve from 0.84525\n",
      "5831/5831 [==============================] - 1s 198us/sample - loss: 0.5226 - acc: 0.9191 - score: 0.9313 - val_loss: 0.8017 - val_acc: 0.8138 - val_score: 0.8414\n",
      "Epoch 121/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.4960 - acc: 0.9304 - score: 0.9409\n",
      "Epoch 00121: val_score did not improve from 0.84525\n",
      "5831/5831 [==============================] - 1s 198us/sample - loss: 0.4964 - acc: 0.9304 - score: 0.9403 - val_loss: 0.8019 - val_acc: 0.8118 - val_score: 0.8394\n",
      "Epoch 122/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.9275 - score: 0.9386\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00122: val_score did not improve from 0.84525\n",
      "5831/5831 [==============================] - 1s 198us/sample - loss: 0.5143 - acc: 0.9261 - score: 0.9343 - val_loss: 0.8015 - val_acc: 0.8138 - val_score: 0.8400\n",
      "Epoch 123/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.9243 - score: 0.9352\n",
      "Epoch 00123: val_score did not improve from 0.84525\n",
      "5831/5831 [==============================] - 1s 198us/sample - loss: 0.5098 - acc: 0.9244 - score: 0.9336 - val_loss: 0.8001 - val_acc: 0.8166 - val_score: 0.8438\n",
      "Epoch 124/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.9253 - score: 0.9376\n",
      "Epoch 00124: val_score did not improve from 0.84525\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.5105 - acc: 0.9245 - score: 0.9326 - val_loss: 0.8001 - val_acc: 0.8118 - val_score: 0.8400\n",
      "Epoch 125/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.9196 - score: 0.9315\n",
      "Epoch 00125: val_score did not improve from 0.84525\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.5073 - acc: 0.9194 - score: 0.9282 - val_loss: 0.7969 - val_acc: 0.8159 - val_score: 0.8427\n",
      "Epoch 126/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.9275 - score: 0.9388\n",
      "Epoch 00126: val_score did not improve from 0.84525\n",
      "5831/5831 [==============================] - 1s 195us/sample - loss: 0.5001 - acc: 0.9273 - score: 0.9369 - val_loss: 0.7952 - val_acc: 0.8152 - val_score: 0.8431\n",
      "Epoch 127/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.9255 - score: 0.9369\n",
      "Epoch 00127: val_score improved from 0.84525 to 0.84586, saving model to fold1.h5\n",
      "5831/5831 [==============================] - 1s 207us/sample - loss: 0.5111 - acc: 0.9252 - score: 0.9347 - val_loss: 0.7969 - val_acc: 0.8186 - val_score: 0.8459\n",
      "Epoch 128/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4990 - acc: 0.9310 - score: 0.9415\n",
      "Epoch 00128: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 195us/sample - loss: 0.4997 - acc: 0.9304 - score: 0.9367 - val_loss: 0.7980 - val_acc: 0.8172 - val_score: 0.8437\n",
      "Epoch 129/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5046 - acc: 0.9264 - score: 0.9385\n",
      "Epoch 00129: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 200us/sample - loss: 0.5034 - acc: 0.9269 - score: 0.9374 - val_loss: 0.7964 - val_acc: 0.8186 - val_score: 0.8450\n",
      "Epoch 130/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.9251 - score: 0.9375\n",
      "Epoch 00130: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.5127 - acc: 0.9251 - score: 0.9372 - val_loss: 0.7978 - val_acc: 0.8159 - val_score: 0.8426\n",
      "Epoch 131/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.9236 - score: 0.9362\n",
      "Epoch 00131: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 196us/sample - loss: 0.5050 - acc: 0.9232 - score: 0.9364 - val_loss: 0.7994 - val_acc: 0.8159 - val_score: 0.8423\n",
      "Epoch 132/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.9217 - score: 0.9332\n",
      "Epoch 00132: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 195us/sample - loss: 0.5126 - acc: 0.9211 - score: 0.9285 - val_loss: 0.7977 - val_acc: 0.8172 - val_score: 0.8440\n",
      "Epoch 133/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.9224 - score: 0.9345\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\n",
      "Epoch 00133: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 198us/sample - loss: 0.5128 - acc: 0.9220 - score: 0.9310 - val_loss: 0.7988 - val_acc: 0.8159 - val_score: 0.8421\n",
      "Epoch 134/300\n",
      "5568/5831 [===========================>..] - ETA: 0s - loss: 0.4972 - acc: 0.9296 - score: 0.9401\n",
      "Epoch 00134: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 199us/sample - loss: 0.4993 - acc: 0.9285 - score: 0.9377 - val_loss: 0.7980 - val_acc: 0.8152 - val_score: 0.8425\n",
      "Epoch 135/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.9269 - score: 0.9378\n",
      "Epoch 00135: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 196us/sample - loss: 0.5038 - acc: 0.9263 - score: 0.9323 - val_loss: 0.7977 - val_acc: 0.8145 - val_score: 0.8415\n",
      "Epoch 136/300\n",
      "5696/5831 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.9249 - score: 0.9366\n",
      "Epoch 00136: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 199us/sample - loss: 0.5112 - acc: 0.9256 - score: 0.9379 - val_loss: 0.7980 - val_acc: 0.8166 - val_score: 0.8433\n",
      "Epoch 137/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.9248 - score: 0.9366\n",
      "Epoch 00137: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 198us/sample - loss: 0.5108 - acc: 0.9245 - score: 0.9306 - val_loss: 0.7995 - val_acc: 0.8159 - val_score: 0.8424\n",
      "Epoch 138/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.9238 - score: 0.9359\n",
      "Epoch 00138: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 195us/sample - loss: 0.5102 - acc: 0.9232 - score: 0.9311 - val_loss: 0.7983 - val_acc: 0.8172 - val_score: 0.8442\n",
      "Epoch 139/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4958 - acc: 0.9294 - score: 0.9408\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\n",
      "Epoch 00139: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 195us/sample - loss: 0.4965 - acc: 0.9292 - score: 0.9385 - val_loss: 0.7979 - val_acc: 0.8159 - val_score: 0.8429\n",
      "Epoch 140/300\n",
      "5824/5831 [============================>.] - ETA: 0s - loss: 0.4940 - acc: 0.9320 - score: 0.9421\n",
      "Epoch 00140: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 195us/sample - loss: 0.4950 - acc: 0.9316 - score: 0.9385 - val_loss: 0.7981 - val_acc: 0.8145 - val_score: 0.8409\n",
      "Epoch 141/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.9241 - score: 0.9352\n",
      "Epoch 00141: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 194us/sample - loss: 0.5053 - acc: 0.9240 - score: 0.9320 - val_loss: 0.7982 - val_acc: 0.8152 - val_score: 0.8422\n",
      "Epoch 142/300\n",
      "5760/5831 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.9250 - score: 0.9362\n",
      "Epoch 00142: val_score did not improve from 0.84586\n",
      "5831/5831 [==============================] - 1s 197us/sample - loss: 0.5064 - acc: 0.9244 - score: 0.9351 - val_loss: 0.7978 - val_acc: 0.8152 - val_score: 0.8422\n",
      "Epoch 00142: early stopping\n",
      "Train on 5834 samples, validate on 1458 samples\n",
      "Epoch 1/300\n",
      "5632/5834 [===========================>..] - ETA: 0s - loss: 8.6351 - acc: 0.3283 - score: 0.4127\n",
      "Epoch 00001: val_score improved from -inf to 0.24267, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 3s 496us/sample - loss: 8.4953 - acc: 0.3293 - score: 0.4150 - val_loss: 5.2678 - val_acc: 0.1324 - val_score: 0.2427\n",
      "Epoch 2/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 3.3497 - acc: 0.4107 - score: 0.4870\n",
      "Epoch 00002: val_score improved from 0.24267 to 0.27167, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 205us/sample - loss: 3.3492 - acc: 0.4109 - score: 0.4872 - val_loss: 3.5847 - val_acc: 0.1653 - val_score: 0.2717\n",
      "Epoch 3/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 2.5061 - acc: 0.4227 - score: 0.5036\n",
      "Epoch 00003: val_score improved from 0.27167 to 0.30589, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 205us/sample - loss: 2.5065 - acc: 0.4225 - score: 0.5020 - val_loss: 3.0640 - val_acc: 0.2085 - val_score: 0.3059\n",
      "Epoch 4/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 2.2333 - acc: 0.4447 - score: 0.5205\n",
      "Epoch 00004: val_score improved from 0.30589 to 0.36259, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 207us/sample - loss: 2.2339 - acc: 0.4445 - score: 0.5195 - val_loss: 2.7788 - val_acc: 0.2702 - val_score: 0.3626\n",
      "Epoch 5/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 2.0986 - acc: 0.4617 - score: 0.5372\n",
      "Epoch 00005: val_score improved from 0.36259 to 0.41288, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 205us/sample - loss: 2.1003 - acc: 0.4613 - score: 0.5349 - val_loss: 2.5673 - val_acc: 0.3128 - val_score: 0.4129\n",
      "Epoch 6/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 2.0153 - acc: 0.4684 - score: 0.5447\n",
      "Epoch 00006: val_score improved from 0.41288 to 0.47234, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 205us/sample - loss: 2.0153 - acc: 0.4681 - score: 0.5431 - val_loss: 2.3629 - val_acc: 0.3786 - val_score: 0.4723\n",
      "Epoch 7/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.9611 - acc: 0.4833 - score: 0.5568\n",
      "Epoch 00007: val_score did not improve from 0.47234\n",
      "5834/5834 [==============================] - 1s 193us/sample - loss: 1.9615 - acc: 0.4832 - score: 0.5561 - val_loss: 2.5184 - val_acc: 0.3093 - val_score: 0.4006\n",
      "Epoch 8/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.8967 - acc: 0.4899 - score: 0.5613\n",
      "Epoch 00008: val_score did not improve from 0.47234\n",
      "5834/5834 [==============================] - 1s 193us/sample - loss: 1.8987 - acc: 0.4895 - score: 0.5595 - val_loss: 2.6620 - val_acc: 0.3059 - val_score: 0.3932\n",
      "Epoch 9/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.8824 - acc: 0.4950 - score: 0.5672\n",
      "Epoch 00009: val_score did not improve from 0.47234\n",
      "5834/5834 [==============================] - 1s 198us/sample - loss: 1.8834 - acc: 0.4950 - score: 0.5674 - val_loss: 2.7055 - val_acc: 0.2325 - val_score: 0.3337\n",
      "Epoch 10/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.8755 - acc: 0.4971 - score: 0.5712\n",
      "Epoch 00010: val_score did not improve from 0.47234\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 1.8759 - acc: 0.4969 - score: 0.5700 - val_loss: 2.3175 - val_acc: 0.3738 - val_score: 0.4597\n",
      "Epoch 11/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.8255 - acc: 0.5087 - score: 0.5798\n",
      "Epoch 00011: val_score did not improve from 0.47234\n",
      "5834/5834 [==============================] - 1s 196us/sample - loss: 1.8267 - acc: 0.5087 - score: 0.5795 - val_loss: 2.2522 - val_acc: 0.3786 - val_score: 0.4701\n",
      "Epoch 12/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.8042 - acc: 0.5168 - score: 0.5878\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00012: val_score did not improve from 0.47234\n",
      "5834/5834 [==============================] - 1s 211us/sample - loss: 1.8040 - acc: 0.5170 - score: 0.5886 - val_loss: 2.2537 - val_acc: 0.3848 - val_score: 0.4574\n",
      "Epoch 13/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.6611 - acc: 0.5477 - score: 0.6137\n",
      "Epoch 00013: val_score improved from 0.47234 to 0.52112, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 208us/sample - loss: 1.6624 - acc: 0.5468 - score: 0.6083 - val_loss: 2.0160 - val_acc: 0.4472 - val_score: 0.5211\n",
      "Epoch 14/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.6266 - acc: 0.5512 - score: 0.6155\n",
      "Epoch 00014: val_score did not improve from 0.52112\n",
      "5834/5834 [==============================] - 1s 198us/sample - loss: 1.6268 - acc: 0.5504 - score: 0.6101 - val_loss: 2.2458 - val_acc: 0.3306 - val_score: 0.4073\n",
      "Epoch 15/300\n",
      "5632/5834 [===========================>..] - ETA: 0s - loss: 1.6008 - acc: 0.5559 - score: 0.6208\n",
      "Epoch 00015: val_score did not improve from 0.52112\n",
      "5834/5834 [==============================] - 1s 198us/sample - loss: 1.6013 - acc: 0.5549 - score: 0.6196 - val_loss: 2.2035 - val_acc: 0.4047 - val_score: 0.4679\n",
      "Epoch 16/300\n",
      "5568/5834 [===========================>..] - ETA: 0s - loss: 1.5703 - acc: 0.5688 - score: 0.6325\n",
      "Epoch 00016: val_score did not improve from 0.52112\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 1.5739 - acc: 0.5674 - score: 0.6279 - val_loss: 2.5776 - val_acc: 0.3573 - val_score: 0.4216\n",
      "Epoch 17/300\n",
      "5568/5834 [===========================>..] - ETA: 0s - loss: 1.5814 - acc: 0.5603 - score: 0.6243\n",
      "Epoch 00017: val_score improved from 0.52112 to 0.55405, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 213us/sample - loss: 1.5859 - acc: 0.5583 - score: 0.6210 - val_loss: 1.8886 - val_acc: 0.4849 - val_score: 0.5540\n",
      "Epoch 18/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.5245 - acc: 0.5841 - score: 0.6449\n",
      "Epoch 00018: val_score did not improve from 0.55405\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 1.5247 - acc: 0.5843 - score: 0.6458 - val_loss: 2.1013 - val_acc: 0.4115 - val_score: 0.4874\n",
      "Epoch 19/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.5032 - acc: 0.5874 - score: 0.6479\n",
      "Epoch 00019: val_score did not improve from 0.55405\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 1.5030 - acc: 0.5876 - score: 0.6490 - val_loss: 2.1967 - val_acc: 0.3951 - val_score: 0.4812\n",
      "Epoch 20/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.4941 - acc: 0.5953 - score: 0.6543\n",
      "Epoch 00020: val_score improved from 0.55405 to 0.60244, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 207us/sample - loss: 1.4948 - acc: 0.5948 - score: 0.6511 - val_loss: 1.6916 - val_acc: 0.5322 - val_score: 0.6024\n",
      "Epoch 21/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.4910 - acc: 0.5901 - score: 0.6504\n",
      "Epoch 00021: val_score did not improve from 0.60244\n",
      "5834/5834 [==============================] - 1s 194us/sample - loss: 1.4904 - acc: 0.5903 - score: 0.6516 - val_loss: 2.0506 - val_acc: 0.4225 - val_score: 0.5025\n",
      "Epoch 22/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.4580 - acc: 0.6065 - score: 0.6659\n",
      "Epoch 00022: val_score did not improve from 0.60244\n",
      "5834/5834 [==============================] - 1s 193us/sample - loss: 1.4587 - acc: 0.6063 - score: 0.6647 - val_loss: 1.8086 - val_acc: 0.5144 - val_score: 0.5805\n",
      "Epoch 23/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 1.4421 - acc: 0.6076 - score: 0.6666\n",
      "Epoch 00023: val_score did not improve from 0.60244\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 1.4417 - acc: 0.6080 - score: 0.6661 - val_loss: 2.0690 - val_acc: 0.4239 - val_score: 0.4949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.4487 - acc: 0.6054 - score: 0.6647\n",
      "Epoch 00024: val_score did not improve from 0.60244\n",
      "5834/5834 [==============================] - 1s 196us/sample - loss: 1.4484 - acc: 0.6047 - score: 0.6627 - val_loss: 1.8752 - val_acc: 0.4822 - val_score: 0.5524\n",
      "Epoch 25/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.4261 - acc: 0.6133 - score: 0.6718\n",
      "Epoch 00025: val_score did not improve from 0.60244\n",
      "5834/5834 [==============================] - 1s 196us/sample - loss: 1.4273 - acc: 0.6128 - score: 0.6690 - val_loss: 1.8281 - val_acc: 0.4870 - val_score: 0.5422\n",
      "Epoch 26/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.4272 - acc: 0.6180 - score: 0.6759\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00026: val_score did not improve from 0.60244\n",
      "5834/5834 [==============================] - 1s 197us/sample - loss: 1.4283 - acc: 0.6178 - score: 0.6746 - val_loss: 2.9079 - val_acc: 0.2805 - val_score: 0.3184\n",
      "Epoch 27/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 1.3063 - acc: 0.6534 - score: 0.7056\n",
      "Epoch 00027: val_score improved from 0.60244 to 0.67754, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 211us/sample - loss: 1.3087 - acc: 0.6531 - score: 0.7038 - val_loss: 1.4459 - val_acc: 0.6221 - val_score: 0.6775\n",
      "Epoch 28/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 1.2731 - acc: 0.6624 - score: 0.7142\n",
      "Epoch 00028: val_score did not improve from 0.67754\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 1.2730 - acc: 0.6618 - score: 0.7136 - val_loss: 1.6119 - val_acc: 0.5590 - val_score: 0.6237\n",
      "Epoch 29/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.2418 - acc: 0.6666 - score: 0.7164\n",
      "Epoch 00029: val_score did not improve from 0.67754\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 1.2422 - acc: 0.6663 - score: 0.7151 - val_loss: 1.6729 - val_acc: 0.5638 - val_score: 0.6194\n",
      "Epoch 30/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.2223 - acc: 0.6767 - score: 0.7255\n",
      "Epoch 00030: val_score did not improve from 0.67754\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 1.2232 - acc: 0.6764 - score: 0.7243 - val_loss: 1.4570 - val_acc: 0.6111 - val_score: 0.6705\n",
      "Epoch 31/300\n",
      "5632/5834 [===========================>..] - ETA: 0s - loss: 1.2206 - acc: 0.6792 - score: 0.7277\n",
      "Epoch 00031: val_score did not improve from 0.67754\n",
      "5834/5834 [==============================] - 1s 201us/sample - loss: 1.2173 - acc: 0.6802 - score: 0.7275 - val_loss: 1.4769 - val_acc: 0.5912 - val_score: 0.6502\n",
      "Epoch 32/300\n",
      "5632/5834 [===========================>..] - ETA: 0s - loss: 1.2140 - acc: 0.6724 - score: 0.7232\n",
      "Epoch 00032: val_score did not improve from 0.67754\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 1.2161 - acc: 0.6712 - score: 0.7199 - val_loss: 1.7253 - val_acc: 0.5267 - val_score: 0.5908\n",
      "Epoch 33/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 1.1818 - acc: 0.6921 - score: 0.7386\n",
      "Epoch 00033: val_score improved from 0.67754 to 0.68978, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 210us/sample - loss: 1.1832 - acc: 0.6913 - score: 0.7346 - val_loss: 1.4141 - val_acc: 0.6372 - val_score: 0.6898\n",
      "Epoch 34/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.1654 - acc: 0.6990 - score: 0.7458\n",
      "Epoch 00034: val_score did not improve from 0.68978\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 1.1670 - acc: 0.6980 - score: 0.7443 - val_loss: 1.5062 - val_acc: 0.5775 - val_score: 0.6436\n",
      "Epoch 35/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.1571 - acc: 0.7030 - score: 0.7488\n",
      "Epoch 00035: val_score did not improve from 0.68978\n",
      "5834/5834 [==============================] - 1s 196us/sample - loss: 1.1583 - acc: 0.7029 - score: 0.7492 - val_loss: 1.5358 - val_acc: 0.5933 - val_score: 0.6492\n",
      "Epoch 36/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.1338 - acc: 0.7093 - score: 0.7547\n",
      "Epoch 00036: val_score did not improve from 0.68978\n",
      "5834/5834 [==============================] - 1s 196us/sample - loss: 1.1340 - acc: 0.7091 - score: 0.7536 - val_loss: 1.5411 - val_acc: 0.5693 - val_score: 0.6383\n",
      "Epoch 37/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.1276 - acc: 0.7048 - score: 0.7497\n",
      "Epoch 00037: val_score did not improve from 0.68978\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 1.1279 - acc: 0.7048 - score: 0.7498 - val_loss: 1.6805 - val_acc: 0.5357 - val_score: 0.5909\n",
      "Epoch 38/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.1289 - acc: 0.7036 - score: 0.7499\n",
      "Epoch 00038: val_score did not improve from 0.68978\n",
      "5834/5834 [==============================] - 1s 196us/sample - loss: 1.1300 - acc: 0.7033 - score: 0.7477 - val_loss: 1.3302 - val_acc: 0.6262 - val_score: 0.6714\n",
      "Epoch 39/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 1.0792 - acc: 0.7306 - score: 0.7731\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00039: val_score did not improve from 0.68978\n",
      "5834/5834 [==============================] - 1s 196us/sample - loss: 1.0795 - acc: 0.7305 - score: 0.7728 - val_loss: 1.7206 - val_acc: 0.5412 - val_score: 0.5978\n",
      "Epoch 40/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 1.0203 - acc: 0.7533 - score: 0.7910\n",
      "Epoch 00040: val_score improved from 0.68978 to 0.72684, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 206us/sample - loss: 1.0225 - acc: 0.7528 - score: 0.7884 - val_loss: 1.2816 - val_acc: 0.6824 - val_score: 0.7268\n",
      "Epoch 41/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9801 - acc: 0.7668 - score: 0.8030\n",
      "Epoch 00041: val_score improved from 0.72684 to 0.73767, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 207us/sample - loss: 0.9820 - acc: 0.7659 - score: 0.7974 - val_loss: 1.1964 - val_acc: 0.6886 - val_score: 0.7377\n",
      "Epoch 42/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9768 - acc: 0.7727 - score: 0.8082\n",
      "Epoch 00042: val_score did not improve from 0.73767\n",
      "5834/5834 [==============================] - 1s 193us/sample - loss: 0.9763 - acc: 0.7731 - score: 0.8103 - val_loss: 1.2723 - val_acc: 0.6687 - val_score: 0.7212\n",
      "Epoch 43/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9358 - acc: 0.7792 - score: 0.8141\n",
      "Epoch 00043: val_score did not improve from 0.73767\n",
      "5834/5834 [==============================] - 1s 194us/sample - loss: 0.9377 - acc: 0.7784 - score: 0.8097 - val_loss: 1.2041 - val_acc: 0.6859 - val_score: 0.7328\n",
      "Epoch 44/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9342 - acc: 0.7787 - score: 0.8127\n",
      "Epoch 00044: val_score did not improve from 0.73767\n",
      "5834/5834 [==============================] - 1s 196us/sample - loss: 0.9346 - acc: 0.7785 - score: 0.8121 - val_loss: 1.2274 - val_acc: 0.6756 - val_score: 0.7242\n",
      "Epoch 45/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.9214 - acc: 0.7795 - score: 0.8138\n",
      "Epoch 00045: val_score did not improve from 0.73767\n",
      "5834/5834 [==============================] - 1s 194us/sample - loss: 0.9222 - acc: 0.7794 - score: 0.8127 - val_loss: 1.3128 - val_acc: 0.6557 - val_score: 0.7097\n",
      "Epoch 46/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.9323 - acc: 0.7771 - score: 0.8114\n",
      "Epoch 00046: val_score improved from 0.73767 to 0.77526, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 211us/sample - loss: 0.9319 - acc: 0.7772 - score: 0.8084 - val_loss: 1.0714 - val_acc: 0.7359 - val_score: 0.7753\n",
      "Epoch 47/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.9165 - acc: 0.7854 - score: 0.8187\n",
      "Epoch 00047: val_score did not improve from 0.77526\n",
      "5834/5834 [==============================] - 1s 197us/sample - loss: 0.9167 - acc: 0.7856 - score: 0.8169 - val_loss: 1.1967 - val_acc: 0.6975 - val_score: 0.7477\n",
      "Epoch 48/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.8947 - acc: 0.7948 - score: 0.8271\n",
      "Epoch 00048: val_score did not improve from 0.77526\n",
      "5834/5834 [==============================] - 1s 194us/sample - loss: 0.8969 - acc: 0.7940 - score: 0.8224 - val_loss: 1.1832 - val_acc: 0.6852 - val_score: 0.7321\n",
      "Epoch 49/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.8929 - acc: 0.7904 - score: 0.8228\n",
      "Epoch 00049: val_score did not improve from 0.77526\n",
      "5834/5834 [==============================] - 1s 197us/sample - loss: 0.8934 - acc: 0.7902 - score: 0.8221 - val_loss: 1.1360 - val_acc: 0.6968 - val_score: 0.7412\n",
      "Epoch 50/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.8673 - acc: 0.8046 - score: 0.8352\n",
      "Epoch 00050: val_score did not improve from 0.77526\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 0.8690 - acc: 0.8036 - score: 0.8300 - val_loss: 1.2150 - val_acc: 0.6955 - val_score: 0.7400\n",
      "Epoch 51/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.8389 - acc: 0.8154 - score: 0.8444\n",
      "Epoch 00051: val_score did not improve from 0.77526\n",
      "5834/5834 [==============================] - 1s 197us/sample - loss: 0.8393 - acc: 0.8154 - score: 0.8444 - val_loss: 1.1916 - val_acc: 0.6866 - val_score: 0.7347\n",
      "Epoch 52/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.8502 - acc: 0.8073 - score: 0.8371\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00052: val_score did not improve from 0.77526\n",
      "5834/5834 [==============================] - 1s 196us/sample - loss: 0.8519 - acc: 0.8068 - score: 0.8345 - val_loss: 1.1125 - val_acc: 0.7188 - val_score: 0.7568\n",
      "Epoch 53/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.7983 - acc: 0.8288 - score: 0.8559\n",
      "Epoch 00053: val_score did not improve from 0.77526\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 0.7996 - acc: 0.8282 - score: 0.8527 - val_loss: 1.0914 - val_acc: 0.7085 - val_score: 0.7553\n",
      "Epoch 54/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.7782 - acc: 0.8399 - score: 0.8650\n",
      "Epoch 00054: val_score improved from 0.77526 to 0.77991, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 208us/sample - loss: 0.7775 - acc: 0.8404 - score: 0.8643 - val_loss: 1.0458 - val_acc: 0.7407 - val_score: 0.7799\n",
      "Epoch 55/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.7647 - acc: 0.8404 - score: 0.8659\n",
      "Epoch 00055: val_score improved from 0.77991 to 0.78653, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 213us/sample - loss: 0.7669 - acc: 0.8399 - score: 0.8636 - val_loss: 0.9885 - val_acc: 0.7476 - val_score: 0.7865\n",
      "Epoch 56/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.7486 - acc: 0.8453 - score: 0.8698\n",
      "Epoch 00056: val_score improved from 0.78653 to 0.80114, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 213us/sample - loss: 0.7519 - acc: 0.8435 - score: 0.8647 - val_loss: 0.9936 - val_acc: 0.7641 - val_score: 0.8011\n",
      "Epoch 57/300\n",
      "5632/5834 [===========================>..] - ETA: 0s - loss: 0.7315 - acc: 0.8533 - score: 0.8769\n",
      "Epoch 00057: val_score did not improve from 0.80114\n",
      "5834/5834 [==============================] - 1s 202us/sample - loss: 0.7409 - acc: 0.8507 - score: 0.8724 - val_loss: 0.9991 - val_acc: 0.7545 - val_score: 0.7913\n",
      "Epoch 58/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.7201 - acc: 0.8601 - score: 0.8816\n",
      "Epoch 00058: val_score improved from 0.80114 to 0.80496, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 219us/sample - loss: 0.7191 - acc: 0.8612 - score: 0.8802 - val_loss: 0.9540 - val_acc: 0.7730 - val_score: 0.8050\n",
      "Epoch 59/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.7320 - acc: 0.8545 - score: 0.8770\n",
      "Epoch 00059: val_score did not improve from 0.80496\n",
      "5834/5834 [==============================] - 1s 201us/sample - loss: 0.7336 - acc: 0.8548 - score: 0.8731 - val_loss: 0.9812 - val_acc: 0.7641 - val_score: 0.7998\n",
      "Epoch 60/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.7126 - acc: 0.8602 - score: 0.8824\n",
      "Epoch 00060: val_score did not improve from 0.80496\n",
      "5834/5834 [==============================] - 1s 199us/sample - loss: 0.7133 - acc: 0.8598 - score: 0.8802 - val_loss: 1.0461 - val_acc: 0.7277 - val_score: 0.7706\n",
      "Epoch 61/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.7130 - acc: 0.8615 - score: 0.8837\n",
      "Epoch 00061: val_score did not improve from 0.80496\n",
      "5834/5834 [==============================] - 1s 198us/sample - loss: 0.7147 - acc: 0.8610 - score: 0.8816 - val_loss: 0.9637 - val_acc: 0.7634 - val_score: 0.7972\n",
      "Epoch 62/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.7013 - acc: 0.8587 - score: 0.8804\n",
      "Epoch 00062: val_score did not improve from 0.80496\n",
      "5834/5834 [==============================] - 1s 197us/sample - loss: 0.7033 - acc: 0.8577 - score: 0.8749 - val_loss: 0.9880 - val_acc: 0.7538 - val_score: 0.7921\n",
      "Epoch 63/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.6887 - acc: 0.8703 - score: 0.8908\n",
      "Epoch 00063: val_score did not improve from 0.80496\n",
      "5834/5834 [==============================] - 1s 198us/sample - loss: 0.6908 - acc: 0.8690 - score: 0.8856 - val_loss: 1.0167 - val_acc: 0.7476 - val_score: 0.7838\n",
      "Epoch 64/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.6907 - acc: 0.8658 - score: 0.8859\n",
      "Epoch 00064: val_score improved from 0.80496 to 0.81360, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 211us/sample - loss: 0.6918 - acc: 0.8656 - score: 0.8837 - val_loss: 0.9636 - val_acc: 0.7805 - val_score: 0.8136\n",
      "Epoch 65/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.6868 - acc: 0.8694 - score: 0.8906\n",
      "Epoch 00065: val_score did not improve from 0.81360\n",
      "5834/5834 [==============================] - 1s 199us/sample - loss: 0.6904 - acc: 0.8677 - score: 0.8857 - val_loss: 0.9908 - val_acc: 0.7599 - val_score: 0.7955\n",
      "Epoch 66/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6830 - acc: 0.8692 - score: 0.8888\n",
      "Epoch 00066: val_score did not improve from 0.81360\n",
      "5834/5834 [==============================] - 1s 198us/sample - loss: 0.6834 - acc: 0.8690 - score: 0.8884 - val_loss: 1.1154 - val_acc: 0.7167 - val_score: 0.7563\n",
      "Epoch 67/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.6716 - acc: 0.8734 - score: 0.8932\n",
      "Epoch 00067: val_score did not improve from 0.81360\n",
      "5834/5834 [==============================] - 1s 201us/sample - loss: 0.6719 - acc: 0.8738 - score: 0.8928 - val_loss: 1.0087 - val_acc: 0.7593 - val_score: 0.7949\n",
      "Epoch 68/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.6514 - acc: 0.8795 - score: 0.8987\n",
      "Epoch 00068: val_score did not improve from 0.81360\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 0.6518 - acc: 0.8793 - score: 0.8968 - val_loss: 1.0086 - val_acc: 0.7627 - val_score: 0.7995\n",
      "Epoch 69/300\n",
      "5632/5834 [===========================>..] - ETA: 0s - loss: 0.6699 - acc: 0.8766 - score: 0.8971\n",
      "Epoch 00069: val_score did not improve from 0.81360\n",
      "5834/5834 [==============================] - 1s 203us/sample - loss: 0.6729 - acc: 0.8756 - score: 0.8938 - val_loss: 1.0177 - val_acc: 0.7545 - val_score: 0.7902\n",
      "Epoch 70/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.6491 - acc: 0.8817 - score: 0.9008\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00070: val_score did not improve from 0.81360\n",
      "5834/5834 [==============================] - 1s 201us/sample - loss: 0.6480 - acc: 0.8824 - score: 0.9017 - val_loss: 0.9671 - val_acc: 0.7716 - val_score: 0.8054\n",
      "Epoch 71/300\n",
      "5632/5834 [===========================>..] - ETA: 0s - loss: 0.6109 - acc: 0.8963 - score: 0.9133\n",
      "Epoch 00071: val_score improved from 0.81360 to 0.82184, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 212us/sample - loss: 0.6111 - acc: 0.8963 - score: 0.9116 - val_loss: 0.9129 - val_acc: 0.7894 - val_score: 0.8218\n",
      "Epoch 72/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.8985 - score: 0.9146\n",
      "Epoch 00072: val_score did not improve from 0.82184\n",
      "5834/5834 [==============================] - 1s 203us/sample - loss: 0.6034 - acc: 0.8977 - score: 0.9096 - val_loss: 0.9093 - val_acc: 0.7874 - val_score: 0.8181\n",
      "Epoch 73/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5906 - acc: 0.9021 - score: 0.9184\n",
      "Epoch 00073: val_score did not improve from 0.82184\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 0.5912 - acc: 0.9016 - score: 0.9162 - val_loss: 0.9048 - val_acc: 0.7860 - val_score: 0.8171\n",
      "Epoch 74/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5848 - acc: 0.9090 - score: 0.9247\n",
      "Epoch 00074: val_score did not improve from 0.82184\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 0.5894 - acc: 0.9073 - score: 0.9189 - val_loss: 0.9005 - val_acc: 0.7826 - val_score: 0.8153\n",
      "Epoch 75/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5924 - acc: 0.9010 - score: 0.9167\n",
      "Epoch 00075: val_score did not improve from 0.82184\n",
      "5834/5834 [==============================] - 1s 201us/sample - loss: 0.5920 - acc: 0.9014 - score: 0.9153 - val_loss: 0.9077 - val_acc: 0.7867 - val_score: 0.8201\n",
      "Epoch 76/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.6004 - acc: 0.8987 - score: 0.9150\n",
      "Epoch 00076: val_score did not improve from 0.82184\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 0.6031 - acc: 0.8973 - score: 0.9133 - val_loss: 0.9347 - val_acc: 0.7743 - val_score: 0.8073\n",
      "Epoch 77/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5777 - acc: 0.9073 - score: 0.9220\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00077: val_score did not improve from 0.82184\n",
      "5834/5834 [==============================] - 1s 198us/sample - loss: 0.5791 - acc: 0.9066 - score: 0.9183 - val_loss: 0.9411 - val_acc: 0.7675 - val_score: 0.8044\n",
      "Epoch 78/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.9177 - score: 0.9312\n",
      "Epoch 00078: val_score did not improve from 0.82184\n",
      "5834/5834 [==============================] - 1s 198us/sample - loss: 0.5551 - acc: 0.9174 - score: 0.9285 - val_loss: 0.8971 - val_acc: 0.7867 - val_score: 0.8185\n",
      "Epoch 79/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5614 - acc: 0.9181 - score: 0.9311\n",
      "Epoch 00079: val_score improved from 0.82184 to 0.83266, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 208us/sample - loss: 0.5629 - acc: 0.9174 - score: 0.9268 - val_loss: 0.8664 - val_acc: 0.8038 - val_score: 0.8327\n",
      "Epoch 80/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5589 - acc: 0.9163 - score: 0.9300\n",
      "Epoch 00080: val_score did not improve from 0.83266\n",
      "5834/5834 [==============================] - 1s 198us/sample - loss: 0.5595 - acc: 0.9164 - score: 0.9275 - val_loss: 0.8718 - val_acc: 0.8038 - val_score: 0.8317\n",
      "Epoch 81/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.9184 - score: 0.9320\n",
      "Epoch 00081: val_score did not improve from 0.83266\n",
      "5834/5834 [==============================] - 1s 197us/sample - loss: 0.5533 - acc: 0.9177 - score: 0.9291 - val_loss: 0.8924 - val_acc: 0.7894 - val_score: 0.8205\n",
      "Epoch 82/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5658 - acc: 0.9120 - score: 0.9251\n",
      "Epoch 00082: val_score did not improve from 0.83266\n",
      "5834/5834 [==============================] - 1s 198us/sample - loss: 0.5659 - acc: 0.9121 - score: 0.9243 - val_loss: 0.8828 - val_acc: 0.7915 - val_score: 0.8192\n",
      "Epoch 83/300\n",
      "5632/5834 [===========================>..] - ETA: 0s - loss: 0.5603 - acc: 0.9189 - score: 0.9320\n",
      "Epoch 00083: val_score did not improve from 0.83266\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 0.5606 - acc: 0.9184 - score: 0.9317 - val_loss: 0.8748 - val_acc: 0.7970 - val_score: 0.8276\n",
      "Epoch 84/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.9276 - score: 0.9391\n",
      "Epoch 00084: val_score did not improve from 0.83266\n",
      "5834/5834 [==============================] - 1s 197us/sample - loss: 0.5389 - acc: 0.9272 - score: 0.9353 - val_loss: 0.8760 - val_acc: 0.7984 - val_score: 0.8292\n",
      "Epoch 85/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.9254 - score: 0.9375\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00085: val_score did not improve from 0.83266\n",
      "5834/5834 [==============================] - 1s 197us/sample - loss: 0.5383 - acc: 0.9258 - score: 0.9377 - val_loss: 0.8830 - val_acc: 0.8032 - val_score: 0.8309\n",
      "Epoch 86/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.9207 - score: 0.9335\n",
      "Epoch 00086: val_score improved from 0.83266 to 0.83449, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 207us/sample - loss: 0.5381 - acc: 0.9203 - score: 0.9314 - val_loss: 0.8653 - val_acc: 0.8080 - val_score: 0.8345\n",
      "Epoch 87/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.9226 - score: 0.9352\n",
      "Epoch 00087: val_score did not improve from 0.83449\n",
      "5834/5834 [==============================] - 1s 196us/sample - loss: 0.5379 - acc: 0.9222 - score: 0.9331 - val_loss: 0.8633 - val_acc: 0.8025 - val_score: 0.8311\n",
      "Epoch 88/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.9215 - score: 0.9336\n",
      "Epoch 00088: val_score did not improve from 0.83449\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 0.5397 - acc: 0.9210 - score: 0.9305 - val_loss: 0.8715 - val_acc: 0.8018 - val_score: 0.8314\n",
      "Epoch 89/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5190 - acc: 0.9275 - score: 0.9390\n",
      "Epoch 00089: val_score did not improve from 0.83449\n",
      "5834/5834 [==============================] - 1s 195us/sample - loss: 0.5199 - acc: 0.9272 - score: 0.9370 - val_loss: 0.8593 - val_acc: 0.7956 - val_score: 0.8263\n",
      "Epoch 90/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.9277 - score: 0.9398\n",
      "Epoch 00090: val_score improved from 0.83449 to 0.83555, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 206us/sample - loss: 0.5280 - acc: 0.9277 - score: 0.9395 - val_loss: 0.8529 - val_acc: 0.8086 - val_score: 0.8355\n",
      "Epoch 91/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.5233 - acc: 0.9242 - score: 0.9362\n",
      "Epoch 00091: val_score did not improve from 0.83555\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 0.5238 - acc: 0.9246 - score: 0.9355 - val_loss: 0.8620 - val_acc: 0.8059 - val_score: 0.8334\n",
      "Epoch 92/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.9249 - score: 0.9376\n",
      "Epoch 00092: val_score did not improve from 0.83555\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 0.5273 - acc: 0.9246 - score: 0.9363 - val_loss: 0.8750 - val_acc: 0.8004 - val_score: 0.8304\n",
      "Epoch 93/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.9245 - score: 0.9367\n",
      "Epoch 00093: val_score did not improve from 0.83555\n",
      "5834/5834 [==============================] - 1s 204us/sample - loss: 0.5265 - acc: 0.9237 - score: 0.9336 - val_loss: 0.8590 - val_acc: 0.8073 - val_score: 0.8350\n",
      "Epoch 94/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.9262 - score: 0.9383\n",
      "Epoch 00094: val_score improved from 0.83555 to 0.83572, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 210us/sample - loss: 0.5236 - acc: 0.9254 - score: 0.9360 - val_loss: 0.8580 - val_acc: 0.8086 - val_score: 0.8357\n",
      "Epoch 95/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.9226 - score: 0.9350\n",
      "Epoch 00095: val_score improved from 0.83572 to 0.83646, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 207us/sample - loss: 0.5264 - acc: 0.9217 - score: 0.9299 - val_loss: 0.8565 - val_acc: 0.8080 - val_score: 0.8365\n",
      "Epoch 96/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.9295 - score: 0.9415\n",
      "Epoch 00096: val_score did not improve from 0.83646\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 0.5222 - acc: 0.9290 - score: 0.9376 - val_loss: 0.8678 - val_acc: 0.8004 - val_score: 0.8280\n",
      "Epoch 97/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5055 - acc: 0.9372 - score: 0.9470\n",
      "Epoch 00097: val_score did not improve from 0.83646\n",
      "5834/5834 [==============================] - 1s 200us/sample - loss: 0.5067 - acc: 0.9364 - score: 0.9428 - val_loss: 0.8611 - val_acc: 0.8032 - val_score: 0.8304\n",
      "Epoch 98/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.9308 - score: 0.9421\n",
      "Epoch 00098: val_score did not improve from 0.83646\n",
      "5834/5834 [==============================] - 1s 203us/sample - loss: 0.5163 - acc: 0.9302 - score: 0.9387 - val_loss: 0.8559 - val_acc: 0.8059 - val_score: 0.8336\n",
      "Epoch 99/300\n",
      "5632/5834 [===========================>..] - ETA: 0s - loss: 0.5128 - acc: 0.9345 - score: 0.945 - ETA: 0s - loss: 0.5175 - acc: 0.9324 - score: 0.9432\n",
      "Epoch 00099: val_score did not improve from 0.83646\n",
      "5834/5834 [==============================] - 1s 203us/sample - loss: 0.5191 - acc: 0.9320 - score: 0.9411 - val_loss: 0.8781 - val_acc: 0.7997 - val_score: 0.8287\n",
      "Epoch 100/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.9272 - score: 0.9390\n",
      "Epoch 00100: val_score did not improve from 0.83646\n",
      "5834/5834 [==============================] - 1s 202us/sample - loss: 0.5251 - acc: 0.9266 - score: 0.9356 - val_loss: 0.8589 - val_acc: 0.8032 - val_score: 0.8309\n",
      "Epoch 101/300\n",
      "5760/5834 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.9304 - score: 0.9419\n",
      "Epoch 00101: val_score improved from 0.83646 to 0.83657, saving model to fold2.h5\n",
      "5834/5834 [==============================] - 1s 210us/sample - loss: 0.5201 - acc: 0.9304 - score: 0.9416 - val_loss: 0.8600 - val_acc: 0.8073 - val_score: 0.8366\n",
      "Epoch 102/300\n",
      "5632/5834 [===========================>..] - ETA: 0s - loss: 0.5072 - acc: 0.9336 - score: 0.9442\n",
      "Epoch 00102: val_score did not improve from 0.83657\n",
      "5834/5834 [==============================] - 1s 199us/sample - loss: 0.5093 - acc: 0.9337 - score: 0.9425 - val_loss: 0.8660 - val_acc: 0.8025 - val_score: 0.8312\n",
      "Epoch 103/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.9320 - score: 0.9435\n",
      "Epoch 00103: val_score did not improve from 0.83657\n",
      "5834/5834 [==============================] - 1s 208us/sample - loss: 0.5146 - acc: 0.9311 - score: 0.9392 - val_loss: 0.8568 - val_acc: 0.8018 - val_score: 0.8310\n",
      "Epoch 104/300\n",
      "5824/5834 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.9336 - score: 0.9446\n",
      "Epoch 00104: val_score did not improve from 0.83657\n",
      "5834/5834 [==============================] - 1s 207us/sample - loss: 0.5125 - acc: 0.9332 - score: 0.9426 - val_loss: 0.8609 - val_acc: 0.7984 - val_score: 0.8263\n",
      "Epoch 105/300\n",
      "5696/5834 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.9287 - score: 0.9400\n",
      "Epoch 00105: val_score did not improve from 0.83657\n",
      "5834/5834 [==============================] - 1s 202us/sample - loss: 0.5168 - acc: 0.9282 - score: 0.9360 - val_loss: 0.8662 - val_acc: 0.7997 - val_score: 0.8297\n",
      "Epoch 00105: early stopping\n",
      "Train on 5837 samples, validate on 1455 samples\n",
      "Epoch 1/300\n",
      "5696/5837 [============================>.] - ETA: 0s - loss: 8.5682 - acc: 0.3411 - score: 0.4210\n",
      "Epoch 00001: val_score improved from -inf to 0.29199, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 3s 490us/sample - loss: 8.4683 - acc: 0.3418 - score: 0.4198 - val_loss: 5.2541 - val_acc: 0.1890 - val_score: 0.2920\n",
      "Epoch 2/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 3.3255 - acc: 0.4049 - score: 0.4823\n",
      "Epoch 00002: val_score did not improve from 0.29199\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 3.3235 - acc: 0.4031 - score: 0.4795 - val_loss: 3.6801 - val_acc: 0.1828 - val_score: 0.2873\n",
      "Epoch 3/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 2.4908 - acc: 0.4378 - score: 0.5156\n",
      "Epoch 00003: val_score improved from 0.29199 to 0.34934, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 203us/sample - loss: 2.4908 - acc: 0.4374 - score: 0.5134 - val_loss: 3.1716 - val_acc: 0.2454 - val_score: 0.3493\n",
      "Epoch 4/300\n",
      "5632/5837 [===========================>..] - ETA: 0s - loss: 2.2227 - acc: 0.4538 - score: 0.5284\n",
      "Epoch 00004: val_score did not improve from 0.34934\n",
      "5837/5837 [==============================] - 1s 200us/sample - loss: 2.2170 - acc: 0.4554 - score: 0.5288 - val_loss: 2.9354 - val_acc: 0.2137 - val_score: 0.2996\n",
      "Epoch 5/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 2.0710 - acc: 0.4753 - score: 0.5461\n",
      "Epoch 00005: val_score improved from 0.34934 to 0.41759, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 204us/sample - loss: 2.0712 - acc: 0.4751 - score: 0.5458 - val_loss: 2.4981 - val_acc: 0.3258 - val_score: 0.4176\n",
      "Epoch 6/300\n",
      "5632/5837 [===========================>..] - ETA: 0s - loss: 2.0194 - acc: 0.4775 - score: 0.5486\n",
      "Epoch 00006: val_score did not improve from 0.41759\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 2.0215 - acc: 0.4761 - score: 0.5479 - val_loss: 2.4074 - val_acc: 0.3265 - val_score: 0.4113\n",
      "Epoch 7/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.9505 - acc: 0.4756 - score: 0.5473\n",
      "Epoch 00007: val_score did not improve from 0.41759\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 1.9506 - acc: 0.4752 - score: 0.5457 - val_loss: 2.7036 - val_acc: 0.2522 - val_score: 0.3335\n",
      "Epoch 8/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.8997 - acc: 0.4943 - score: 0.5674\n",
      "Epoch 00008: val_score did not improve from 0.41759\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 1.9018 - acc: 0.4939 - score: 0.5675 - val_loss: 2.9588 - val_acc: 0.2880 - val_score: 0.3539\n",
      "Epoch 9/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.8594 - acc: 0.5093 - score: 0.5782\n",
      "Epoch 00009: val_score improved from 0.41759 to 0.51780, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 203us/sample - loss: 1.8598 - acc: 0.5088 - score: 0.5761 - val_loss: 2.1623 - val_acc: 0.4474 - val_score: 0.5178\n",
      "Epoch 10/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.8222 - acc: 0.5019 - score: 0.5715\n",
      "Epoch 00010: val_score did not improve from 0.51780\n",
      "5837/5837 [==============================] - 1s 198us/sample - loss: 1.8221 - acc: 0.5020 - score: 0.5717 - val_loss: 2.5366 - val_acc: 0.3381 - val_score: 0.4150\n",
      "Epoch 11/300\n",
      "5696/5837 [============================>.] - ETA: 0s - loss: 1.7986 - acc: 0.5083 - score: 0.5781\n",
      "Epoch 00011: val_score did not improve from 0.51780\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 1.7988 - acc: 0.5073 - score: 0.5751 - val_loss: 4.2083 - val_acc: 0.1883 - val_score: 0.2349\n",
      "Epoch 12/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.8065 - acc: 0.5136 - score: 0.5820\n",
      "Epoch 00012: val_score did not improve from 0.51780\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 1.8079 - acc: 0.5131 - score: 0.5800 - val_loss: 2.8232 - val_acc: 0.3017 - val_score: 0.3917\n",
      "Epoch 13/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.7736 - acc: 0.5221 - score: 0.5914\n",
      "Epoch 00013: val_score did not improve from 0.51780\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 1.7731 - acc: 0.5224 - score: 0.5925 - val_loss: 3.7382 - val_acc: 0.1629 - val_score: 0.2262\n",
      "Epoch 14/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.7574 - acc: 0.5129 - score: 0.5848\n",
      "Epoch 00014: val_score did not improve from 0.51780\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 1.7579 - acc: 0.5128 - score: 0.5837 - val_loss: 2.6603 - val_acc: 0.3155 - val_score: 0.3796\n",
      "Epoch 15/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5568/5837 [===========================>..] - ETA: 0s - loss: 1.7352 - acc: 0.5271 - score: 0.5968\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00015: val_score did not improve from 0.51780\n",
      "5837/5837 [==============================] - 1s 211us/sample - loss: 1.7356 - acc: 0.5258 - score: 0.5937 - val_loss: 2.0984 - val_acc: 0.4096 - val_score: 0.4863\n",
      "Epoch 16/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.5957 - acc: 0.5677 - score: 0.6309\n",
      "Epoch 00016: val_score improved from 0.51780 to 0.60905, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 205us/sample - loss: 1.5966 - acc: 0.5674 - score: 0.6294 - val_loss: 1.8466 - val_acc: 0.5416 - val_score: 0.6091\n",
      "Epoch 17/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.5662 - acc: 0.5649 - score: 0.6272\n",
      "Epoch 00017: val_score did not improve from 0.60905\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 1.5650 - acc: 0.5654 - score: 0.6287 - val_loss: 1.9999 - val_acc: 0.4625 - val_score: 0.5424\n",
      "Epoch 18/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.5285 - acc: 0.5876 - score: 0.6484\n",
      "Epoch 00018: val_score did not improve from 0.60905\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 1.5285 - acc: 0.5876 - score: 0.6488 - val_loss: 1.8626 - val_acc: 0.4825 - val_score: 0.5523\n",
      "Epoch 19/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.5210 - acc: 0.5818 - score: 0.6437\n",
      "Epoch 00019: val_score did not improve from 0.60905\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 1.5214 - acc: 0.5816 - score: 0.6430 - val_loss: 2.0330 - val_acc: 0.4268 - val_score: 0.4977\n",
      "Epoch 20/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.4836 - acc: 0.5955 - score: 0.6538\n",
      "Epoch 00020: val_score did not improve from 0.60905\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 1.4844 - acc: 0.5948 - score: 0.6515 - val_loss: 1.8650 - val_acc: 0.4454 - val_score: 0.5085\n",
      "Epoch 21/300\n",
      "5696/5837 [============================>.] - ETA: 0s - loss: 1.4661 - acc: 0.5985 - score: 0.6575\n",
      "Epoch 00021: val_score did not improve from 0.60905\n",
      "5837/5837 [==============================] - 1s 198us/sample - loss: 1.4652 - acc: 0.5983 - score: 0.6567 - val_loss: 2.3433 - val_acc: 0.3416 - val_score: 0.4119\n",
      "Epoch 22/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.4524 - acc: 0.6080 - score: 0.6655\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00022: val_score did not improve from 0.60905\n",
      "5837/5837 [==============================] - 1s 199us/sample - loss: 1.4534 - acc: 0.6077 - score: 0.6641 - val_loss: 1.8488 - val_acc: 0.4969 - val_score: 0.5641\n",
      "Epoch 23/300\n",
      "5696/5837 [============================>.] - ETA: 0s - loss: 1.3619 - acc: 0.6354 - score: 0.6903\n",
      "Epoch 00023: val_score improved from 0.60905 to 0.61871, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 211us/sample - loss: 1.3628 - acc: 0.6351 - score: 0.6880 - val_loss: 1.6354 - val_acc: 0.5601 - val_score: 0.6187\n",
      "Epoch 24/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.3071 - acc: 0.6524 - score: 0.7045\n",
      "Epoch 00024: val_score improved from 0.61871 to 0.64793, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 205us/sample - loss: 1.3084 - acc: 0.6526 - score: 0.7049 - val_loss: 1.5595 - val_acc: 0.5863 - val_score: 0.6479\n",
      "Epoch 25/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.2695 - acc: 0.6693 - score: 0.7200\n",
      "Epoch 00025: val_score did not improve from 0.64793\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 1.2705 - acc: 0.6687 - score: 0.7172 - val_loss: 1.8937 - val_acc: 0.4845 - val_score: 0.5554\n",
      "Epoch 26/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.2504 - acc: 0.6681 - score: 0.7191\n",
      "Epoch 00026: val_score improved from 0.64793 to 0.66685, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 205us/sample - loss: 1.2508 - acc: 0.6675 - score: 0.7161 - val_loss: 1.4574 - val_acc: 0.6110 - val_score: 0.6669\n",
      "Epoch 27/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.2465 - acc: 0.6683 - score: 0.7187\n",
      "Epoch 00027: val_score did not improve from 0.66685\n",
      "5837/5837 [==============================] - 1s 196us/sample - loss: 1.2478 - acc: 0.6680 - score: 0.7175 - val_loss: 1.5529 - val_acc: 0.5567 - val_score: 0.6284\n",
      "Epoch 28/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.2146 - acc: 0.6794 - score: 0.7279\n",
      "Epoch 00028: val_score did not improve from 0.66685\n",
      "5837/5837 [==============================] - 1s 198us/sample - loss: 1.2171 - acc: 0.6788 - score: 0.7250 - val_loss: 1.5333 - val_acc: 0.5560 - val_score: 0.6198\n",
      "Epoch 29/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.2272 - acc: 0.6795 - score: 0.7274\n",
      "Epoch 00029: val_score did not improve from 0.66685\n",
      "5837/5837 [==============================] - 1s 198us/sample - loss: 1.2287 - acc: 0.6791 - score: 0.7251 - val_loss: 1.5319 - val_acc: 0.5739 - val_score: 0.6354\n",
      "Epoch 30/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.2060 - acc: 0.6807 - score: 0.7290\n",
      "Epoch 00030: val_score did not improve from 0.66685\n",
      "5837/5837 [==============================] - 1s 198us/sample - loss: 1.2075 - acc: 0.6805 - score: 0.7272 - val_loss: 1.4505 - val_acc: 0.6000 - val_score: 0.6587\n",
      "Epoch 31/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1737 - acc: 0.6918 - score: 0.7390\n",
      "Epoch 00031: val_score improved from 0.66685 to 0.66788, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 207us/sample - loss: 1.1756 - acc: 0.6909 - score: 0.7356 - val_loss: 1.4743 - val_acc: 0.6027 - val_score: 0.6679\n",
      "Epoch 32/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1791 - acc: 0.6904 - score: 0.7382\n",
      "Epoch 00032: val_score improved from 0.66788 to 0.67340, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 205us/sample - loss: 1.1813 - acc: 0.6897 - score: 0.7347 - val_loss: 1.4088 - val_acc: 0.6192 - val_score: 0.6734\n",
      "Epoch 33/300\n",
      "5632/5837 [===========================>..] - ETA: 0s - loss: 1.1239 - acc: 0.7159 - score: 0.7572\n",
      "Epoch 00033: val_score did not improve from 0.67340\n",
      "5837/5837 [==============================] - 1s 200us/sample - loss: 1.1275 - acc: 0.7148 - score: 0.7554 - val_loss: 1.6267 - val_acc: 0.5595 - val_score: 0.6211\n",
      "Epoch 34/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1424 - acc: 0.7060 - score: 0.7510\n",
      "Epoch 00034: val_score did not improve from 0.67340\n",
      "5837/5837 [==============================] - 1s 198us/sample - loss: 1.1433 - acc: 0.7055 - score: 0.7485 - val_loss: 1.4235 - val_acc: 0.6055 - val_score: 0.6653\n",
      "Epoch 35/300\n",
      "5696/5837 [============================>.] - ETA: 0s - loss: 1.1275 - acc: 0.7142 - score: 0.7589\n",
      "Epoch 00035: val_score did not improve from 0.67340\n",
      "5837/5837 [==============================] - 1s 200us/sample - loss: 1.1286 - acc: 0.7127 - score: 0.7551 - val_loss: 1.4724 - val_acc: 0.6062 - val_score: 0.6655\n",
      "Epoch 36/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.1222 - acc: 0.7127 - score: 0.7563\n",
      "Epoch 00036: val_score improved from 0.67340 to 0.69048, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 208us/sample - loss: 1.1232 - acc: 0.7122 - score: 0.7539 - val_loss: 1.3646 - val_acc: 0.6364 - val_score: 0.6905\n",
      "Epoch 37/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0957 - acc: 0.7191 - score: 0.7617\n",
      "Epoch 00037: val_score did not improve from 0.69048\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 1.0967 - acc: 0.7187 - score: 0.7597 - val_loss: 1.5101 - val_acc: 0.6089 - val_score: 0.6666\n",
      "Epoch 38/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0747 - acc: 0.7313 - score: 0.7728\n",
      "Epoch 00038: val_score did not improve from 0.69048\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 1.0752 - acc: 0.7312 - score: 0.7722 - val_loss: 1.4543 - val_acc: 0.6069 - val_score: 0.6626\n",
      "Epoch 39/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5760/5837 [============================>.] - ETA: 0s - loss: 1.0927 - acc: 0.7255 - score: 0.7671\n",
      "Epoch 00039: val_score did not improve from 0.69048\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 1.0934 - acc: 0.7257 - score: 0.7646 - val_loss: 1.4389 - val_acc: 0.6048 - val_score: 0.6611\n",
      "Epoch 40/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0701 - acc: 0.7340 - score: 0.7745\n",
      "Epoch 00040: val_score improved from 0.69048 to 0.72671, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 207us/sample - loss: 1.0709 - acc: 0.7334 - score: 0.7720 - val_loss: 1.2603 - val_acc: 0.6708 - val_score: 0.7267\n",
      "Epoch 41/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0707 - acc: 0.7363 - score: 0.7765\n",
      "Epoch 00041: val_score did not improve from 0.72671\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 1.0710 - acc: 0.7363 - score: 0.7768 - val_loss: 1.4503 - val_acc: 0.6082 - val_score: 0.6655\n",
      "Epoch 42/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0472 - acc: 0.7419 - score: 0.7814\n",
      "Epoch 00042: val_score did not improve from 0.72671\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 1.0486 - acc: 0.7413 - score: 0.7790 - val_loss: 1.3900 - val_acc: 0.6234 - val_score: 0.6824\n",
      "Epoch 43/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0243 - acc: 0.7589 - score: 0.7964\n",
      "Epoch 00043: val_score improved from 0.72671 to 0.72873, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 205us/sample - loss: 1.0257 - acc: 0.7584 - score: 0.7945 - val_loss: 1.2719 - val_acc: 0.6756 - val_score: 0.7287\n",
      "Epoch 44/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0036 - acc: 0.7586 - score: 0.7958\n",
      "Epoch 00044: val_score did not improve from 0.72873\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 1.0048 - acc: 0.7584 - score: 0.7950 - val_loss: 1.4445 - val_acc: 0.6186 - val_score: 0.6776\n",
      "Epoch 45/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0372 - acc: 0.7478 - score: 0.7859\n",
      "Epoch 00045: val_score did not improve from 0.72873\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 1.0376 - acc: 0.7475 - score: 0.7846 - val_loss: 1.5414 - val_acc: 0.5835 - val_score: 0.6503\n",
      "Epoch 46/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0165 - acc: 0.7515 - score: 0.7889\n",
      "Epoch 00046: val_score did not improve from 0.72873\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 1.0175 - acc: 0.7511 - score: 0.7872 - val_loss: 1.4594 - val_acc: 0.6254 - val_score: 0.6777\n",
      "Epoch 47/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0132 - acc: 0.7541 - score: 0.7923\n",
      "Epoch 00047: val_score did not improve from 0.72873\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 1.0131 - acc: 0.7543 - score: 0.7932 - val_loss: 1.6939 - val_acc: 0.5354 - val_score: 0.6078\n",
      "Epoch 48/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 1.0197 - acc: 0.7576 - score: 0.7942\n",
      "Epoch 00048: val_score did not improve from 0.72873\n",
      "5837/5837 [==============================] - 1s 196us/sample - loss: 1.0210 - acc: 0.7572 - score: 0.7927 - val_loss: 1.2460 - val_acc: 0.6790 - val_score: 0.7283\n",
      "Epoch 49/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.9964 - acc: 0.7679 - score: 0.8029\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00049: val_score did not improve from 0.72873\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 0.9982 - acc: 0.7670 - score: 0.7991 - val_loss: 1.4000 - val_acc: 0.6378 - val_score: 0.6859\n",
      "Epoch 50/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.9105 - acc: 0.7967 - score: 0.8295\n",
      "Epoch 00050: val_score improved from 0.72873 to 0.77098, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 205us/sample - loss: 0.9111 - acc: 0.7965 - score: 0.8283 - val_loss: 1.1519 - val_acc: 0.7278 - val_score: 0.7710\n",
      "Epoch 51/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.8488 - acc: 0.8141 - score: 0.8419\n",
      "Epoch 00051: val_score did not improve from 0.77098\n",
      "5837/5837 [==============================] - 1s 196us/sample - loss: 0.8515 - acc: 0.8126 - score: 0.8365 - val_loss: 1.1724 - val_acc: 0.7113 - val_score: 0.7577\n",
      "Epoch 52/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.8357 - acc: 0.8185 - score: 0.8472\n",
      "Epoch 00052: val_score improved from 0.77098 to 0.78389, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 205us/sample - loss: 0.8375 - acc: 0.8179 - score: 0.8441 - val_loss: 1.1041 - val_acc: 0.7443 - val_score: 0.7839\n",
      "Epoch 53/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.8283 - acc: 0.8199 - score: 0.8478\n",
      "Epoch 00053: val_score did not improve from 0.78389\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.8289 - acc: 0.8196 - score: 0.8469 - val_loss: 1.1452 - val_acc: 0.7120 - val_score: 0.7557\n",
      "Epoch 54/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.8249 - acc: 0.8255 - score: 0.8537\n",
      "Epoch 00054: val_score did not improve from 0.78389\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 0.8269 - acc: 0.8253 - score: 0.8512 - val_loss: 1.1267 - val_acc: 0.7182 - val_score: 0.7598\n",
      "Epoch 55/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.8289 - acc: 0.8199 - score: 0.8485\n",
      "Epoch 00055: val_score did not improve from 0.78389\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.8302 - acc: 0.8193 - score: 0.8456 - val_loss: 1.1530 - val_acc: 0.7251 - val_score: 0.7660\n",
      "Epoch 56/300\n",
      "5632/5837 [===========================>..] - ETA: 0s - loss: 0.8145 - acc: 0.8303 - score: 0.8560\n",
      "Epoch 00056: val_score did not improve from 0.78389\n",
      "5837/5837 [==============================] - 1s 198us/sample - loss: 0.8140 - acc: 0.8302 - score: 0.8544 - val_loss: 1.3488 - val_acc: 0.6687 - val_score: 0.7192\n",
      "Epoch 57/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.8172 - acc: 0.8218 - score: 0.8494\n",
      "Epoch 00057: val_score did not improve from 0.78389\n",
      "5837/5837 [==============================] - 1s 192us/sample - loss: 0.8178 - acc: 0.8215 - score: 0.8481 - val_loss: 1.1409 - val_acc: 0.7278 - val_score: 0.7721\n",
      "Epoch 58/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.7899 - acc: 0.8352 - score: 0.8600\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00058: val_score did not improve from 0.78389\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 0.7907 - acc: 0.8352 - score: 0.8599 - val_loss: 1.2374 - val_acc: 0.6880 - val_score: 0.7346\n",
      "Epoch 59/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.7236 - acc: 0.8583 - score: 0.8803\n",
      "Epoch 00059: val_score improved from 0.78389 to 0.82036, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 206us/sample - loss: 0.7250 - acc: 0.8580 - score: 0.8786 - val_loss: 0.9775 - val_acc: 0.7876 - val_score: 0.8204\n",
      "Epoch 60/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.6978 - acc: 0.8707 - score: 0.8917\n",
      "Epoch 00060: val_score did not improve from 0.82036\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.6996 - acc: 0.8700 - score: 0.8882 - val_loss: 0.9914 - val_acc: 0.7574 - val_score: 0.7945\n",
      "Epoch 61/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.8711 - score: 0.8918\n",
      "Epoch 00061: val_score did not improve from 0.82036\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.6936 - acc: 0.8703 - score: 0.8890 - val_loss: 0.9714 - val_acc: 0.7828 - val_score: 0.8163\n",
      "Epoch 62/300\n",
      "5696/5837 [============================>.] - ETA: 0s - loss: 0.6761 - acc: 0.8789 - score: 0.8992\n",
      "Epoch 00062: val_score did not improve from 0.82036\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 0.6761 - acc: 0.8787 - score: 0.8981 - val_loss: 1.0067 - val_acc: 0.7588 - val_score: 0.7963\n",
      "Epoch 63/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.8721 - score: 0.8920\n",
      "Epoch 00063: val_score did not improve from 0.82036\n",
      "5837/5837 [==============================] - 1s 196us/sample - loss: 0.6815 - acc: 0.8719 - score: 0.8910 - val_loss: 0.9765 - val_acc: 0.7814 - val_score: 0.8132\n",
      "Epoch 64/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.6737 - acc: 0.8777 - score: 0.8970\n",
      "Epoch 00064: val_score did not improve from 0.82036\n",
      "5837/5837 [==============================] - 1s 196us/sample - loss: 0.6744 - acc: 0.8772 - score: 0.8945 - val_loss: 1.0021 - val_acc: 0.7588 - val_score: 0.7971\n",
      "Epoch 65/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.6613 - acc: 0.8772 - score: 0.8973\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00065: val_score did not improve from 0.82036\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.6623 - acc: 0.8765 - score: 0.8942 - val_loss: 0.9642 - val_acc: 0.7801 - val_score: 0.8144\n",
      "Epoch 66/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.6150 - acc: 0.8995 - score: 0.9151\n",
      "Epoch 00066: val_score improved from 0.82036 to 0.82170, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 209us/sample - loss: 0.6168 - acc: 0.8989 - score: 0.9134 - val_loss: 0.9267 - val_acc: 0.7883 - val_score: 0.8217\n",
      "Epoch 67/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.6253 - acc: 0.8932 - score: 0.9112\n",
      "Epoch 00067: val_score improved from 0.82170 to 0.82686, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 207us/sample - loss: 0.6264 - acc: 0.8928 - score: 0.9089 - val_loss: 0.9161 - val_acc: 0.7952 - val_score: 0.8269\n",
      "Epoch 68/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.6052 - acc: 0.9011 - score: 0.9169\n",
      "Epoch 00068: val_score improved from 0.82686 to 0.83137, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 207us/sample - loss: 0.6060 - acc: 0.9010 - score: 0.9161 - val_loss: 0.8936 - val_acc: 0.8014 - val_score: 0.8314\n",
      "Epoch 69/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.6083 - acc: 0.8997 - score: 0.9156\n",
      "Epoch 00069: val_score did not improve from 0.83137\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 0.6092 - acc: 0.8993 - score: 0.9134 - val_loss: 0.9109 - val_acc: 0.7924 - val_score: 0.8256\n",
      "Epoch 70/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.6068 - acc: 0.9020 - score: 0.9174\n",
      "Epoch 00070: val_score improved from 0.83137 to 0.83171, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 204us/sample - loss: 0.6077 - acc: 0.9018 - score: 0.9170 - val_loss: 0.9091 - val_acc: 0.8021 - val_score: 0.8317\n",
      "Epoch 71/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.9011 - score: 0.9167\n",
      "Epoch 00071: val_score did not improve from 0.83171\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.5964 - acc: 0.9010 - score: 0.9164 - val_loss: 0.9032 - val_acc: 0.7973 - val_score: 0.8275\n",
      "Epoch 72/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.5876 - acc: 0.9069 - score: 0.9220\n",
      "Epoch 00072: val_score did not improve from 0.83171\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 0.5900 - acc: 0.9063 - score: 0.9203 - val_loss: 0.9075 - val_acc: 0.7973 - val_score: 0.8279\n",
      "Epoch 73/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5798 - acc: 0.9121 - score: 0.9254\n",
      "Epoch 00073: val_score improved from 0.83171 to 0.83325, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 205us/sample - loss: 0.5808 - acc: 0.9116 - score: 0.9234 - val_loss: 0.8885 - val_acc: 0.8027 - val_score: 0.8332\n",
      "Epoch 74/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.9105 - score: 0.9246\n",
      "Epoch 00074: val_score did not improve from 0.83325\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.5805 - acc: 0.9102 - score: 0.9231 - val_loss: 0.8785 - val_acc: 0.8027 - val_score: 0.8326\n",
      "Epoch 75/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5764 - acc: 0.9109 - score: 0.9260\n",
      "Epoch 00075: val_score improved from 0.83325 to 0.83521, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 205us/sample - loss: 0.5784 - acc: 0.9101 - score: 0.9223 - val_loss: 0.8882 - val_acc: 0.8048 - val_score: 0.8352\n",
      "Epoch 76/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5810 - acc: 0.9116 - score: 0.9253\n",
      "Epoch 00076: val_score did not improve from 0.83521\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.5815 - acc: 0.9113 - score: 0.9242 - val_loss: 0.9035 - val_acc: 0.7911 - val_score: 0.8240\n",
      "Epoch 77/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5668 - acc: 0.9164 - score: 0.9294\n",
      "Epoch 00077: val_score did not improve from 0.83521\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.5686 - acc: 0.9155 - score: 0.9255 - val_loss: 0.9085 - val_acc: 0.7959 - val_score: 0.8264\n",
      "Epoch 78/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.5791 - acc: 0.9080 - score: 0.9230\n",
      "Epoch 00078: val_score did not improve from 0.83521\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 0.5798 - acc: 0.9077 - score: 0.9220 - val_loss: 0.9028 - val_acc: 0.8021 - val_score: 0.8314\n",
      "Epoch 79/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.9196 - score: 0.9321\n",
      "Epoch 00079: val_score did not improve from 0.83521\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 0.5558 - acc: 0.9193 - score: 0.9306 - val_loss: 0.8922 - val_acc: 0.7904 - val_score: 0.8234\n",
      "Epoch 80/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.5580 - acc: 0.9142 - score: 0.9285\n",
      "Epoch 00080: val_score improved from 0.83521 to 0.83616, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 211us/sample - loss: 0.5597 - acc: 0.9135 - score: 0.9252 - val_loss: 0.9074 - val_acc: 0.8055 - val_score: 0.8362\n",
      "Epoch 81/300\n",
      "5568/5837 [===========================>..] - ETA: 0s - loss: 0.5663 - acc: 0.9100 - score: 0.9247\n",
      "Epoch 00081: val_score did not improve from 0.83616\n",
      "5837/5837 [==============================] - 1s 199us/sample - loss: 0.5649 - acc: 0.9109 - score: 0.9243 - val_loss: 0.9080 - val_acc: 0.7986 - val_score: 0.8305\n",
      "Epoch 82/300\n",
      "5632/5837 [===========================>..] - ETA: 0s - loss: 0.5549 - acc: 0.9160 - score: 0.9296\n",
      "Epoch 00082: val_score did not improve from 0.83616\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 0.5570 - acc: 0.9150 - score: 0.9271 - val_loss: 0.9284 - val_acc: 0.7945 - val_score: 0.8246\n",
      "Epoch 83/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.9214 - score: 0.9345\n",
      "Epoch 00083: val_score did not improve from 0.83616\n",
      "5837/5837 [==============================] - 1s 192us/sample - loss: 0.5427 - acc: 0.9208 - score: 0.9325 - val_loss: 0.9079 - val_acc: 0.8007 - val_score: 0.8302\n",
      "Epoch 84/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.9182 - score: 0.9321\n",
      "Epoch 00084: val_score did not improve from 0.83616\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 0.5434 - acc: 0.9179 - score: 0.9291 - val_loss: 0.8922 - val_acc: 0.7938 - val_score: 0.8253\n",
      "Epoch 85/300\n",
      "5632/5837 [===========================>..] - ETA: 0s - loss: 0.5344 - acc: 0.9237 - score: 0.9364\n",
      "Epoch 00085: val_score did not improve from 0.83616\n",
      "5837/5837 [==============================] - 1s 200us/sample - loss: 0.5351 - acc: 0.9224 - score: 0.9354 - val_loss: 0.8918 - val_acc: 0.8021 - val_score: 0.8320\n",
      "Epoch 86/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.9309 - score: 0.9419\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00086: val_score did not improve from 0.83616\n",
      "5837/5837 [==============================] - 1s 198us/sample - loss: 0.5253 - acc: 0.9306 - score: 0.9396 - val_loss: 0.9113 - val_acc: 0.7993 - val_score: 0.8297\n",
      "Epoch 87/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.9277 - score: 0.9391\n",
      "Epoch 00087: val_score improved from 0.83616 to 0.83921, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 206us/sample - loss: 0.5235 - acc: 0.9274 - score: 0.9376 - val_loss: 0.8649 - val_acc: 0.8110 - val_score: 0.8392\n",
      "Epoch 88/300\n",
      "5632/5837 [===========================>..] - ETA: 0s - loss: 0.5132 - acc: 0.9293 - score: 0.9401\n",
      "Epoch 00088: val_score did not improve from 0.83921\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 0.5166 - acc: 0.9282 - score: 0.9386 - val_loss: 0.8720 - val_acc: 0.8082 - val_score: 0.8379\n",
      "Epoch 89/300\n",
      "5568/5837 [===========================>..] - ETA: 0s - loss: 0.5168 - acc: 0.9305 - score: 0.9414\n",
      "Epoch 00089: val_score improved from 0.83921 to 0.84068, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 210us/sample - loss: 0.5186 - acc: 0.9291 - score: 0.9388 - val_loss: 0.8620 - val_acc: 0.8110 - val_score: 0.8407\n",
      "Epoch 90/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.9302 - score: 0.9417\n",
      "Epoch 00090: val_score did not improve from 0.84068\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.5154 - acc: 0.9303 - score: 0.9416 - val_loss: 0.8653 - val_acc: 0.8007 - val_score: 0.8308\n",
      "Epoch 91/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5018 - acc: 0.9358 - score: 0.9456\n",
      "Epoch 00091: val_score did not improve from 0.84068\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 0.5026 - acc: 0.9356 - score: 0.9445 - val_loss: 0.8675 - val_acc: 0.8103 - val_score: 0.8383\n",
      "Epoch 92/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.9305 - score: 0.9418\n",
      "Epoch 00092: val_score did not improve from 0.84068\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.5142 - acc: 0.9294 - score: 0.9373 - val_loss: 0.8733 - val_acc: 0.8117 - val_score: 0.8387\n",
      "Epoch 93/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.9351 - score: 0.9454\n",
      "Epoch 00093: val_score did not improve from 0.84068\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.5075 - acc: 0.9340 - score: 0.9440 - val_loss: 0.8717 - val_acc: 0.8117 - val_score: 0.8397\n",
      "Epoch 94/300\n",
      "5696/5837 [============================>.] - ETA: 0s - loss: 0.5001 - acc: 0.9338 - score: 0.9446\n",
      "Epoch 00094: val_score did not improve from 0.84068\n",
      "5837/5837 [==============================] - 1s 200us/sample - loss: 0.5006 - acc: 0.9334 - score: 0.9440 - val_loss: 0.8674 - val_acc: 0.8055 - val_score: 0.8349\n",
      "Epoch 95/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.9342 - score: 0.9449\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00095: val_score did not improve from 0.84068\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 0.5026 - acc: 0.9339 - score: 0.9433 - val_loss: 0.8744 - val_acc: 0.8110 - val_score: 0.8399\n",
      "Epoch 96/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4852 - acc: 0.9387 - score: 0.9484\n",
      "Epoch 00096: val_score did not improve from 0.84068\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.4863 - acc: 0.9385 - score: 0.9476 - val_loss: 0.8574 - val_acc: 0.8110 - val_score: 0.8397\n",
      "Epoch 97/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4817 - acc: 0.9440 - score: 0.9537\n",
      "Epoch 00097: val_score improved from 0.84068 to 0.84668, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 205us/sample - loss: 0.4839 - acc: 0.9431 - score: 0.9499 - val_loss: 0.8521 - val_acc: 0.8199 - val_score: 0.8467\n",
      "Epoch 98/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4760 - acc: 0.9473 - score: 0.9555\n",
      "Epoch 00098: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.4766 - acc: 0.9471 - score: 0.9544 - val_loss: 0.8457 - val_acc: 0.8165 - val_score: 0.8450\n",
      "Epoch 99/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.9389 - score: 0.9490\n",
      "Epoch 00099: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 196us/sample - loss: 0.4897 - acc: 0.9375 - score: 0.9458 - val_loss: 0.8541 - val_acc: 0.8144 - val_score: 0.8427\n",
      "Epoch 100/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4771 - acc: 0.9433 - score: 0.9530\n",
      "Epoch 00100: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 0.4776 - acc: 0.9433 - score: 0.9527 - val_loss: 0.8542 - val_acc: 0.8124 - val_score: 0.8397\n",
      "Epoch 101/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.4852 - acc: 0.9401 - score: 0.9500\n",
      "Epoch 00101: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 197us/sample - loss: 0.4867 - acc: 0.9399 - score: 0.9485 - val_loss: 0.8599 - val_acc: 0.8131 - val_score: 0.8419\n",
      "Epoch 102/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4775 - acc: 0.9464 - score: 0.9547\n",
      "Epoch 00102: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 0.4793 - acc: 0.9459 - score: 0.9522 - val_loss: 0.8526 - val_acc: 0.8089 - val_score: 0.8365\n",
      "Epoch 103/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4772 - acc: 0.9427 - score: 0.9522\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00103: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 0.4795 - acc: 0.9418 - score: 0.9486 - val_loss: 0.8545 - val_acc: 0.8103 - val_score: 0.8390\n",
      "Epoch 104/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.9406 - score: 0.9497\n",
      "Epoch 00104: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 0.4827 - acc: 0.9404 - score: 0.9491 - val_loss: 0.8515 - val_acc: 0.8144 - val_score: 0.8411\n",
      "Epoch 105/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.9454 - score: 0.9540\n",
      "Epoch 00105: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 0.4720 - acc: 0.9448 - score: 0.9518 - val_loss: 0.8539 - val_acc: 0.8137 - val_score: 0.8397\n",
      "Epoch 106/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4789 - acc: 0.9449 - score: 0.9542\n",
      "Epoch 00106: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 195us/sample - loss: 0.4804 - acc: 0.9438 - score: 0.9496 - val_loss: 0.8488 - val_acc: 0.8131 - val_score: 0.8411\n",
      "Epoch 107/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4760 - acc: 0.9408 - score: 0.9499\n",
      "Epoch 00107: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 194us/sample - loss: 0.4766 - acc: 0.9404 - score: 0.9485 - val_loss: 0.8494 - val_acc: 0.8117 - val_score: 0.8391\n",
      "Epoch 108/300\n",
      "5760/5837 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.9431 - score: 0.9521\n",
      "Epoch 00108: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 196us/sample - loss: 0.4705 - acc: 0.9431 - score: 0.9514 - val_loss: 0.8455 - val_acc: 0.8124 - val_score: 0.8393\n",
      "Epoch 109/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4787 - acc: 0.9416 - score: 0.9505\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00109: val_score did not improve from 0.84668\n",
      "5837/5837 [==============================] - 1s 193us/sample - loss: 0.4793 - acc: 0.9412 - score: 0.9486 - val_loss: 0.8473 - val_acc: 0.8158 - val_score: 0.8442\n",
      "Epoch 110/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4791 - acc: 0.9435 - score: 0.9524\n",
      "Epoch 00110: val_score improved from 0.84668 to 0.84694, saving model to fold3.h5\n",
      "5837/5837 [==============================] - 1s 201us/sample - loss: 0.4804 - acc: 0.9431 - score: 0.9508 - val_loss: 0.8424 - val_acc: 0.8192 - val_score: 0.8469\n",
      "Epoch 111/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4652 - acc: 0.9475 - score: 0.9558\n",
      "Epoch 00111: val_score did not improve from 0.84694\n",
      "5837/5837 [==============================] - 1s 191us/sample - loss: 0.4668 - acc: 0.9467 - score: 0.9524 - val_loss: 0.8454 - val_acc: 0.8110 - val_score: 0.8386\n",
      "Epoch 112/300\n",
      "5824/5837 [============================>.] - ETA: 0s - loss: 0.4654 - acc: 0.9463 - score: 0.9549\n",
      "Epoch 00112: val_score did not improve from 0.84694\n",
      "5837/5837 [==============================] - 1s 189us/sample - loss: 0.4671 - acc: 0.9455 - score: 0.9517 - val_loss: 0.8455 - val_acc: 0.8124 - val_score: 0.8397\n",
      "Epoch 00112: early stopping\n",
      "Train on 5841 samples, validate on 1451 samples\n",
      "Epoch 1/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 8.5500 - acc: 0.3278 - score: 0.4082\n",
      "Epoch 00001: val_score improved from -inf to 0.21126, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 3s 487us/sample - loss: 8.5390 - acc: 0.3277 - score: 0.4080 - val_loss: 5.3368 - val_acc: 0.1268 - val_score: 0.2113\n",
      "Epoch 2/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 3.4027 - acc: 0.4133 - score: 0.4898\n",
      "Epoch 00002: val_score improved from 0.21126 to 0.29781, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 205us/sample - loss: 3.4026 - acc: 0.4131 - score: 0.4891 - val_loss: 3.6894 - val_acc: 0.1950 - val_score: 0.2978\n",
      "Epoch 3/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 2.4987 - acc: 0.4466 - score: 0.5214\n",
      "Epoch 00003: val_score improved from 0.29781 to 0.31437, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 203us/sample - loss: 2.4991 - acc: 0.4463 - score: 0.5203 - val_loss: 3.1717 - val_acc: 0.2123 - val_score: 0.3144\n",
      "Epoch 4/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 2.2395 - acc: 0.4519 - score: 0.5239\n",
      "Epoch 00004: val_score did not improve from 0.31437\n",
      "5841/5841 [==============================] - 1s 193us/sample - loss: 2.2399 - acc: 0.4515 - score: 0.5222 - val_loss: 2.8820 - val_acc: 0.2212 - val_score: 0.3009\n",
      "Epoch 5/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 2.0978 - acc: 0.4624 - score: 0.5369\n",
      "Epoch 00005: val_score improved from 0.31437 to 0.38744, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 202us/sample - loss: 2.0986 - acc: 0.4617 - score: 0.5353 - val_loss: 2.7148 - val_acc: 0.2936 - val_score: 0.3874\n",
      "Epoch 6/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 2.0207 - acc: 0.4760 - score: 0.5495\n",
      "Epoch 00006: val_score improved from 0.38744 to 0.40732, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 202us/sample - loss: 2.0214 - acc: 0.4756 - score: 0.5486 - val_loss: 2.5441 - val_acc: 0.3343 - val_score: 0.4073\n",
      "Epoch 7/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.9491 - acc: 0.4863 - score: 0.5579\n",
      "Epoch 00007: val_score did not improve from 0.40732\n",
      "5841/5841 [==============================] - 1s 190us/sample - loss: 1.9498 - acc: 0.4855 - score: 0.5555 - val_loss: 2.5623 - val_acc: 0.2708 - val_score: 0.3440\n",
      "Epoch 8/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.9088 - acc: 0.4876 - score: 0.5589\n",
      "Epoch 00008: val_score improved from 0.40732 to 0.41990, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 204us/sample - loss: 1.9089 - acc: 0.4874 - score: 0.5583 - val_loss: 2.5594 - val_acc: 0.3446 - val_score: 0.4199\n",
      "Epoch 9/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.8778 - acc: 0.4895 - score: 0.5646\n",
      "Epoch 00009: val_score improved from 0.41990 to 0.44805, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 205us/sample - loss: 1.8782 - acc: 0.4895 - score: 0.5643 - val_loss: 2.3598 - val_acc: 0.3611 - val_score: 0.4480\n",
      "Epoch 10/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.8695 - acc: 0.5005 - score: 0.5705\n",
      "Epoch 00010: val_score did not improve from 0.44805\n",
      "5841/5841 [==============================] - 1s 193us/sample - loss: 1.8711 - acc: 0.4997 - score: 0.5677 - val_loss: 2.3966 - val_acc: 0.3101 - val_score: 0.3870\n",
      "Epoch 11/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.8175 - acc: 0.5086 - score: 0.5804\n",
      "Epoch 00011: val_score did not improve from 0.44805\n",
      "5841/5841 [==============================] - 1s 192us/sample - loss: 1.8187 - acc: 0.5081 - score: 0.5790 - val_loss: 3.8087 - val_acc: 0.2123 - val_score: 0.2908\n",
      "Epoch 12/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 1.8089 - acc: 0.5122 - score: 0.5827\n",
      "Epoch 00012: val_score improved from 0.44805 to 0.49302, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 206us/sample - loss: 1.8096 - acc: 0.5112 - score: 0.5804 - val_loss: 2.1938 - val_acc: 0.4018 - val_score: 0.4930\n",
      "Epoch 13/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.7535 - acc: 0.5237 - score: 0.5926\n",
      "Epoch 00013: val_score did not improve from 0.49302\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.7542 - acc: 0.5234 - score: 0.5913 - val_loss: 2.6922 - val_acc: 0.2750 - val_score: 0.3498\n",
      "Epoch 14/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.7448 - acc: 0.5352 - score: 0.6022\n",
      "Epoch 00014: val_score did not improve from 0.49302\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.7449 - acc: 0.5350 - score: 0.6019 - val_loss: 2.4059 - val_acc: 0.3315 - val_score: 0.4049\n",
      "Epoch 15/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.7131 - acc: 0.5326 - score: 0.6005\n",
      "Epoch 00015: val_score improved from 0.49302 to 0.53032, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 206us/sample - loss: 1.7134 - acc: 0.5326 - score: 0.6006 - val_loss: 2.1383 - val_acc: 0.4555 - val_score: 0.5303\n",
      "Epoch 16/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.7086 - acc: 0.5275 - score: 0.5961\n",
      "Epoch 00016: val_score did not improve from 0.53032\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.7081 - acc: 0.5275 - score: 0.5963 - val_loss: 2.1284 - val_acc: 0.4218 - val_score: 0.4951\n",
      "Epoch 17/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.6824 - acc: 0.5383 - score: 0.6051\n",
      "Epoch 00017: val_score did not improve from 0.53032\n",
      "5841/5841 [==============================] - 1s 196us/sample - loss: 1.6815 - acc: 0.5388 - score: 0.6065 - val_loss: 2.4326 - val_acc: 0.3501 - val_score: 0.4372\n",
      "Epoch 18/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.6516 - acc: 0.5470 - score: 0.6126\n",
      "Epoch 00018: val_score did not improve from 0.53032\n",
      "5841/5841 [==============================] - 1s 197us/sample - loss: 1.6529 - acc: 0.5472 - score: 0.6132 - val_loss: 2.2029 - val_acc: 0.4011 - val_score: 0.4791\n",
      "Epoch 19/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.6980 - acc: 0.5349 - score: 0.6039\n",
      "Epoch 00019: val_score did not improve from 0.53032\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 1.6985 - acc: 0.5350 - score: 0.6043 - val_loss: 2.9412 - val_acc: 0.2984 - val_score: 0.3825\n",
      "Epoch 20/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 1.6457 - acc: 0.5437 - score: 0.6115\n",
      "Epoch 00020: val_score did not improve from 0.53032\n",
      "5841/5841 [==============================] - 1s 197us/sample - loss: 1.6457 - acc: 0.5437 - score: 0.6083 - val_loss: 2.4022 - val_acc: 0.3653 - val_score: 0.4487\n",
      "Epoch 21/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.6449 - acc: 0.5531 - score: 0.6198\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00021: val_score did not improve from 0.53032\n",
      "5841/5841 [==============================] - 1s 211us/sample - loss: 1.6449 - acc: 0.5528 - score: 0.6191 - val_loss: 2.6728 - val_acc: 0.3046 - val_score: 0.3930\n",
      "Epoch 22/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.5073 - acc: 0.5879 - score: 0.6503\n",
      "Epoch 00022: val_score improved from 0.53032 to 0.55896, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 206us/sample - loss: 1.5082 - acc: 0.5877 - score: 0.6500 - val_loss: 1.9035 - val_acc: 0.4866 - val_score: 0.5590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.4581 - acc: 0.5958 - score: 0.6558\n",
      "Epoch 00023: val_score did not improve from 0.55896\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 1.4593 - acc: 0.5958 - score: 0.6558 - val_loss: 2.8163 - val_acc: 0.3501 - val_score: 0.4401\n",
      "Epoch 24/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.4251 - acc: 0.6219 - score: 0.6782\n",
      "Epoch 00024: val_score did not improve from 0.55896\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 1.4266 - acc: 0.6220 - score: 0.6783 - val_loss: 2.0572 - val_acc: 0.4452 - val_score: 0.5172\n",
      "Epoch 25/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.4164 - acc: 0.6120 - score: 0.6716\n",
      "Epoch 00025: val_score improved from 0.55896 to 0.56991, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 205us/sample - loss: 1.4185 - acc: 0.6109 - score: 0.6683 - val_loss: 1.7595 - val_acc: 0.4990 - val_score: 0.5699\n",
      "Epoch 26/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 1.4339 - acc: 0.6109 - score: 0.6694\n",
      "Epoch 00026: val_score improved from 0.56991 to 0.59116, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 207us/sample - loss: 1.4341 - acc: 0.6103 - score: 0.6670 - val_loss: 1.7556 - val_acc: 0.5162 - val_score: 0.5912\n",
      "Epoch 27/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.3851 - acc: 0.6241 - score: 0.6808\n",
      "Epoch 00027: val_score improved from 0.59116 to 0.59673, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 206us/sample - loss: 1.3863 - acc: 0.6239 - score: 0.6799 - val_loss: 1.7422 - val_acc: 0.5286 - val_score: 0.5967\n",
      "Epoch 28/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.4067 - acc: 0.6231 - score: 0.6784\n",
      "Epoch 00028: val_score did not improve from 0.59673\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.4064 - acc: 0.6234 - score: 0.6792 - val_loss: 1.8127 - val_acc: 0.4928 - val_score: 0.5596\n",
      "Epoch 29/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.3281 - acc: 0.6439 - score: 0.6960\n",
      "Epoch 00029: val_score did not improve from 0.59673\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.3276 - acc: 0.6441 - score: 0.6965 - val_loss: 1.7317 - val_acc: 0.5072 - val_score: 0.5779\n",
      "Epoch 30/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.3266 - acc: 0.6365 - score: 0.6921\n",
      "Epoch 00030: val_score did not improve from 0.59673\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 1.3289 - acc: 0.6359 - score: 0.6894 - val_loss: 1.9951 - val_acc: 0.4845 - val_score: 0.5468\n",
      "Epoch 31/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.3189 - acc: 0.6484 - score: 0.7008\n",
      "Epoch 00031: val_score did not improve from 0.59673\n",
      "5841/5841 [==============================] - 1s 196us/sample - loss: 1.3186 - acc: 0.6485 - score: 0.7010 - val_loss: 1.8690 - val_acc: 0.4914 - val_score: 0.5607\n",
      "Epoch 32/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.3034 - acc: 0.6497 - score: 0.7038\n",
      "Epoch 00032: val_score did not improve from 0.59673\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.3046 - acc: 0.6490 - score: 0.7015 - val_loss: 1.7508 - val_acc: 0.5134 - val_score: 0.5821\n",
      "Epoch 33/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.3039 - acc: 0.6575 - score: 0.7095\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00033: val_score did not improve from 0.59673\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.3046 - acc: 0.6571 - score: 0.7084 - val_loss: 2.2091 - val_acc: 0.4190 - val_score: 0.4891\n",
      "Epoch 34/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.2092 - acc: 0.6842 - score: 0.7325\n",
      "Epoch 00034: val_score improved from 0.59673 to 0.63109, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 205us/sample - loss: 1.2104 - acc: 0.6838 - score: 0.7310 - val_loss: 1.6096 - val_acc: 0.5734 - val_score: 0.6311\n",
      "Epoch 35/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.1589 - acc: 0.6959 - score: 0.7427\n",
      "Epoch 00035: val_score did not improve from 0.63109\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.1602 - acc: 0.6956 - score: 0.7416 - val_loss: 1.7364 - val_acc: 0.5362 - val_score: 0.6046\n",
      "Epoch 36/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.1437 - acc: 0.7043 - score: 0.7491\n",
      "Epoch 00036: val_score did not improve from 0.63109\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.1444 - acc: 0.7040 - score: 0.7481 - val_loss: 1.7233 - val_acc: 0.5369 - val_score: 0.6022\n",
      "Epoch 37/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.1066 - acc: 0.7114 - score: 0.7581\n",
      "Epoch 00037: val_score improved from 0.63109 to 0.66426, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 206us/sample - loss: 1.1062 - acc: 0.7117 - score: 0.7592 - val_loss: 1.4766 - val_acc: 0.6058 - val_score: 0.6643\n",
      "Epoch 38/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 1.1058 - acc: 0.7174 - score: 0.7600\n",
      "Epoch 00038: val_score improved from 0.66426 to 0.69151, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 206us/sample - loss: 1.1045 - acc: 0.7182 - score: 0.7599 - val_loss: 1.3871 - val_acc: 0.6396 - val_score: 0.6915\n",
      "Epoch 39/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.0696 - acc: 0.7279 - score: 0.7708\n",
      "Epoch 00039: val_score did not improve from 0.69151\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 1.0708 - acc: 0.7274 - score: 0.7695 - val_loss: 1.4351 - val_acc: 0.6203 - val_score: 0.6774\n",
      "Epoch 40/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.0647 - acc: 0.7248 - score: 0.7677\n",
      "Epoch 00040: val_score did not improve from 0.69151\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.0654 - acc: 0.7244 - score: 0.7665 - val_loss: 1.4376 - val_acc: 0.6113 - val_score: 0.6717\n",
      "Epoch 41/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.0528 - acc: 0.7373 - score: 0.7776\n",
      "Epoch 00041: val_score did not improve from 0.69151\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.0538 - acc: 0.7370 - score: 0.7771 - val_loss: 1.6363 - val_acc: 0.5555 - val_score: 0.6201\n",
      "Epoch 42/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.0556 - acc: 0.7330 - score: 0.7741\n",
      "Epoch 00042: val_score did not improve from 0.69151\n",
      "5841/5841 [==============================] - 1s 193us/sample - loss: 1.0579 - acc: 0.7322 - score: 0.7715 - val_loss: 1.4325 - val_acc: 0.6189 - val_score: 0.6703\n",
      "Epoch 43/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.0468 - acc: 0.7380 - score: 0.7786\n",
      "Epoch 00043: val_score improved from 0.69151 to 0.70147, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 205us/sample - loss: 1.0475 - acc: 0.7375 - score: 0.7773 - val_loss: 1.3560 - val_acc: 0.6506 - val_score: 0.7015\n",
      "Epoch 44/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.0228 - acc: 0.7466 - score: 0.7861\n",
      "Epoch 00044: val_score improved from 0.70147 to 0.71636, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 207us/sample - loss: 1.0243 - acc: 0.7461 - score: 0.7846 - val_loss: 1.2663 - val_acc: 0.6657 - val_score: 0.7164\n",
      "Epoch 45/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 1.0140 - acc: 0.7554 - score: 0.7942\n",
      "Epoch 00045: val_score did not improve from 0.71636\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 1.0164 - acc: 0.7547 - score: 0.7918 - val_loss: 1.4632 - val_acc: 0.6037 - val_score: 0.6617\n",
      "Epoch 46/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 1.0035 - acc: 0.7466 - score: 0.7864\n",
      "Epoch 00046: val_score did not improve from 0.71636\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 1.0040 - acc: 0.7463 - score: 0.7855 - val_loss: 1.3870 - val_acc: 0.6203 - val_score: 0.6797\n",
      "Epoch 47/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 1.0007 - acc: 0.7465 - score: 0.7874\n",
      "Epoch 00047: val_score did not improve from 0.71636\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 1.0016 - acc: 0.7466 - score: 0.7866 - val_loss: 1.4471 - val_acc: 0.6106 - val_score: 0.6668\n",
      "Epoch 48/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.9836 - acc: 0.7562 - score: 0.7941\n",
      "Epoch 00048: val_score did not improve from 0.71636\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.9853 - acc: 0.7559 - score: 0.7932 - val_loss: 1.5444 - val_acc: 0.5693 - val_score: 0.6321\n",
      "Epoch 49/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.9754 - acc: 0.7646 - score: 0.8004\n",
      "Epoch 00049: val_score did not improve from 0.71636\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.9762 - acc: 0.7641 - score: 0.7991 - val_loss: 1.5829 - val_acc: 0.5748 - val_score: 0.6370\n",
      "Epoch 50/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.9736 - acc: 0.7612 - score: 0.7979\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00050: val_score did not improve from 0.71636\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.9743 - acc: 0.7612 - score: 0.7980 - val_loss: 1.4316 - val_acc: 0.6216 - val_score: 0.6771\n",
      "Epoch 51/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.8934 - acc: 0.7943 - score: 0.8267\n",
      "Epoch 00051: val_score improved from 0.71636 to 0.73513, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 206us/sample - loss: 0.8937 - acc: 0.7940 - score: 0.8259 - val_loss: 1.2058 - val_acc: 0.6885 - val_score: 0.7351\n",
      "Epoch 52/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.8460 - acc: 0.8096 - score: 0.8396\n",
      "Epoch 00052: val_score did not improve from 0.73513\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 0.8461 - acc: 0.8096 - score: 0.8399 - val_loss: 1.2581 - val_acc: 0.6733 - val_score: 0.7233\n",
      "Epoch 53/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.8294 - acc: 0.8140 - score: 0.8430\n",
      "Epoch 00053: val_score improved from 0.73513 to 0.74727, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 206us/sample - loss: 0.8304 - acc: 0.8136 - score: 0.8415 - val_loss: 1.1710 - val_acc: 0.7002 - val_score: 0.7473\n",
      "Epoch 54/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.8334 - acc: 0.8120 - score: 0.8415\n",
      "Epoch 00054: val_score improved from 0.74727 to 0.75020, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 204us/sample - loss: 0.8341 - acc: 0.8115 - score: 0.8397 - val_loss: 1.1736 - val_acc: 0.7043 - val_score: 0.7502\n",
      "Epoch 55/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.8107 - acc: 0.8189 - score: 0.8467\n",
      "Epoch 00055: val_score did not improve from 0.75020\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.8118 - acc: 0.8185 - score: 0.8456 - val_loss: 1.4899 - val_acc: 0.6409 - val_score: 0.6897\n",
      "Epoch 56/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.8196 - acc: 0.8156 - score: 0.8441\n",
      "Epoch 00056: val_score did not improve from 0.75020\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 0.8212 - acc: 0.8148 - score: 0.8415 - val_loss: 1.1618 - val_acc: 0.7016 - val_score: 0.7485\n",
      "Epoch 57/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.8173 - acc: 0.8125 - score: 0.8413\n",
      "Epoch 00057: val_score did not improve from 0.75020\n",
      "5841/5841 [==============================] - 1s 197us/sample - loss: 0.8179 - acc: 0.8124 - score: 0.8410 - val_loss: 1.1573 - val_acc: 0.7037 - val_score: 0.7487\n",
      "Epoch 58/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.7967 - acc: 0.8283 - score: 0.8541\n",
      "Epoch 00058: val_score improved from 0.75020 to 0.76438, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 207us/sample - loss: 0.7978 - acc: 0.8278 - score: 0.8525 - val_loss: 1.1504 - val_acc: 0.7209 - val_score: 0.7644\n",
      "Epoch 59/300\n",
      "5696/5841 [============================>.] - ETA: 0s - loss: 0.7756 - acc: 0.8246 - score: 0.8525\n",
      "Epoch 00059: val_score did not improve from 0.76438\n",
      "5841/5841 [==============================] - 1s 196us/sample - loss: 0.7755 - acc: 0.8243 - score: 0.8520 - val_loss: 1.1842 - val_acc: 0.7071 - val_score: 0.7515\n",
      "Epoch 60/300\n",
      "5696/5841 [============================>.] - ETA: 0s - loss: 0.7593 - acc: 0.8373 - score: 0.8615\n",
      "Epoch 00060: val_score did not improve from 0.76438\n",
      "5841/5841 [==============================] - 1s 197us/sample - loss: 0.7563 - acc: 0.8382 - score: 0.8622 - val_loss: 1.1013 - val_acc: 0.7181 - val_score: 0.7626\n",
      "Epoch 61/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 0.7361 - acc: 0.8429 - score: 0.8675\n",
      "Epoch 00061: val_score did not improve from 0.76438\n",
      "5841/5841 [==============================] - 1s 196us/sample - loss: 0.7383 - acc: 0.8415 - score: 0.8651 - val_loss: 1.1185 - val_acc: 0.7112 - val_score: 0.7547\n",
      "Epoch 62/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.7505 - acc: 0.8346 - score: 0.8611\n",
      "Epoch 00062: val_score improved from 0.76438 to 0.76627, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 205us/sample - loss: 0.7506 - acc: 0.8344 - score: 0.8603 - val_loss: 1.0963 - val_acc: 0.7236 - val_score: 0.7663\n",
      "Epoch 63/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.7310 - acc: 0.8427 - score: 0.8682\n",
      "Epoch 00063: val_score did not improve from 0.76627\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 0.7342 - acc: 0.8416 - score: 0.8646 - val_loss: 1.1164 - val_acc: 0.7174 - val_score: 0.7582\n",
      "Epoch 64/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 0.7473 - acc: 0.8424 - score: 0.8671\n",
      "Epoch 00064: val_score improved from 0.76627 to 0.76752, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 210us/sample - loss: 0.7491 - acc: 0.8413 - score: 0.8640 - val_loss: 1.1199 - val_acc: 0.7250 - val_score: 0.7675\n",
      "Epoch 65/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.7209 - acc: 0.8542 - score: 0.8770\n",
      "Epoch 00065: val_score did not improve from 0.76752\n",
      "5841/5841 [==============================] - 1s 196us/sample - loss: 0.7217 - acc: 0.8538 - score: 0.8753 - val_loss: 1.1836 - val_acc: 0.7043 - val_score: 0.7506\n",
      "Epoch 66/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 0.7286 - acc: 0.8469 - score: 0.8714\n",
      "Epoch 00066: val_score improved from 0.76752 to 0.77186, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 207us/sample - loss: 0.7307 - acc: 0.8461 - score: 0.8704 - val_loss: 1.0748 - val_acc: 0.7264 - val_score: 0.7719\n",
      "Epoch 67/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.7222 - acc: 0.8475 - score: 0.8708\n",
      "Epoch 00067: val_score did not improve from 0.77186\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.7229 - acc: 0.8473 - score: 0.8701 - val_loss: 1.2913 - val_acc: 0.6609 - val_score: 0.7153\n",
      "Epoch 68/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.7125 - acc: 0.8558 - score: 0.8788\n",
      "Epoch 00068: val_score did not improve from 0.77186\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.7136 - acc: 0.8548 - score: 0.8759 - val_loss: 1.1327 - val_acc: 0.7202 - val_score: 0.7643\n",
      "Epoch 69/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.6952 - acc: 0.8594 - score: 0.8813\n",
      "Epoch 00069: val_score did not improve from 0.77186\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.6965 - acc: 0.8593 - score: 0.8811 - val_loss: 1.2383 - val_acc: 0.6747 - val_score: 0.7218\n",
      "Epoch 70/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.7060 - acc: 0.8544 - score: 0.8772\n",
      "Epoch 00070: val_score did not improve from 0.77186\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.7068 - acc: 0.8541 - score: 0.8764 - val_loss: 1.2031 - val_acc: 0.7016 - val_score: 0.7454\n",
      "Epoch 71/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.6907 - acc: 0.8620 - score: 0.8846\n",
      "Epoch 00071: val_score did not improve from 0.77186\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.6935 - acc: 0.8612 - score: 0.8821 - val_loss: 1.2022 - val_acc: 0.6975 - val_score: 0.7458\n",
      "Epoch 72/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.6876 - acc: 0.8645 - score: 0.8864\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00072: val_score did not improve from 0.77186\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.6887 - acc: 0.8644 - score: 0.8863 - val_loss: 1.0890 - val_acc: 0.7250 - val_score: 0.7702\n",
      "Epoch 73/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.6590 - acc: 0.8716 - score: 0.8914\n",
      "Epoch 00073: val_score improved from 0.77186 to 0.78691, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 206us/sample - loss: 0.6592 - acc: 0.8714 - score: 0.8912 - val_loss: 1.0194 - val_acc: 0.7491 - val_score: 0.7869\n",
      "Epoch 74/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.6086 - acc: 0.8922 - score: 0.9093\n",
      "Epoch 00074: val_score did not improve from 0.78691\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.6107 - acc: 0.8913 - score: 0.9064 - val_loss: 1.0267 - val_acc: 0.7457 - val_score: 0.7862\n",
      "Epoch 75/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5918 - acc: 0.8985 - score: 0.9149\n",
      "Epoch 00075: val_score improved from 0.78691 to 0.80026, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 207us/sample - loss: 0.5938 - acc: 0.8978 - score: 0.9126 - val_loss: 0.9930 - val_acc: 0.7650 - val_score: 0.8003\n",
      "Epoch 76/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.9033 - score: 0.9188\n",
      "Epoch 00076: val_score did not improve from 0.80026\n",
      "5841/5841 [==============================] - 1s 193us/sample - loss: 0.5828 - acc: 0.9031 - score: 0.9179 - val_loss: 1.0471 - val_acc: 0.7422 - val_score: 0.7827\n",
      "Epoch 77/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5817 - acc: 0.8992 - score: 0.9147\n",
      "Epoch 00077: val_score did not improve from 0.80026\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.5827 - acc: 0.8988 - score: 0.9133 - val_loss: 1.0077 - val_acc: 0.7595 - val_score: 0.7969\n",
      "Epoch 78/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5863 - acc: 0.8997 - score: 0.9152\n",
      "Epoch 00078: val_score did not improve from 0.80026\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.5890 - acc: 0.8986 - score: 0.9116 - val_loss: 0.9848 - val_acc: 0.7636 - val_score: 0.7994\n",
      "Epoch 79/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.9083 - score: 0.9234\n",
      "Epoch 00079: val_score did not improve from 0.80026\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.5721 - acc: 0.9077 - score: 0.9214 - val_loss: 1.0365 - val_acc: 0.7498 - val_score: 0.7876\n",
      "Epoch 80/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5624 - acc: 0.9071 - score: 0.9222\n",
      "Epoch 00080: val_score did not improve from 0.80026\n",
      "5841/5841 [==============================] - 1s 193us/sample - loss: 0.5638 - acc: 0.9065 - score: 0.9204 - val_loss: 1.0275 - val_acc: 0.7409 - val_score: 0.7819\n",
      "Epoch 81/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.9013 - score: 0.9167\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00081: val_score did not improve from 0.80026\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.5705 - acc: 0.9005 - score: 0.9143 - val_loss: 1.0432 - val_acc: 0.7409 - val_score: 0.7803\n",
      "Epoch 82/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.9181 - score: 0.9310\n",
      "Epoch 00082: val_score improved from 0.80026 to 0.80812, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 204us/sample - loss: 0.5346 - acc: 0.9177 - score: 0.9294 - val_loss: 0.9437 - val_acc: 0.7719 - val_score: 0.8081\n",
      "Epoch 83/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.9219 - score: 0.9344\n",
      "Epoch 00083: val_score did not improve from 0.80812\n",
      "5841/5841 [==============================] - 1s 193us/sample - loss: 0.5258 - acc: 0.9216 - score: 0.9334 - val_loss: 0.9487 - val_acc: 0.7726 - val_score: 0.8071\n",
      "Epoch 84/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.9286 - score: 0.9401\n",
      "Epoch 00084: val_score did not improve from 0.80812\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.5164 - acc: 0.9284 - score: 0.9396 - val_loss: 0.9550 - val_acc: 0.7726 - val_score: 0.8073\n",
      "Epoch 85/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.9208 - score: 0.9336\n",
      "Epoch 00085: val_score did not improve from 0.80812\n",
      "5841/5841 [==============================] - 1s 196us/sample - loss: 0.5268 - acc: 0.9202 - score: 0.9314 - val_loss: 0.9617 - val_acc: 0.7636 - val_score: 0.8022\n",
      "Epoch 86/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.9309 - score: 0.9426\n",
      "Epoch 00086: val_score did not improve from 0.80812\n",
      "5841/5841 [==============================] - 1s 197us/sample - loss: 0.5094 - acc: 0.9303 - score: 0.9411 - val_loss: 0.9598 - val_acc: 0.7719 - val_score: 0.8080\n",
      "Epoch 87/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.9215 - score: 0.9349\n",
      "Epoch 00087: val_score improved from 0.80812 to 0.81776, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 207us/sample - loss: 0.5156 - acc: 0.9211 - score: 0.9333 - val_loss: 0.9492 - val_acc: 0.7822 - val_score: 0.8178\n",
      "Epoch 88/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.9313 - score: 0.9422\n",
      "Epoch 00088: val_score did not improve from 0.81776\n",
      "5841/5841 [==============================] - 1s 193us/sample - loss: 0.4961 - acc: 0.9308 - score: 0.9404 - val_loss: 0.9507 - val_acc: 0.7705 - val_score: 0.8058\n",
      "Epoch 89/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 0.4943 - acc: 0.9267 - score: 0.9390\n",
      "Epoch 00089: val_score did not improve from 0.81776\n",
      "5841/5841 [==============================] - 1s 196us/sample - loss: 0.4956 - acc: 0.9269 - score: 0.9393 - val_loss: 0.9581 - val_acc: 0.7629 - val_score: 0.8008\n",
      "Epoch 90/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.9348 - score: 0.9453\n",
      "Epoch 00090: val_score did not improve from 0.81776\n",
      "5841/5841 [==============================] - 1s 197us/sample - loss: 0.4966 - acc: 0.9346 - score: 0.9448 - val_loss: 0.9369 - val_acc: 0.7788 - val_score: 0.8137\n",
      "Epoch 91/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4975 - acc: 0.9287 - score: 0.9401\n",
      "Epoch 00091: val_score did not improve from 0.81776\n",
      "5841/5841 [==============================] - 1s 196us/sample - loss: 0.4985 - acc: 0.9284 - score: 0.9391 - val_loss: 0.9542 - val_acc: 0.7684 - val_score: 0.8047\n",
      "Epoch 92/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.9282 - score: 0.9397\n",
      "Epoch 00092: val_score did not improve from 0.81776\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.4974 - acc: 0.9276 - score: 0.9376 - val_loss: 0.9710 - val_acc: 0.7657 - val_score: 0.8011\n",
      "Epoch 93/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4956 - acc: 0.9279 - score: 0.9397\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00093: val_score did not improve from 0.81776\n",
      "5841/5841 [==============================] - 1s 196us/sample - loss: 0.4959 - acc: 0.9274 - score: 0.9380 - val_loss: 0.9553 - val_acc: 0.7726 - val_score: 0.8060\n",
      "Epoch 94/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4813 - acc: 0.9329 - score: 0.9434\n",
      "Epoch 00094: val_score did not improve from 0.81776\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.4815 - acc: 0.9327 - score: 0.9429 - val_loss: 0.9273 - val_acc: 0.7781 - val_score: 0.8128\n",
      "Epoch 95/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.9368 - score: 0.9470\n",
      "Epoch 00095: val_score improved from 0.81776 to 0.82250, saving model to fold4.h5\n",
      "5841/5841 [==============================] - 1s 206us/sample - loss: 0.4786 - acc: 0.9367 - score: 0.9467 - val_loss: 0.9173 - val_acc: 0.7891 - val_score: 0.8225\n",
      "Epoch 96/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4700 - acc: 0.9378 - score: 0.9477\n",
      "Epoch 00096: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.4702 - acc: 0.9379 - score: 0.9477 - val_loss: 0.9170 - val_acc: 0.7843 - val_score: 0.8190\n",
      "Epoch 97/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.9339 - score: 0.9437\n",
      "Epoch 00097: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.4722 - acc: 0.9332 - score: 0.9418 - val_loss: 0.9128 - val_acc: 0.7822 - val_score: 0.8152\n",
      "Epoch 98/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4669 - acc: 0.9402 - score: 0.9501\n",
      "Epoch 00098: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 0.4681 - acc: 0.9394 - score: 0.9477 - val_loss: 0.9014 - val_acc: 0.7836 - val_score: 0.8181\n",
      "Epoch 99/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4601 - acc: 0.9463 - score: 0.9548\n",
      "Epoch 00099: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 0.4605 - acc: 0.9461 - score: 0.9543 - val_loss: 0.9085 - val_acc: 0.7781 - val_score: 0.8125\n",
      "Epoch 100/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4840 - acc: 0.9336 - score: 0.9438\n",
      "Epoch 00100: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 193us/sample - loss: 0.4849 - acc: 0.9332 - score: 0.9427 - val_loss: 0.9415 - val_acc: 0.7712 - val_score: 0.8062\n",
      "Epoch 101/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4588 - acc: 0.9420 - score: 0.9504\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00101: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 0.4591 - acc: 0.9420 - score: 0.9504 - val_loss: 0.9206 - val_acc: 0.7870 - val_score: 0.8181\n",
      "Epoch 102/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.9454 - score: 0.9536\n",
      "Epoch 00102: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.4515 - acc: 0.9445 - score: 0.9506 - val_loss: 0.9113 - val_acc: 0.7850 - val_score: 0.8179\n",
      "Epoch 103/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.9439 - score: 0.9530\n",
      "Epoch 00103: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 193us/sample - loss: 0.4604 - acc: 0.9435 - score: 0.9518 - val_loss: 0.8987 - val_acc: 0.7884 - val_score: 0.8217\n",
      "Epoch 104/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.9454 - score: 0.9541\n",
      "Epoch 00104: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.4566 - acc: 0.9449 - score: 0.9522 - val_loss: 0.9078 - val_acc: 0.7808 - val_score: 0.8160\n",
      "Epoch 105/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4558 - acc: 0.9452 - score: 0.9538\n",
      "Epoch 00105: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 0.4573 - acc: 0.9445 - score: 0.9514 - val_loss: 0.9088 - val_acc: 0.7815 - val_score: 0.8157\n",
      "Epoch 106/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.9444 - score: 0.9534\n",
      "Epoch 00106: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 194us/sample - loss: 0.4583 - acc: 0.9440 - score: 0.9522 - val_loss: 0.9122 - val_acc: 0.7829 - val_score: 0.8179\n",
      "Epoch 107/300\n",
      "5760/5841 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.9446 - score: 0.9533\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00107: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 0.4553 - acc: 0.9440 - score: 0.9510 - val_loss: 0.8989 - val_acc: 0.7891 - val_score: 0.8221\n",
      "Epoch 108/300\n",
      "5632/5841 [===========================>..] - ETA: 0s - loss: 0.4493 - acc: 0.9473 - score: 0.9558\n",
      "Epoch 00108: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 0.4533 - acc: 0.9456 - score: 0.9523 - val_loss: 0.9020 - val_acc: 0.7829 - val_score: 0.8159\n",
      "Epoch 109/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.9564 - score: 0.9638\n",
      "Epoch 00109: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 197us/sample - loss: 0.4267 - acc: 0.9557 - score: 0.9613 - val_loss: 0.9050 - val_acc: 0.7808 - val_score: 0.8148\n",
      "Epoch 110/300\n",
      "5824/5841 [============================>.] - ETA: 0s - loss: 0.4466 - acc: 0.9469 - score: 0.9554\n",
      "Epoch 00110: val_score did not improve from 0.82250\n",
      "5841/5841 [==============================] - 1s 195us/sample - loss: 0.4473 - acc: 0.9466 - score: 0.9543 - val_loss: 0.8987 - val_acc: 0.7870 - val_score: 0.8189\n",
      "Epoch 00110: early stopping\n",
      "on_train_set: [0.95384467 0.94025314 0.9427666  0.9543948  0.96134883]\n",
      "average: 0.95052165\n",
      "on_test_set: [0.83474845 0.8458577  0.8365735  0.8469431  0.8225044 ]\n",
      "average: 0.8373254\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess=tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "kfold = StratifiedKFold(5, shuffle=True,random_state=20001026)\n",
    "proba_t = np.zeros((7500, 19))\n",
    "train_score=[]\n",
    "test_score=[]\n",
    "for fold,(xx,yy) in enumerate(kfold.split(x,y)):\n",
    "    tf.reset_default_graph()\n",
    "    inputs=Input(shape=[64,8])\n",
    "    outputs=CNN(inputs,num_classes=19)\n",
    "    model=Model(inputs=inputs,outputs=outputs)\n",
    "    _y=to_categorical(y,19)\n",
    "    plateau = ReduceLROnPlateau(monitor=\"val_score\",\n",
    "                                verbose=1,\n",
    "                                mode='max',\n",
    "                                factor=0.5,\n",
    "                                patience=6)\n",
    "    early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                                   verbose=1,\n",
    "                                   mode='max',\n",
    "                                   patience=15)\n",
    "    checkpoint = ModelCheckpoint(f'fold{fold}.h5',\n",
    "                                 monitor='val_score',\n",
    "                                 verbose=1,\n",
    "                                 mode='max',\n",
    "                                 save_best_only=True)\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer=Adam(),metrics=[\"acc\",score])\n",
    "    trained_model=model.fit(\n",
    "            x[xx],\n",
    "            _y[xx],\n",
    "            batch_size=batch_size,\n",
    "            class_weight=(1-class_weight)**3,\n",
    "            shuffle=True,\n",
    "            validation_data=(x[yy],_y[yy]),\n",
    "            epochs=300,\n",
    "            callbacks=[plateau,early_stopping,checkpoint])\n",
    "    model.load_weights(f'fold{fold}.h5')\n",
    "    proba_t += model.predict(x_val, verbose=0, batch_size=1024) / 5.\n",
    "    train_score.append(np.array(trained_model.history[\"score\"]).max())\n",
    "    test_score.append(np.array(trained_model.history[\"val_score\"]).max())\n",
    "label=proba_t.argmax(axis=1)\n",
    "print(\"on_train_set:\",np.array(train_score))\n",
    "print(\"average:\",np.array(train_score).mean())\n",
    "print(\"on_test_set:\",np.array(test_score))\n",
    "print(\"average:\",np.array(test_score).mean())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_train_set: [0.91047037 0.90115476 0.9113633  0.9011248  0.9129262 ]\n",
      "average: 0.9074079\n",
      "on_test_set: [0.82076246 0.83817184 0.81423265 0.83439636 0.80578   ]\n",
      "average: 0.8226687\n"
     ]
    }
   ],
   "source": [
    "print(\"on_train_set:\",np.array(train_score))\n",
    "print(\"average:\",np.array(train_score).mean())\n",
    "print(\"on_test_set:\",np.array(test_score))\n",
    "print(\"average:\",np.array(test_score).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "frame = pd.DataFrame(label)\n",
    "frame.rename(columns={0:'behavior_id'},inplace = True)\n",
    "frame.reset_index(inplace = True)\n",
    "frame.rename(columns={'index':'fragment_id',0:'behavior_id'},inplace = True)\n",
    "frame.to_csv('submit_8373.9505.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
